{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_19300\\4271784750.py:2: DeprecationWarning: The Tix Tk extension is unmaintained, and the tkinter.tix wrapper module is deprecated in favor of tkinter.ttk\n",
      "  from tkinter.tix import AUTO\n"
     ]
    }
   ],
   "source": [
    "from dis import dis\n",
    "from tkinter.tix import AUTO\n",
    "from requests import head\n",
    "import torch\n",
    "import config\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from re import X\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from CustomDataset import CustomRawDataset\n",
    "from model_dispatcher import dispatch_model\n",
    "# from model_dispatcher_cnn import dispatch_model\n",
    "import config\n",
    "from torch import nn\n",
    "import os\n",
    "from glob import glob\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from CustomDataset import custom_collate_fn\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,TimeDistributed\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed\", \"*\", \"*.csv\"))\n",
    "# print(raw_angle_files_1)\n",
    "all_filenames = [i for i in raw_angle_files_1]\n",
    "df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "data=df\n",
    "data.to_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\auto_encoder_merged.csv\",index=False)\n",
    "#df=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\FEATURES_EXTRACTED\\DISTANCES\\both_hand_frontup_left_leg_frontup\\aatish_both_hand_frontup_left_leg_frontup_trial1_interpolated.csv\")\n",
    "   \n",
    "# data=df\n",
    "# print(data.shape)\n",
    "# target=data['54']\n",
    "# data=data.drop(['54'],axis=1)\n",
    "# # data.head()\n",
    "# print(data.shape)\n",
    "# AutoEncoder(data)\n",
    "# data['12']=target\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2322, 12)\n",
      "(2322,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523543</td>\n",
       "      <td>-0.088508</td>\n",
       "      <td>-0.194850</td>\n",
       "      <td>-0.087364</td>\n",
       "      <td>0.470946</td>\n",
       "      <td>-0.292149</td>\n",
       "      <td>0.781330</td>\n",
       "      <td>-0.326664</td>\n",
       "      <td>0.146532</td>\n",
       "      <td>0.255213</td>\n",
       "      <td>-0.137174</td>\n",
       "      <td>0.126982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.522329</td>\n",
       "      <td>-0.104959</td>\n",
       "      <td>-0.205592</td>\n",
       "      <td>-0.075078</td>\n",
       "      <td>0.481907</td>\n",
       "      <td>-0.292485</td>\n",
       "      <td>0.779297</td>\n",
       "      <td>-0.339359</td>\n",
       "      <td>0.174192</td>\n",
       "      <td>0.260329</td>\n",
       "      <td>-0.137726</td>\n",
       "      <td>0.114085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523989</td>\n",
       "      <td>-0.105846</td>\n",
       "      <td>-0.206193</td>\n",
       "      <td>-0.074144</td>\n",
       "      <td>0.483203</td>\n",
       "      <td>-0.290903</td>\n",
       "      <td>0.779869</td>\n",
       "      <td>-0.340611</td>\n",
       "      <td>0.177164</td>\n",
       "      <td>0.259578</td>\n",
       "      <td>-0.139317</td>\n",
       "      <td>0.113020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.525422</td>\n",
       "      <td>-0.104197</td>\n",
       "      <td>-0.204314</td>\n",
       "      <td>-0.076454</td>\n",
       "      <td>0.482316</td>\n",
       "      <td>-0.289938</td>\n",
       "      <td>0.780534</td>\n",
       "      <td>-0.339569</td>\n",
       "      <td>0.175237</td>\n",
       "      <td>0.258143</td>\n",
       "      <td>-0.140063</td>\n",
       "      <td>0.114026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.523434</td>\n",
       "      <td>-0.105483</td>\n",
       "      <td>-0.205797</td>\n",
       "      <td>-0.075720</td>\n",
       "      <td>0.482699</td>\n",
       "      <td>-0.291388</td>\n",
       "      <td>0.780379</td>\n",
       "      <td>-0.340481</td>\n",
       "      <td>0.177614</td>\n",
       "      <td>0.258089</td>\n",
       "      <td>-0.138193</td>\n",
       "      <td>0.113836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.523543 -0.088508 -0.194850 -0.087364  0.470946 -0.292149  0.781330   \n",
       "1  0.522329 -0.104959 -0.205592 -0.075078  0.481907 -0.292485  0.779297   \n",
       "2  0.523989 -0.105846 -0.206193 -0.074144  0.483203 -0.290903  0.779869   \n",
       "3  0.525422 -0.104197 -0.204314 -0.076454  0.482316 -0.289938  0.780534   \n",
       "4  0.523434 -0.105483 -0.205797 -0.075720  0.482699 -0.291388  0.780379   \n",
       "\n",
       "          7         8         9        10        11  \n",
       "0 -0.326664  0.146532  0.255213 -0.137174  0.126982  \n",
       "1 -0.339359  0.174192  0.260329 -0.137726  0.114085  \n",
       "2 -0.340611  0.177164  0.259578 -0.139317  0.113020  \n",
       "3 -0.339569  0.175237  0.258143 -0.140063  0.114026  \n",
       "4 -0.340481  0.177614  0.258089 -0.138193  0.113836  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\auto_encoder_merged.csv\")\n",
    "data.head()\n",
    "target=data['12']\n",
    "data=data.drop(['12'],axis=1)\n",
    "# data.head()\n",
    "# print(data.shape)\n",
    "# AutoEncoder(data)\n",
    "# data['12']=target\n",
    "# print(data.shape)\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :12], sequences[end_ix-1, 12:]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2322, 21)\n",
      "(1853, 6, 12)\n",
      "(1853, 9)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 4, 64)             2368      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 2, 64)             12352     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1, 64)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               6500      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100)              400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,630\n",
      "Trainable params: 22,430\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 9) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m \u001b[39m# # print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,batch_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mBATCH_SIZE,epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(x_test,y_test),verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     48\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n\u001b[0;32m     49\u001b[0m y_pred\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file01yokttt.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 9) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(df.shape)\n",
    "# df = df.sample(frac = 1)\n",
    "# # df.iloc[1:10,:]\n",
    "data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\auto_encoder_merged.csv\")\n",
    "data=pd.get_dummies(data,columns=['12'])\n",
    "data=data.to_numpy()\n",
    "x,y=split_sequences(data,6)\n",
    "# x=x[None:]\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# data=data.to_numpy()\n",
    "print(data.shape)\n",
    "\n",
    "# y=data.iloc[:,26:]\n",
    "# x=data.iloc[:,:26]\n",
    "# # x=data[-1:26]\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# x=x.to_numpy()\n",
    "# y=y.to_numpy()\n",
    "x_train,x_test,y_train,y_test= train_test_split(x, y, test_size = 0.2)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(6,12)))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "# # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # print(x.shape)\n",
    "history = model.fit(x_train, y_train,batch_size=config.BATCH_SIZE,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "y_pred=model.predict(x_test)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "y_test=np.argmax(y_test,axis=1)\n",
    "# print(y_pred)\n",
    "cf_matrix=metrics.confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix\\n',cf_matrix)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229cdfb8eedfa4964725b7eb0da8d7a63b25d97a6ab808f09bd6b506844c0629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
