{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17164\\584230919.py:2: DeprecationWarning: The Tix Tk extension is unmaintained, and the tkinter.tix wrapper module is deprecated in favor of tkinter.ttk\n",
      "  from tkinter.tix import AUTO\n"
     ]
    }
   ],
   "source": [
    "from dis import dis\n",
    "from tkinter.tix import AUTO\n",
    "from requests import head\n",
    "import torch\n",
    "import config\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from re import X\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from CustomDataset import CustomRawDataset\n",
    "from model_dispatcher import dispatch_model\n",
    "# from model_dispatcher_cnn import dispatch_model\n",
    "import config\n",
    "from torch import nn\n",
    "import os\n",
    "from glob import glob\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from CustomDataset import custom_collate_fn\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,TimeDistributed\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed\", \"*\", \"*.csv\"))\n",
    "# print(raw_angle_files_1)\n",
    "all_filenames = [i for i in raw_angle_files_1]\n",
    "df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "data=df\n",
    "data.to_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\merged.csv\",index=False)\n",
    "#df=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\FEATURES_EXTRACTED\\DISTANCES\\both_hand_frontup_left_leg_frontup\\aatish_both_hand_frontup_left_leg_frontup_trial1_interpolated.csv\")\n",
    "   \n",
    "# data=df\n",
    "# print(data.shape)\n",
    "# target=data['54']\n",
    "# data=data.drop(['54'],axis=1)\n",
    "# # data.head()\n",
    "# print(data.shape)\n",
    "# AutoEncoder(data)\n",
    "# data['12']=target\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_csv(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mResearch_Project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMy_project_22\u001b[39m\u001b[39m\\\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mfinal_preprocessed_merged\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmerged_modified.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m data\u001b[39m.\u001b[39mhead()\n\u001b[0;32m      3\u001b[0m target\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\merged_modified.csv\")\n",
    "data.head()\n",
    "target=data['target']\n",
    "data=data.drop(['target'],axis=1)\n",
    "# data.head()\n",
    "# print(data.shape)\n",
    "# AutoEncoder(data)\n",
    "# data['12']=target\n",
    "# print(data.shape)\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculating scores\n",
    "x=data.to_numpy()\n",
    "y=target.to_numpy()\n",
    "top_k=26\n",
    "\n",
    "ranks = fisher_score.fisher_score(x, y,mode=\"ranks\")\n",
    "\n",
    "\n",
    "feat_importances=pd.Series(ranks, data.columns[0:len(data.columns)])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_id=[]\n",
    "#top 26 values\n",
    "# print(feat_importances[0])\n",
    "id=0\n",
    "sorted_feat_importances=[]\n",
    "feat_imp_index=[i for i in feat_importances.index]\n",
    "feat_imp_value=[i for i in feat_importances.values]\n",
    "for i in range(len(feat_importances)):\n",
    "    if(feat_imp_value[i]<top_k):\n",
    "\n",
    "        sorted_id.append(feat_imp_index[i])\n",
    "        sorted_feat_importances.append(feat_imp_value[i])\n",
    "data=data[sorted_id]\n",
    "\n",
    "# feat_importances.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'features')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(20,10))\n",
    "spacing = 0.1\n",
    "fig.subplots_adjust(bottom=spacing)\n",
    "x=sorted_feat_importances\n",
    "y=sorted_id\n",
    "plt.barh(x,y)\n",
    "plt.xlabel('ranks',fontsize=18)\n",
    "plt.ylabel('features',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_0_1</th>\n",
       "      <th>dist_2_1</th>\n",
       "      <th>dist_3_1</th>\n",
       "      <th>dist_4_1</th>\n",
       "      <th>dist_5_1</th>\n",
       "      <th>dist_6_1</th>\n",
       "      <th>dist_7_1</th>\n",
       "      <th>dist_8_1</th>\n",
       "      <th>dist_9_1</th>\n",
       "      <th>dist_10_1</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid_3_21_15_a</th>\n",
       "      <th>centroid_20_5_1_b</th>\n",
       "      <th>centroid_15_12_19_a</th>\n",
       "      <th>centroid_15_12_19_c</th>\n",
       "      <th>centroid_15_1_10_b</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324883</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.475169</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>0.224666</td>\n",
       "      <td>0.380845</td>\n",
       "      <td>0.480537</td>\n",
       "      <td>0.253687</td>\n",
       "      <td>0.224452</td>\n",
       "      <td>0.390823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294137</td>\n",
       "      <td>0.305887</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>2.261033</td>\n",
       "      <td>-0.239840</td>\n",
       "      <td>-0.296553</td>\n",
       "      <td>0.162107</td>\n",
       "      <td>-0.027582</td>\n",
       "      <td>0.031990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322438</td>\n",
       "      <td>0.312178</td>\n",
       "      <td>0.470942</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.229644</td>\n",
       "      <td>0.384718</td>\n",
       "      <td>0.450586</td>\n",
       "      <td>0.252057</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.390271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292750</td>\n",
       "      <td>0.300970</td>\n",
       "      <td>0.350710</td>\n",
       "      <td>2.268333</td>\n",
       "      <td>-0.241051</td>\n",
       "      <td>-0.288412</td>\n",
       "      <td>0.157114</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.029938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321594</td>\n",
       "      <td>0.311465</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.386289</td>\n",
       "      <td>0.442863</td>\n",
       "      <td>0.251537</td>\n",
       "      <td>0.221754</td>\n",
       "      <td>0.390644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292657</td>\n",
       "      <td>0.299360</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>2.270667</td>\n",
       "      <td>-0.241705</td>\n",
       "      <td>-0.288063</td>\n",
       "      <td>0.156264</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>0.029953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321504</td>\n",
       "      <td>0.311379</td>\n",
       "      <td>0.469090</td>\n",
       "      <td>0.261484</td>\n",
       "      <td>0.231913</td>\n",
       "      <td>0.386717</td>\n",
       "      <td>0.444212</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>0.221688</td>\n",
       "      <td>0.390763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292577</td>\n",
       "      <td>0.299230</td>\n",
       "      <td>0.352107</td>\n",
       "      <td>2.270800</td>\n",
       "      <td>-0.241827</td>\n",
       "      <td>-0.288264</td>\n",
       "      <td>0.155941</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321446</td>\n",
       "      <td>0.311360</td>\n",
       "      <td>0.469092</td>\n",
       "      <td>0.261522</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.387123</td>\n",
       "      <td>0.445079</td>\n",
       "      <td>0.251438</td>\n",
       "      <td>0.222014</td>\n",
       "      <td>0.390721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291617</td>\n",
       "      <td>0.299090</td>\n",
       "      <td>0.352097</td>\n",
       "      <td>2.270833</td>\n",
       "      <td>-0.241924</td>\n",
       "      <td>-0.286949</td>\n",
       "      <td>0.155132</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist_0_1  dist_2_1  dist_3_1  dist_4_1  dist_5_1  dist_6_1  dist_7_1  \\\n",
       "0  0.324883  0.314224  0.475169  0.257537  0.224666  0.380845  0.480537   \n",
       "1  0.322438  0.312178  0.470942  0.260437  0.229644  0.384718  0.450586   \n",
       "2  0.321594  0.311465  0.469381  0.261410  0.231567  0.386289  0.442863   \n",
       "3  0.321504  0.311379  0.469090  0.261484  0.231913  0.386717  0.444212   \n",
       "4  0.321446  0.311360  0.469092  0.261522  0.232146  0.387123  0.445079   \n",
       "\n",
       "   dist_8_1  dist_9_1  dist_10_1  ...  centroid_3_21_15_a  centroid_20_5_1_b  \\\n",
       "0  0.253687  0.224452   0.390823  ...            0.294137           0.305887   \n",
       "1  0.252057  0.222488   0.390271  ...            0.292750           0.300970   \n",
       "2  0.251537  0.221754   0.390644  ...            0.292657           0.299360   \n",
       "3  0.251446  0.221688   0.390763  ...            0.292577           0.299230   \n",
       "4  0.251438  0.222014   0.390721  ...            0.291617           0.299090   \n",
       "\n",
       "   centroid_15_12_19_a  centroid_15_12_19_c  centroid_15_1_10_b         4  \\\n",
       "0             0.346157             2.261033           -0.239840 -0.296553   \n",
       "1             0.350710             2.268333           -0.241051 -0.288412   \n",
       "2             0.352113             2.270667           -0.241705 -0.288063   \n",
       "3             0.352107             2.270800           -0.241827 -0.288264   \n",
       "4             0.352097             2.270833           -0.241924 -0.286949   \n",
       "\n",
       "          9        10        13  target  \n",
       "0  0.162107 -0.027582  0.031990       0  \n",
       "1  0.157114 -0.009114  0.029938       0  \n",
       "2  0.156264 -0.002401  0.029953       0  \n",
       "3  0.155941 -0.001148  0.030147       0  \n",
       "4  0.155132 -0.000888  0.030157       0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len1=data.shape[1]\n",
    "# l1=list(range(0,len1))\n",
    "# l2=[]\n",
    "# for i in l1:\n",
    "#     i=str(i)\n",
    "#     l2.append(i)\n",
    "# l1=l2\n",
    "    \n",
    "# data.columns=l1\n",
    "# data.head()\n",
    "data['target']=target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_modified.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :26], sequences[end_ix-1, 26:]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 36)\n",
      "(9804, 6, 26)\n",
      "(9804, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 4, 64)             5056      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 2, 64)             12352     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1, 64)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               6500      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100)              400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,318\n",
      "Trainable params: 25,118\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "(9804, 10)\n",
      "Epoch 1/100\n",
      "154/154 [==============================] - 5s 10ms/step - loss: 2.2611 - accuracy: 0.1904 - val_loss: 2.2857 - val_accuracy: 0.1134\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.8353 - accuracy: 0.3102 - val_loss: 2.2775 - val_accuracy: 0.1102\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.6196 - accuracy: 0.3921 - val_loss: 2.3479 - val_accuracy: 0.2162\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.5074 - accuracy: 0.4349 - val_loss: 2.2608 - val_accuracy: 0.2579\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 1s 8ms/step - loss: 1.3992 - accuracy: 0.4839 - val_loss: 2.2021 - val_accuracy: 0.2897\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.3711 - accuracy: 0.4874 - val_loss: 2.1766 - val_accuracy: 0.3594\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.3932 - accuracy: 0.4814 - val_loss: 2.2369 - val_accuracy: 0.3423\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.2836 - accuracy: 0.5254 - val_loss: 2.2728 - val_accuracy: 0.3774\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.2031 - accuracy: 0.5630 - val_loss: 2.1742 - val_accuracy: 0.3509\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.1957 - accuracy: 0.5593 - val_loss: 2.3207 - val_accuracy: 0.3998\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.1609 - accuracy: 0.5755 - val_loss: 2.3932 - val_accuracy: 0.4545\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.1006 - accuracy: 0.5974 - val_loss: 2.0114 - val_accuracy: 0.4206\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.0550 - accuracy: 0.6115 - val_loss: 1.9809 - val_accuracy: 0.4574\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.0211 - accuracy: 0.6266 - val_loss: 2.4800 - val_accuracy: 0.4888\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 1.0084 - accuracy: 0.6289 - val_loss: 2.2222 - val_accuracy: 0.4855\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.9729 - accuracy: 0.6412 - val_loss: 1.8074 - val_accuracy: 0.5720\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.9704 - accuracy: 0.6476 - val_loss: 1.6279 - val_accuracy: 0.6071\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.9041 - accuracy: 0.6656 - val_loss: 1.3981 - val_accuracy: 0.6401\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.8751 - accuracy: 0.6762 - val_loss: 1.5200 - val_accuracy: 0.5394\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.8446 - accuracy: 0.6906 - val_loss: 1.4204 - val_accuracy: 0.5630\n",
      "Epoch 21/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.8294 - accuracy: 0.6957 - val_loss: 1.2313 - val_accuracy: 0.6544\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.8328 - accuracy: 0.6925 - val_loss: 1.1829 - val_accuracy: 0.6524\n",
      "Epoch 23/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.7723 - accuracy: 0.7167 - val_loss: 1.1227 - val_accuracy: 0.6642\n",
      "Epoch 24/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.7617 - accuracy: 0.7238 - val_loss: 1.0856 - val_accuracy: 0.6308\n",
      "Epoch 25/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.7752 - accuracy: 0.7202 - val_loss: 1.1412 - val_accuracy: 0.6067\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6906 - accuracy: 0.7515 - val_loss: 1.0583 - val_accuracy: 0.6385\n",
      "Epoch 27/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6961 - accuracy: 0.7448 - val_loss: 0.9349 - val_accuracy: 0.7022\n",
      "Epoch 28/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6819 - accuracy: 0.7484 - val_loss: 0.8810 - val_accuracy: 0.7177\n",
      "Epoch 29/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6718 - accuracy: 0.7543 - val_loss: 0.9487 - val_accuracy: 0.7062\n",
      "Epoch 30/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6541 - accuracy: 0.7657 - val_loss: 0.9502 - val_accuracy: 0.7124\n",
      "Epoch 31/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6444 - accuracy: 0.7622 - val_loss: 0.9168 - val_accuracy: 0.7022\n",
      "Epoch 32/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6361 - accuracy: 0.7672 - val_loss: 0.9017 - val_accuracy: 0.7332\n",
      "Epoch 33/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6117 - accuracy: 0.7775 - val_loss: 0.9078 - val_accuracy: 0.6952\n",
      "Epoch 34/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6075 - accuracy: 0.7798 - val_loss: 0.8192 - val_accuracy: 0.7499\n",
      "Epoch 35/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.6376 - accuracy: 0.7653 - val_loss: 0.7077 - val_accuracy: 0.7858\n",
      "Epoch 36/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5511 - accuracy: 0.8020 - val_loss: 0.7700 - val_accuracy: 0.7552\n",
      "Epoch 37/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5339 - accuracy: 0.8088 - val_loss: 0.7286 - val_accuracy: 0.7634\n",
      "Epoch 38/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5703 - accuracy: 0.7935 - val_loss: 0.8624 - val_accuracy: 0.6883\n",
      "Epoch 39/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5538 - accuracy: 0.7987 - val_loss: 0.6293 - val_accuracy: 0.8140\n",
      "Epoch 40/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5337 - accuracy: 0.8062 - val_loss: 0.7523 - val_accuracy: 0.7160\n",
      "Epoch 41/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5199 - accuracy: 0.8134 - val_loss: 0.6843 - val_accuracy: 0.7568\n",
      "Epoch 42/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5310 - accuracy: 0.8042 - val_loss: 0.7016 - val_accuracy: 0.7699\n",
      "Epoch 43/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4890 - accuracy: 0.8243 - val_loss: 0.5787 - val_accuracy: 0.7944\n",
      "Epoch 44/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4824 - accuracy: 0.8261 - val_loss: 0.5608 - val_accuracy: 0.8099\n",
      "Epoch 45/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.5058 - accuracy: 0.8157 - val_loss: 0.5279 - val_accuracy: 0.8315\n",
      "Epoch 46/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4603 - accuracy: 0.8349 - val_loss: 0.4967 - val_accuracy: 0.8409\n",
      "Epoch 47/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4840 - accuracy: 0.8301 - val_loss: 0.6108 - val_accuracy: 0.8009\n",
      "Epoch 48/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4410 - accuracy: 0.8412 - val_loss: 0.5955 - val_accuracy: 0.8038\n",
      "Epoch 49/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.8196 - val_loss: 0.5459 - val_accuracy: 0.8339\n",
      "Epoch 50/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4405 - accuracy: 0.8378 - val_loss: 0.6226 - val_accuracy: 0.8058\n",
      "Epoch 51/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4569 - accuracy: 0.8338 - val_loss: 0.6008 - val_accuracy: 0.8123\n",
      "Epoch 52/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4474 - accuracy: 0.8417 - val_loss: 0.6097 - val_accuracy: 0.8078\n",
      "Epoch 53/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.8381 - val_loss: 0.5857 - val_accuracy: 0.8172\n",
      "Epoch 54/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4252 - accuracy: 0.8467 - val_loss: 0.5613 - val_accuracy: 0.8213\n",
      "Epoch 55/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4189 - accuracy: 0.8503 - val_loss: 0.5415 - val_accuracy: 0.8229\n",
      "Epoch 56/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3949 - accuracy: 0.8607 - val_loss: 0.4504 - val_accuracy: 0.8580\n",
      "Epoch 57/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4236 - accuracy: 0.8494 - val_loss: 0.5217 - val_accuracy: 0.8246\n",
      "Epoch 58/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4417 - accuracy: 0.8426 - val_loss: 0.5077 - val_accuracy: 0.8193\n",
      "Epoch 59/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4093 - accuracy: 0.8536 - val_loss: 0.5332 - val_accuracy: 0.8286\n",
      "Epoch 60/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4399 - accuracy: 0.8418 - val_loss: 0.6101 - val_accuracy: 0.7854\n",
      "Epoch 61/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4179 - accuracy: 0.8538 - val_loss: 0.5891 - val_accuracy: 0.7842\n",
      "Epoch 62/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4332 - accuracy: 0.8464 - val_loss: 0.7150 - val_accuracy: 0.7634\n",
      "Epoch 63/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4151 - accuracy: 0.8501 - val_loss: 0.5821 - val_accuracy: 0.8070\n",
      "Epoch 64/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3908 - accuracy: 0.8583 - val_loss: 0.5527 - val_accuracy: 0.8425\n",
      "Epoch 65/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4120 - accuracy: 0.8528 - val_loss: 0.5765 - val_accuracy: 0.8082\n",
      "Epoch 66/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4294 - accuracy: 0.8472 - val_loss: 2.2125 - val_accuracy: 0.8499\n",
      "Epoch 67/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4042 - accuracy: 0.8547 - val_loss: 1.0403 - val_accuracy: 0.7813\n",
      "Epoch 68/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.4109 - accuracy: 0.8522 - val_loss: 0.9992 - val_accuracy: 0.7666\n",
      "Epoch 69/100\n",
      "154/154 [==============================] - 1s 8ms/step - loss: 0.3645 - accuracy: 0.8713 - val_loss: 0.8636 - val_accuracy: 0.7560\n",
      "Epoch 70/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8625 - val_loss: 0.8585 - val_accuracy: 0.7209\n",
      "Epoch 71/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3804 - accuracy: 0.8628 - val_loss: 0.6191 - val_accuracy: 0.8082\n",
      "Epoch 72/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3726 - accuracy: 0.8692 - val_loss: 0.5172 - val_accuracy: 0.8413\n",
      "Epoch 73/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3703 - accuracy: 0.8688 - val_loss: 0.5692 - val_accuracy: 0.8127\n",
      "Epoch 74/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3910 - accuracy: 0.8609 - val_loss: 0.4293 - val_accuracy: 0.8703\n",
      "Epoch 75/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3756 - accuracy: 0.8680 - val_loss: 0.4008 - val_accuracy: 0.8902\n",
      "Epoch 76/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3777 - accuracy: 0.8683 - val_loss: 0.4102 - val_accuracy: 0.8731\n",
      "Epoch 77/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3501 - accuracy: 0.8770 - val_loss: 0.4442 - val_accuracy: 0.8552\n",
      "Epoch 78/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3741 - accuracy: 0.8686 - val_loss: 0.3936 - val_accuracy: 0.8780\n",
      "Epoch 79/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3805 - accuracy: 0.8663 - val_loss: 0.3959 - val_accuracy: 0.8833\n",
      "Epoch 80/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3415 - accuracy: 0.8769 - val_loss: 0.4223 - val_accuracy: 0.8698\n",
      "Epoch 81/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3266 - accuracy: 0.8861 - val_loss: 0.3666 - val_accuracy: 0.8817\n",
      "Epoch 82/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3656 - accuracy: 0.8703 - val_loss: 0.3925 - val_accuracy: 0.8870\n",
      "Epoch 83/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3710 - accuracy: 0.8685 - val_loss: 0.4237 - val_accuracy: 0.8678\n",
      "Epoch 84/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3586 - accuracy: 0.8766 - val_loss: 0.5476 - val_accuracy: 0.8156\n",
      "Epoch 85/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3309 - accuracy: 0.8868 - val_loss: 0.5631 - val_accuracy: 0.7931\n",
      "Epoch 86/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3601 - accuracy: 0.8743 - val_loss: 0.4249 - val_accuracy: 0.8535\n",
      "Epoch 87/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3858 - accuracy: 0.8677 - val_loss: 0.5677 - val_accuracy: 0.8046\n",
      "Epoch 88/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3648 - accuracy: 0.8717 - val_loss: 0.6232 - val_accuracy: 0.7989\n",
      "Epoch 89/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3303 - accuracy: 0.8814 - val_loss: 0.5085 - val_accuracy: 0.8348\n",
      "Epoch 90/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3438 - accuracy: 0.8774 - val_loss: 0.4543 - val_accuracy: 0.8454\n",
      "Epoch 91/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3333 - accuracy: 0.8841 - val_loss: 0.9361 - val_accuracy: 0.8972\n",
      "Epoch 92/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3624 - accuracy: 0.8716 - val_loss: 0.6770 - val_accuracy: 0.8237\n",
      "Epoch 93/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3626 - accuracy: 0.8687 - val_loss: 0.4075 - val_accuracy: 0.8743\n",
      "Epoch 94/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3224 - accuracy: 0.8892 - val_loss: 0.4080 - val_accuracy: 0.8601\n",
      "Epoch 95/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3356 - accuracy: 0.8798 - val_loss: 0.3615 - val_accuracy: 0.8800\n",
      "Epoch 96/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3329 - accuracy: 0.8857 - val_loss: 0.4328 - val_accuracy: 0.8731\n",
      "Epoch 97/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3119 - accuracy: 0.8913 - val_loss: 0.5675 - val_accuracy: 0.7948\n",
      "Epoch 98/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3589 - accuracy: 0.8745 - val_loss: 0.5165 - val_accuracy: 0.8229\n",
      "Epoch 99/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3358 - accuracy: 0.8832 - val_loss: 0.5453 - val_accuracy: 0.8054\n",
      "Epoch 100/100\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3421 - accuracy: 0.8836 - val_loss: 0.5109 - val_accuracy: 0.8246\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5109 - accuracy: 0.8246\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "Confusion matrix\n",
      " [[211   1  50   7  18   0   0   0   1   9]\n",
      " [ 17 156   2   5  19   2   4   0  18   3]\n",
      " [  1   8 216   0   8   0   7   0   1  10]\n",
      " [  1   7  35 190   3   0   7   4   1   3]\n",
      " [  1   0   1   0 246   2   1   0   3   5]\n",
      " [  0   1   0   0   9 158  16  19   1   4]\n",
      " [  0   0   1   0   5  42 218   3   1   0]\n",
      " [  0   0   0   0   1   0   7 221   1   1]\n",
      " [  0   0   6   0   8   5   8   4 219  13]\n",
      " [  0   0   1   0   0   1   4   2   1 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80       297\n",
      "           1       0.90      0.69      0.78       226\n",
      "           2       0.69      0.86      0.77       251\n",
      "           3       0.94      0.76      0.84       251\n",
      "           4       0.78      0.95      0.85       259\n",
      "           5       0.75      0.76      0.76       208\n",
      "           6       0.80      0.81      0.80       270\n",
      "           7       0.87      0.96      0.91       231\n",
      "           8       0.89      0.83      0.86       263\n",
      "           9       0.79      0.95      0.87       195\n",
      "\n",
      "    accuracy                           0.82      2451\n",
      "   macro avg       0.83      0.83      0.82      2451\n",
      "weighted avg       0.84      0.82      0.82      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed_modified\", \"*\", \"*.csv\"))\n",
    "# # print(raw_angle_files_1)\n",
    "# all_filenames = [i for i in raw_angle_files_1]\n",
    "# df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_modified.csv\")\n",
    "# print(df.shape)\n",
    "# df = df.sample(frac = 1)\n",
    "# # df.iloc[1:10,:]\n",
    "data=pd.get_dummies(data,columns=['target'])\n",
    "data=data.to_numpy()\n",
    "x,y=split_sequences(data,6)\n",
    "# x=x[None:]\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# data=data.to_numpy()\n",
    "print(data.shape)\n",
    "\n",
    "# y=data.iloc[:,26:]\n",
    "# x=data.iloc[:,:26]\n",
    "# # x=data[-1:26]\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# x=x.to_numpy()\n",
    "# y=y.to_numpy()\n",
    "x_train,x_test,y_train,y_test= train_test_split(x, y, test_size = 0.2)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(6,26)))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "# # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # print(x.shape)\n",
    "history = model.fit(x_train, y_train,batch_size=config.BATCH_SIZE,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "y_pred=model.predict(x_test)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "y_test=np.argmax(y_test,axis=1)\n",
    "# print(y_pred)\n",
    "cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix\\n',cf_matrix)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[211   1  50   7  18   0   0   0   1   9]\n",
      " [ 17 156   2   5  19   2   4   0  18   3]\n",
      " [  1   8 216   0   8   0   7   0   1  10]\n",
      " [  1   7  35 190   3   0   7   4   1   3]\n",
      " [  1   0   1   0 246   2   1   0   3   5]\n",
      " [  0   1   0   0   9 158  16  19   1   4]\n",
      " [  0   0   1   0   5  42 218   3   1   0]\n",
      " [  0   0   0   0   1   0   7 221   1   1]\n",
      " [  0   0   6   0   8   5   8   4 219  13]\n",
      " [  0   0   1   0   0   1   4   2   1 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80       297\n",
      "           1       0.90      0.69      0.78       226\n",
      "           2       0.69      0.86      0.77       251\n",
      "           3       0.94      0.76      0.84       251\n",
      "           4       0.78      0.95      0.85       259\n",
      "           5       0.75      0.76      0.76       208\n",
      "           6       0.80      0.81      0.80       270\n",
      "           7       0.87      0.96      0.91       231\n",
      "           8       0.89      0.83      0.86       263\n",
      "           9       0.79      0.95      0.87       195\n",
      "\n",
      "    accuracy                           0.82      2451\n",
      "   macro avg       0.83      0.83      0.82      2451\n",
      "weighted avg       0.84      0.82      0.82      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix\\n',cf_matrix)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity:\n",
      "0   0.7104377104377104\n",
      "1   0.6902654867256637\n",
      "2   0.8605577689243028\n",
      "3   0.7569721115537849\n",
      "4   0.9498069498069498\n",
      "5   0.7596153846153846\n",
      "6   0.8074074074074075\n",
      "7   0.9567099567099567\n",
      "8   0.8326996197718631\n",
      "9   0.9538461538461539\n",
      "specificity avg\n",
      " 0.8278318549799177\n"
     ]
    }
   ],
   "source": [
    "print('specificity:')\n",
    "tot=0\n",
    "for i in range (10):\n",
    "    sum=0\n",
    "    for j in range(10):\n",
    "        sum=sum+cf_matrix[i,j]\n",
    "    spec=cf_matrix[i,i]/sum\n",
    "    tot+=spec\n",
    "    print(i,\" \",spec)\n",
    "print('specificity avg\\n',tot/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = history.history[\"accuracy\"]\n",
    "loss_val = history.history[\"val_accuracy\"]\n",
    "epochs = range(1,99)\n",
    "plt.plot(loss_train, 'g', label='Training accuracy')\n",
    "plt.plot(loss_val, 'b', label='validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = history.history[\"loss\"]\n",
    "loss_val = history.history[\"val_loss\"]\n",
    "epochs = range(1,99)\n",
    "plt.plot(loss_train, 'g', label='Training loss')\n",
    "plt.plot(loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate,pool size and kernel size graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 166)\n",
      "(12260,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_0_1</th>\n",
       "      <th>dist_2_1</th>\n",
       "      <th>dist_3_1</th>\n",
       "      <th>dist_4_1</th>\n",
       "      <th>dist_5_1</th>\n",
       "      <th>dist_6_1</th>\n",
       "      <th>dist_7_1</th>\n",
       "      <th>dist_8_1</th>\n",
       "      <th>dist_9_1</th>\n",
       "      <th>dist_10_1</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324883</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.475169</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>0.224666</td>\n",
       "      <td>0.380845</td>\n",
       "      <td>0.480537</td>\n",
       "      <td>0.253687</td>\n",
       "      <td>0.224452</td>\n",
       "      <td>0.390823</td>\n",
       "      <td>...</td>\n",
       "      <td>1.597459</td>\n",
       "      <td>-0.140846</td>\n",
       "      <td>1.246003</td>\n",
       "      <td>0.169821</td>\n",
       "      <td>0.162107</td>\n",
       "      <td>-0.027582</td>\n",
       "      <td>0.081955</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.031990</td>\n",
       "      <td>0.131282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322438</td>\n",
       "      <td>0.312178</td>\n",
       "      <td>0.470942</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.229644</td>\n",
       "      <td>0.384718</td>\n",
       "      <td>0.450586</td>\n",
       "      <td>0.252057</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.390271</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614270</td>\n",
       "      <td>-0.147442</td>\n",
       "      <td>1.296214</td>\n",
       "      <td>0.211502</td>\n",
       "      <td>0.157114</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.106628</td>\n",
       "      <td>0.088090</td>\n",
       "      <td>0.029938</td>\n",
       "      <td>0.127583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321594</td>\n",
       "      <td>0.311465</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.386289</td>\n",
       "      <td>0.442863</td>\n",
       "      <td>0.251537</td>\n",
       "      <td>0.221754</td>\n",
       "      <td>0.390644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619788</td>\n",
       "      <td>-0.148863</td>\n",
       "      <td>1.306293</td>\n",
       "      <td>0.224825</td>\n",
       "      <td>0.156264</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.079567</td>\n",
       "      <td>0.029953</td>\n",
       "      <td>0.126045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321504</td>\n",
       "      <td>0.311379</td>\n",
       "      <td>0.469090</td>\n",
       "      <td>0.261484</td>\n",
       "      <td>0.231913</td>\n",
       "      <td>0.386717</td>\n",
       "      <td>0.444212</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>0.221688</td>\n",
       "      <td>0.390763</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619716</td>\n",
       "      <td>-0.148812</td>\n",
       "      <td>1.302433</td>\n",
       "      <td>0.224631</td>\n",
       "      <td>0.155941</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>0.116238</td>\n",
       "      <td>0.078291</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.125562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321446</td>\n",
       "      <td>0.311360</td>\n",
       "      <td>0.469092</td>\n",
       "      <td>0.261522</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.387123</td>\n",
       "      <td>0.445079</td>\n",
       "      <td>0.251438</td>\n",
       "      <td>0.222014</td>\n",
       "      <td>0.390721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619620</td>\n",
       "      <td>-0.149545</td>\n",
       "      <td>1.296863</td>\n",
       "      <td>0.226248</td>\n",
       "      <td>0.155132</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>0.077394</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>0.125631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist_0_1  dist_2_1  dist_3_1  dist_4_1  dist_5_1  dist_6_1  dist_7_1  \\\n",
       "0  0.324883  0.314224  0.475169  0.257537  0.224666  0.380845  0.480537   \n",
       "1  0.322438  0.312178  0.470942  0.260437  0.229644  0.384718  0.450586   \n",
       "2  0.321594  0.311465  0.469381  0.261410  0.231567  0.386289  0.442863   \n",
       "3  0.321504  0.311379  0.469090  0.261484  0.231913  0.386717  0.444212   \n",
       "4  0.321446  0.311360  0.469092  0.261522  0.232146  0.387123  0.445079   \n",
       "\n",
       "   dist_8_1  dist_9_1  dist_10_1  ...         5         6         7         8  \\\n",
       "0  0.253687  0.224452   0.390823  ...  1.597459 -0.140846  1.246003  0.169821   \n",
       "1  0.252057  0.222488   0.390271  ...  1.614270 -0.147442  1.296214  0.211502   \n",
       "2  0.251537  0.221754   0.390644  ...  1.619788 -0.148863  1.306293  0.224825   \n",
       "3  0.251446  0.221688   0.390763  ...  1.619716 -0.148812  1.302433  0.224631   \n",
       "4  0.251438  0.222014   0.390721  ...  1.619620 -0.149545  1.296863  0.226248   \n",
       "\n",
       "          9        10        11        12        13        14  \n",
       "0  0.162107 -0.027582  0.081955  0.113375  0.031990  0.131282  \n",
       "1  0.157114 -0.009114  0.106628  0.088090  0.029938  0.127583  \n",
       "2  0.156264 -0.002401  0.115748  0.079567  0.029953  0.126045  \n",
       "3  0.155941 -0.001148  0.116238  0.078291  0.030147  0.125562  \n",
       "4  0.155132 -0.000888  0.115037  0.077394  0.030157  0.125631  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\merged_modified.csv\")\n",
    "data.head()\n",
    "target=data['target']\n",
    "data=data.drop(['target'],axis=1)\n",
    "# data.head()\n",
    "# print(data.shape)\n",
    "# AutoEncoder(data)\n",
    "# data['12']=target\n",
    "# print(data.shape)\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculating scores\n",
    "x=data.to_numpy()\n",
    "y=target.to_numpy()\n",
    "top_k=60\n",
    "\n",
    "ranks = fisher_score.fisher_score(x, y,mode=\"ranks\")\n",
    "\n",
    "\n",
    "feat_importances=pd.Series(ranks, data.columns[0:len(data.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_id=[]\n",
    "#top 26 values\n",
    "# print(feat_importances[0])\n",
    "id=0\n",
    "sorted_feat_importances=[]\n",
    "feat_imp_index=[i for i in feat_importances.index]\n",
    "feat_imp_value=[i for i in feat_importances.values]\n",
    "for i in range(len(feat_importances)):\n",
    "    if(feat_imp_value[i]<top_k):\n",
    "\n",
    "        sorted_id.append(feat_imp_index[i])\n",
    "        sorted_feat_importances.append(feat_imp_value[i])\n",
    "data=data[sorted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_0_1</th>\n",
       "      <th>dist_2_1</th>\n",
       "      <th>dist_3_1</th>\n",
       "      <th>dist_4_1</th>\n",
       "      <th>dist_5_1</th>\n",
       "      <th>dist_6_1</th>\n",
       "      <th>dist_7_1</th>\n",
       "      <th>dist_8_1</th>\n",
       "      <th>dist_9_1</th>\n",
       "      <th>dist_10_1</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324883</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.475169</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>0.224666</td>\n",
       "      <td>0.380845</td>\n",
       "      <td>0.480537</td>\n",
       "      <td>0.253687</td>\n",
       "      <td>0.224452</td>\n",
       "      <td>0.390823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140846</td>\n",
       "      <td>1.246003</td>\n",
       "      <td>0.169821</td>\n",
       "      <td>0.162107</td>\n",
       "      <td>-0.027582</td>\n",
       "      <td>0.081955</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.031990</td>\n",
       "      <td>0.131282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322438</td>\n",
       "      <td>0.312178</td>\n",
       "      <td>0.470942</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.229644</td>\n",
       "      <td>0.384718</td>\n",
       "      <td>0.450586</td>\n",
       "      <td>0.252057</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.390271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147442</td>\n",
       "      <td>1.296214</td>\n",
       "      <td>0.211502</td>\n",
       "      <td>0.157114</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.106628</td>\n",
       "      <td>0.088090</td>\n",
       "      <td>0.029938</td>\n",
       "      <td>0.127583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321594</td>\n",
       "      <td>0.311465</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.386289</td>\n",
       "      <td>0.442863</td>\n",
       "      <td>0.251537</td>\n",
       "      <td>0.221754</td>\n",
       "      <td>0.390644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148863</td>\n",
       "      <td>1.306293</td>\n",
       "      <td>0.224825</td>\n",
       "      <td>0.156264</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.079567</td>\n",
       "      <td>0.029953</td>\n",
       "      <td>0.126045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321504</td>\n",
       "      <td>0.311379</td>\n",
       "      <td>0.469090</td>\n",
       "      <td>0.261484</td>\n",
       "      <td>0.231913</td>\n",
       "      <td>0.386717</td>\n",
       "      <td>0.444212</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>0.221688</td>\n",
       "      <td>0.390763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148812</td>\n",
       "      <td>1.302433</td>\n",
       "      <td>0.224631</td>\n",
       "      <td>0.155941</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>0.116238</td>\n",
       "      <td>0.078291</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.125562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321446</td>\n",
       "      <td>0.311360</td>\n",
       "      <td>0.469092</td>\n",
       "      <td>0.261522</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.387123</td>\n",
       "      <td>0.445079</td>\n",
       "      <td>0.251438</td>\n",
       "      <td>0.222014</td>\n",
       "      <td>0.390721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149545</td>\n",
       "      <td>1.296863</td>\n",
       "      <td>0.226248</td>\n",
       "      <td>0.155132</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>0.077394</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>0.125631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist_0_1  dist_2_1  dist_3_1  dist_4_1  dist_5_1  dist_6_1  dist_7_1  \\\n",
       "0  0.324883  0.314224  0.475169  0.257537  0.224666  0.380845  0.480537   \n",
       "1  0.322438  0.312178  0.470942  0.260437  0.229644  0.384718  0.450586   \n",
       "2  0.321594  0.311465  0.469381  0.261410  0.231567  0.386289  0.442863   \n",
       "3  0.321504  0.311379  0.469090  0.261484  0.231913  0.386717  0.444212   \n",
       "4  0.321446  0.311360  0.469092  0.261522  0.232146  0.387123  0.445079   \n",
       "\n",
       "   dist_8_1  dist_9_1  dist_10_1  ...         6         7         8         9  \\\n",
       "0  0.253687  0.224452   0.390823  ... -0.140846  1.246003  0.169821  0.162107   \n",
       "1  0.252057  0.222488   0.390271  ... -0.147442  1.296214  0.211502  0.157114   \n",
       "2  0.251537  0.221754   0.390644  ... -0.148863  1.306293  0.224825  0.156264   \n",
       "3  0.251446  0.221688   0.390763  ... -0.148812  1.302433  0.224631  0.155941   \n",
       "4  0.251438  0.222014   0.390721  ... -0.149545  1.296863  0.226248  0.155132   \n",
       "\n",
       "         10        11        12        13        14  target  \n",
       "0 -0.027582  0.081955  0.113375  0.031990  0.131282       0  \n",
       "1 -0.009114  0.106628  0.088090  0.029938  0.127583       0  \n",
       "2 -0.002401  0.115748  0.079567  0.029953  0.126045       0  \n",
       "3 -0.001148  0.116238  0.078291  0.030147  0.125562       0  \n",
       "4 -0.000888  0.115037  0.077394  0.030157  0.125631       0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len1=data.shape[1]\n",
    "# l1=list(range(0,len1))\n",
    "# l2=[]\n",
    "# for i in l1:\n",
    "#     i=str(i)\n",
    "#     l2.append(i)\n",
    "# l1=l2\n",
    "    \n",
    "# data.columns=l1\n",
    "# data.head()\n",
    "data['target']=target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_extended_modified.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :60], sequences[end_ix-1, 60:]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 70)\n",
      "pool size  2\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_116 (Conv1D)         (None, 12, 64)            34624     \n",
      "                                                                 \n",
      " conv1d_117 (Conv1D)         (None, 4, 64)             36928     \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 4, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_58 (MaxPoolin  (None, 2, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_57 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 100)               12900     \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,862\n",
      "Trainable params: 85,662\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2175 - accuracy: 0.2023 - val_loss: 2.2824 - val_accuracy: 0.1352\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5164 - accuracy: 0.4350 - val_loss: 2.2823 - val_accuracy: 0.1450\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2655 - accuracy: 0.5322 - val_loss: 2.2700 - val_accuracy: 0.1646\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1756 - accuracy: 0.5725 - val_loss: 2.2487 - val_accuracy: 0.1944\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0954 - accuracy: 0.5968 - val_loss: 2.4036 - val_accuracy: 0.2479\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0461 - accuracy: 0.6139 - val_loss: 2.2382 - val_accuracy: 0.2797\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0014 - accuracy: 0.6277 - val_loss: 2.1204 - val_accuracy: 0.3209\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9175 - accuracy: 0.6606 - val_loss: 2.0904 - val_accuracy: 0.3258\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8805 - accuracy: 0.6753 - val_loss: 2.0248 - val_accuracy: 0.3283\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8394 - accuracy: 0.6864 - val_loss: 1.9319 - val_accuracy: 0.3606\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8205 - accuracy: 0.6961 - val_loss: 1.8502 - val_accuracy: 0.4157\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8068 - accuracy: 0.7042 - val_loss: 2.1358 - val_accuracy: 0.5116\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7726 - accuracy: 0.7178 - val_loss: 1.8238 - val_accuracy: 0.3989\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7095 - accuracy: 0.7392 - val_loss: 1.7598 - val_accuracy: 0.4541\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7126 - accuracy: 0.7410 - val_loss: 1.8726 - val_accuracy: 0.3887\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7364 - accuracy: 0.7285 - val_loss: 1.7467 - val_accuracy: 0.4149\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6598 - accuracy: 0.7601 - val_loss: 1.8244 - val_accuracy: 0.4055\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6480 - accuracy: 0.7670 - val_loss: 1.8922 - val_accuracy: 0.4463\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6240 - accuracy: 0.7739 - val_loss: 1.7099 - val_accuracy: 0.4275\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5877 - accuracy: 0.7870 - val_loss: 1.6493 - val_accuracy: 0.4557\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5627 - accuracy: 0.7973 - val_loss: 1.5405 - val_accuracy: 0.5039\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5891 - accuracy: 0.7830 - val_loss: 1.4443 - val_accuracy: 0.5729\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5276 - accuracy: 0.8071 - val_loss: 1.4857 - val_accuracy: 0.5774\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5325 - accuracy: 0.8098 - val_loss: 1.2870 - val_accuracy: 0.6452\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4973 - accuracy: 0.8228 - val_loss: 1.2548 - val_accuracy: 0.6717\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5353 - accuracy: 0.8060 - val_loss: 1.1625 - val_accuracy: 0.7121\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4635 - accuracy: 0.8342 - val_loss: 1.2940 - val_accuracy: 0.5998\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4928 - accuracy: 0.8253 - val_loss: 1.3310 - val_accuracy: 0.5729\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4514 - accuracy: 0.8398 - val_loss: 1.2268 - val_accuracy: 0.6721\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4400 - accuracy: 0.8409 - val_loss: 1.5875 - val_accuracy: 0.5272\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4176 - accuracy: 0.8489 - val_loss: 1.2449 - val_accuracy: 0.6660\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3997 - accuracy: 0.8515 - val_loss: 1.1506 - val_accuracy: 0.6844\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4265 - accuracy: 0.8490 - val_loss: 1.0060 - val_accuracy: 0.7391\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4092 - accuracy: 0.8514 - val_loss: 1.0796 - val_accuracy: 0.7207\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3880 - accuracy: 0.8597 - val_loss: 1.1895 - val_accuracy: 0.6546\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3840 - accuracy: 0.8649 - val_loss: 1.1273 - val_accuracy: 0.6868\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3488 - accuracy: 0.8764 - val_loss: 0.9765 - val_accuracy: 0.7383\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3036 - accuracy: 0.8902 - val_loss: 1.0606 - val_accuracy: 0.7407\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3572 - accuracy: 0.8736 - val_loss: 0.9222 - val_accuracy: 0.7624\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3313 - accuracy: 0.8826 - val_loss: 0.7918 - val_accuracy: 0.8060\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3371 - accuracy: 0.8783 - val_loss: 0.8132 - val_accuracy: 0.7905\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3252 - accuracy: 0.8879 - val_loss: 0.8271 - val_accuracy: 0.8216\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2487 - accuracy: 0.9093 - val_loss: 0.9405 - val_accuracy: 0.7317\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2862 - accuracy: 0.8953 - val_loss: 1.0152 - val_accuracy: 0.7419\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3055 - accuracy: 0.8917 - val_loss: 1.0279 - val_accuracy: 0.7089\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2918 - accuracy: 0.8955 - val_loss: 0.7210 - val_accuracy: 0.8314\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2618 - accuracy: 0.9087 - val_loss: 0.7018 - val_accuracy: 0.8224\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2537 - accuracy: 0.9121 - val_loss: 0.8497 - val_accuracy: 0.7538\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2553 - accuracy: 0.9117 - val_loss: 0.8085 - val_accuracy: 0.7783\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2773 - accuracy: 0.8997 - val_loss: 0.8276 - val_accuracy: 0.7787\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2657 - accuracy: 0.9083 - val_loss: 0.7029 - val_accuracy: 0.7942\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2243 - accuracy: 0.9236 - val_loss: 0.8116 - val_accuracy: 0.7636\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2622 - accuracy: 0.9051 - val_loss: 0.5833 - val_accuracy: 0.8530\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2636 - accuracy: 0.9086 - val_loss: 0.6218 - val_accuracy: 0.8154\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2078 - accuracy: 0.9284 - val_loss: 0.5998 - val_accuracy: 0.8485\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2059 - accuracy: 0.9263 - val_loss: 0.7648 - val_accuracy: 0.7272\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2082 - accuracy: 0.9274 - val_loss: 0.9318 - val_accuracy: 0.6668\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2935 - accuracy: 0.8958 - val_loss: 1.0288 - val_accuracy: 0.8220\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2032 - accuracy: 0.9260 - val_loss: 1.0609 - val_accuracy: 0.8138\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2048 - accuracy: 0.9317 - val_loss: 0.7516 - val_accuracy: 0.7934\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1838 - accuracy: 0.9362 - val_loss: 0.5504 - val_accuracy: 0.8440\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2171 - accuracy: 0.9260 - val_loss: 0.6286 - val_accuracy: 0.8738\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2192 - accuracy: 0.9227 - val_loss: 1.2075 - val_accuracy: 0.8048\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1751 - accuracy: 0.9401 - val_loss: 1.8986 - val_accuracy: 0.8628\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2088 - accuracy: 0.9260 - val_loss: 0.3477 - val_accuracy: 0.9155\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1919 - accuracy: 0.9336 - val_loss: 0.4880 - val_accuracy: 0.8506\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2134 - accuracy: 0.9244 - val_loss: 0.5154 - val_accuracy: 0.8987\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1758 - accuracy: 0.9358 - val_loss: 0.9761 - val_accuracy: 0.8628\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1832 - accuracy: 0.9360 - val_loss: 0.6251 - val_accuracy: 0.7958\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2048 - accuracy: 0.9282 - val_loss: 0.5685 - val_accuracy: 0.8085\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1705 - accuracy: 0.9392 - val_loss: 0.5709 - val_accuracy: 0.8187\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1906 - accuracy: 0.9352 - val_loss: 0.6801 - val_accuracy: 0.8383\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1591 - accuracy: 0.9444 - val_loss: 0.8927 - val_accuracy: 0.8040\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2084 - accuracy: 0.9278 - val_loss: 0.8379 - val_accuracy: 0.8812\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2189 - accuracy: 0.9231 - val_loss: 0.6643 - val_accuracy: 0.8261\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1730 - accuracy: 0.9394 - val_loss: 1.0442 - val_accuracy: 0.8910\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1583 - accuracy: 0.9466 - val_loss: 0.5353 - val_accuracy: 0.8681\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1680 - accuracy: 0.9421 - val_loss: 0.4409 - val_accuracy: 0.8836\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1667 - accuracy: 0.9404 - val_loss: 0.4479 - val_accuracy: 0.8657\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1590 - accuracy: 0.9453 - val_loss: 0.4271 - val_accuracy: 0.8853\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1382 - accuracy: 0.9545 - val_loss: 0.3297 - val_accuracy: 0.9236\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1656 - accuracy: 0.9430 - val_loss: 0.4497 - val_accuracy: 0.8861\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1538 - accuracy: 0.9481 - val_loss: 0.2820 - val_accuracy: 0.9473\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1356 - accuracy: 0.9521 - val_loss: 0.4727 - val_accuracy: 0.9004\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1561 - accuracy: 0.9467 - val_loss: 0.4005 - val_accuracy: 0.9383\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1886 - accuracy: 0.9366 - val_loss: 0.4913 - val_accuracy: 0.9089\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1588 - accuracy: 0.9434 - val_loss: 0.5581 - val_accuracy: 0.8938\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1371 - accuracy: 0.9530 - val_loss: 0.4535 - val_accuracy: 0.9208\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1778 - accuracy: 0.9384 - val_loss: 0.7005 - val_accuracy: 0.8832\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1969 - accuracy: 0.9317 - val_loss: 0.5021 - val_accuracy: 0.9171\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1420 - accuracy: 0.9514 - val_loss: 0.5581 - val_accuracy: 0.9175\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1150 - accuracy: 0.9609 - val_loss: 0.5819 - val_accuracy: 0.9310\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1275 - accuracy: 0.9575 - val_loss: 0.4474 - val_accuracy: 0.9081\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1439 - accuracy: 0.9529 - val_loss: 0.3301 - val_accuracy: 0.9334\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1153 - accuracy: 0.9592 - val_loss: 0.3833 - val_accuracy: 0.9171\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1767 - accuracy: 0.9404 - val_loss: 0.5399 - val_accuracy: 0.8787\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1270 - accuracy: 0.9564 - val_loss: 0.2838 - val_accuracy: 0.9526\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1682 - accuracy: 0.9416 - val_loss: 0.5092 - val_accuracy: 0.9285\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1464 - accuracy: 0.9484 - val_loss: 1.0710 - val_accuracy: 0.8751\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1492 - accuracy: 0.9464 - val_loss: 0.5101 - val_accuracy: 0.9359\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5101 - accuracy: 0.9359\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[5 3 9 ... 9 2 6]\n",
      "Confusion matrix\n",
      " [[242   0   0   0   0   0  15   0   8   5]\n",
      " [  3 239   6   2   0   0   4   0   1   1]\n",
      " [  0   0 227   3   0   0  21   0   0   0]\n",
      " [  0   3   1 235   0   0   0   0   1   0]\n",
      " [  0   0   0   0 233   0  11   0   0   5]\n",
      " [  0   0   0   0   0 206  21   0   0   3]\n",
      " [  0   0   0   0   0   0 267   0   1   0]\n",
      " [  0   0   0   0   2   2  22 201   8   5]\n",
      " [  0   0   0   0   0   0   2   0 252   0]\n",
      " [  0   0   0   0   0   0   1   0   0 190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       270\n",
      "           1       0.99      0.93      0.96       256\n",
      "           2       0.97      0.90      0.94       251\n",
      "           3       0.98      0.98      0.98       240\n",
      "           4       0.99      0.94      0.96       249\n",
      "           5       0.99      0.90      0.94       230\n",
      "           6       0.73      1.00      0.84       268\n",
      "           7       1.00      0.84      0.91       240\n",
      "           8       0.93      0.99      0.96       254\n",
      "           9       0.91      0.99      0.95       191\n",
      "\n",
      "    accuracy                           0.94      2449\n",
      "   macro avg       0.95      0.94      0.94      2449\n",
      "weighted avg       0.95      0.94      0.94      2449\n",
      "\n",
      "(12260, 70)\n",
      "pool size  3\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_118 (Conv1D)         (None, 12, 64)            34624     \n",
      "                                                                 \n",
      " conv1d_119 (Conv1D)         (None, 4, 64)             36928     \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 4, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_59 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_58 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 100)               6500      \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,462\n",
      "Trainable params: 79,262\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 9ms/step - loss: 2.1273 - accuracy: 0.2266 - val_loss: 2.2579 - val_accuracy: 0.1290\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5229 - accuracy: 0.4292 - val_loss: 2.2690 - val_accuracy: 0.1217\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2872 - accuracy: 0.5328 - val_loss: 2.2495 - val_accuracy: 0.1262\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1563 - accuracy: 0.5789 - val_loss: 2.2329 - val_accuracy: 0.1543\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0717 - accuracy: 0.6175 - val_loss: 2.3653 - val_accuracy: 0.2634\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0211 - accuracy: 0.6284 - val_loss: 2.3223 - val_accuracy: 0.1976\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9526 - accuracy: 0.6547 - val_loss: 2.2354 - val_accuracy: 0.2160\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9226 - accuracy: 0.6645 - val_loss: 2.2195 - val_accuracy: 0.2385\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8694 - accuracy: 0.6849 - val_loss: 2.1930 - val_accuracy: 0.2879\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8391 - accuracy: 0.6983 - val_loss: 2.1602 - val_accuracy: 0.2544\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7954 - accuracy: 0.7125 - val_loss: 2.1569 - val_accuracy: 0.2650\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7615 - accuracy: 0.7279 - val_loss: 2.1132 - val_accuracy: 0.2944\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7503 - accuracy: 0.7347 - val_loss: 2.1307 - val_accuracy: 0.2564\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7287 - accuracy: 0.7410 - val_loss: 2.0271 - val_accuracy: 0.3246\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7378 - accuracy: 0.7347 - val_loss: 1.9393 - val_accuracy: 0.3438\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6815 - accuracy: 0.7558 - val_loss: 2.0331 - val_accuracy: 0.4014\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6432 - accuracy: 0.7666 - val_loss: 1.9065 - val_accuracy: 0.4222\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7921 - val_loss: 1.8973 - val_accuracy: 0.3173\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5909 - accuracy: 0.7817 - val_loss: 1.9518 - val_accuracy: 0.3291\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5590 - accuracy: 0.7993 - val_loss: 1.8682 - val_accuracy: 0.3589\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5269 - accuracy: 0.8104 - val_loss: 1.8012 - val_accuracy: 0.4173\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5310 - accuracy: 0.8066 - val_loss: 1.8906 - val_accuracy: 0.3356\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4928 - accuracy: 0.8199 - val_loss: 1.8013 - val_accuracy: 0.4398\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4449 - accuracy: 0.8373 - val_loss: 1.7271 - val_accuracy: 0.4332\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4663 - accuracy: 0.8322 - val_loss: 1.7700 - val_accuracy: 0.3797\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4320 - accuracy: 0.8438 - val_loss: 1.6721 - val_accuracy: 0.4145\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4277 - accuracy: 0.8467 - val_loss: 1.5496 - val_accuracy: 0.4524\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3993 - accuracy: 0.8606 - val_loss: 1.4644 - val_accuracy: 0.5725\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3751 - accuracy: 0.8674 - val_loss: 1.4561 - val_accuracy: 0.5259\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3832 - accuracy: 0.8674 - val_loss: 1.4976 - val_accuracy: 0.5022\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3732 - accuracy: 0.8639 - val_loss: 1.4397 - val_accuracy: 0.5913\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3489 - accuracy: 0.8767 - val_loss: 1.2653 - val_accuracy: 0.6897\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3222 - accuracy: 0.8873 - val_loss: 1.1250 - val_accuracy: 0.7052\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2934 - accuracy: 0.8942 - val_loss: 1.0624 - val_accuracy: 0.7534\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2925 - accuracy: 0.8959 - val_loss: 1.0489 - val_accuracy: 0.7468\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3441 - accuracy: 0.8738 - val_loss: 1.1755 - val_accuracy: 0.6378\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2748 - accuracy: 0.9021 - val_loss: 0.9792 - val_accuracy: 0.7615\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3041 - accuracy: 0.8899 - val_loss: 0.8530 - val_accuracy: 0.7934\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2841 - accuracy: 0.8986 - val_loss: 0.9957 - val_accuracy: 0.7207\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2570 - accuracy: 0.9072 - val_loss: 0.9943 - val_accuracy: 0.7178\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2633 - accuracy: 0.9072 - val_loss: 0.9427 - val_accuracy: 0.7685\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3092 - accuracy: 0.8897 - val_loss: 0.9952 - val_accuracy: 0.7301\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3026 - accuracy: 0.8921 - val_loss: 0.6938 - val_accuracy: 0.8338\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2238 - accuracy: 0.9209 - val_loss: 0.6429 - val_accuracy: 0.8420\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2052 - accuracy: 0.9300 - val_loss: 0.6146 - val_accuracy: 0.8575\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2369 - accuracy: 0.9154 - val_loss: 0.6702 - val_accuracy: 0.8252\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2672 - accuracy: 0.9049 - val_loss: 0.6445 - val_accuracy: 0.8795\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2629 - accuracy: 0.9050 - val_loss: 0.6634 - val_accuracy: 0.8604\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2264 - accuracy: 0.9204 - val_loss: 0.7173 - val_accuracy: 0.7848\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2207 - accuracy: 0.9223 - val_loss: 0.8221 - val_accuracy: 0.7448\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2113 - accuracy: 0.9296 - val_loss: 0.9155 - val_accuracy: 0.7174\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2349 - accuracy: 0.9197 - val_loss: 0.8062 - val_accuracy: 0.8056\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1668 - accuracy: 0.9428 - val_loss: 0.9216 - val_accuracy: 0.7566\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2273 - accuracy: 0.9223 - val_loss: 0.7597 - val_accuracy: 0.8228\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2149 - accuracy: 0.9247 - val_loss: 0.8247 - val_accuracy: 0.7799\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1891 - accuracy: 0.9352 - val_loss: 0.8017 - val_accuracy: 0.7562\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1927 - accuracy: 0.9348 - val_loss: 0.8536 - val_accuracy: 0.7452\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2044 - accuracy: 0.9289 - val_loss: 0.9155 - val_accuracy: 0.7717\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1907 - accuracy: 0.9354 - val_loss: 0.9422 - val_accuracy: 0.7178\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1901 - accuracy: 0.9331 - val_loss: 0.8471 - val_accuracy: 0.7619\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2250 - accuracy: 0.9232 - val_loss: 1.0228 - val_accuracy: 0.6868\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9328 - val_loss: 1.0357 - val_accuracy: 0.7583\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1837 - accuracy: 0.9379 - val_loss: 0.9001 - val_accuracy: 0.7272\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1549 - accuracy: 0.9475 - val_loss: 0.8565 - val_accuracy: 0.7493\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1926 - accuracy: 0.9334 - val_loss: 0.8852 - val_accuracy: 0.7309\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1848 - accuracy: 0.9367 - val_loss: 1.0806 - val_accuracy: 0.6325\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1849 - accuracy: 0.9365 - val_loss: 0.9845 - val_accuracy: 0.6982\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1682 - accuracy: 0.9469 - val_loss: 1.1465 - val_accuracy: 0.5623\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1503 - accuracy: 0.9484 - val_loss: 1.0899 - val_accuracy: 0.6137\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1901 - accuracy: 0.9371 - val_loss: 0.9150 - val_accuracy: 0.7485\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1777 - accuracy: 0.9413 - val_loss: 0.9043 - val_accuracy: 0.7452\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2048 - accuracy: 0.9305 - val_loss: 0.9872 - val_accuracy: 0.6782\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1397 - accuracy: 0.9511 - val_loss: 0.7930 - val_accuracy: 0.7717\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1665 - accuracy: 0.9423 - val_loss: 0.6394 - val_accuracy: 0.8428\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1289 - accuracy: 0.9571 - val_loss: 0.6483 - val_accuracy: 0.8065\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1263 - accuracy: 0.9582 - val_loss: 0.6704 - val_accuracy: 0.7962\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1533 - accuracy: 0.9480 - val_loss: 0.7126 - val_accuracy: 0.7575\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1519 - accuracy: 0.9484 - val_loss: 0.7181 - val_accuracy: 0.7881\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1273 - accuracy: 0.9567 - val_loss: 0.6485 - val_accuracy: 0.8036\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1547 - accuracy: 0.9494 - val_loss: 0.8461 - val_accuracy: 0.7305\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1794 - accuracy: 0.9361 - val_loss: 0.9511 - val_accuracy: 0.6378\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1252 - accuracy: 0.9576 - val_loss: 0.8484 - val_accuracy: 0.7142\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1479 - accuracy: 0.9494 - val_loss: 0.9048 - val_accuracy: 0.7166\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1631 - accuracy: 0.9415 - val_loss: 0.6924 - val_accuracy: 0.8069\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1632 - accuracy: 0.9440 - val_loss: 0.9951 - val_accuracy: 0.7142\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1521 - accuracy: 0.9477 - val_loss: 0.5765 - val_accuracy: 0.8416\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1289 - accuracy: 0.9559 - val_loss: 0.7891 - val_accuracy: 0.7493\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1252 - accuracy: 0.9559 - val_loss: 0.7917 - val_accuracy: 0.7207\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1775 - accuracy: 0.9395 - val_loss: 0.8499 - val_accuracy: 0.7191\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1269 - accuracy: 0.9569 - val_loss: 0.7751 - val_accuracy: 0.7252\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1371 - accuracy: 0.9536 - val_loss: 0.7636 - val_accuracy: 0.7379\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1421 - accuracy: 0.9515 - val_loss: 0.8492 - val_accuracy: 0.7121\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1551 - accuracy: 0.9463 - val_loss: 0.9060 - val_accuracy: 0.6709\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1524 - accuracy: 0.9480 - val_loss: 0.9725 - val_accuracy: 0.6158\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1338 - accuracy: 0.9541 - val_loss: 0.5728 - val_accuracy: 0.7971\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0964 - accuracy: 0.9674 - val_loss: 0.4969 - val_accuracy: 0.8261\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1277 - accuracy: 0.9559 - val_loss: 0.6524 - val_accuracy: 0.7603\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1108 - accuracy: 0.9614 - val_loss: 0.6326 - val_accuracy: 0.7762\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1345 - accuracy: 0.9553 - val_loss: 0.6644 - val_accuracy: 0.7717\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1511 - accuracy: 0.9480 - val_loss: 0.6378 - val_accuracy: 0.7958\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.7958\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[9 6 6 ... 8 0 7]\n",
      "Confusion matrix\n",
      " [[222   0  18   8  19   0   4   0   0   0]\n",
      " [  0 199  20   7   0   0  26   0   0   0]\n",
      " [  0   0 236   0   0   0  35   0   0   0]\n",
      " [  0   0  16 234   0   0   3   0   0   0]\n",
      " [  0   0   5   0 240   0  43   0   0   0]\n",
      " [  1   0  11   0   2 118  70   4   0   0]\n",
      " [  0   0   0   0   0   0 256   0   0   0]\n",
      " [  0   0  17   0   0   0  45 155   0   0]\n",
      " [  0   0   8   0  11   0  47  10 168   3]\n",
      " [  0   0   2   0  12   0  52   1   0 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90       271\n",
      "           1       1.00      0.79      0.88       252\n",
      "           2       0.71      0.87      0.78       271\n",
      "           3       0.94      0.92      0.93       253\n",
      "           4       0.85      0.83      0.84       288\n",
      "           5       1.00      0.57      0.73       206\n",
      "           6       0.44      1.00      0.61       256\n",
      "           7       0.91      0.71      0.80       217\n",
      "           8       1.00      0.68      0.81       247\n",
      "           9       0.98      0.64      0.78       188\n",
      "\n",
      "    accuracy                           0.80      2449\n",
      "   macro avg       0.88      0.78      0.81      2449\n",
      "weighted avg       0.87      0.80      0.81      2449\n",
      "\n",
      "(12260, 70)\n",
      "pool size  4\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_120 (Conv1D)         (None, 12, 64)            34624     \n",
      "                                                                 \n",
      " conv1d_121 (Conv1D)         (None, 4, 64)             36928     \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 4, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_60 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_59 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 100)               6500      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,462\n",
      "Trainable params: 79,262\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.0364 - accuracy: 0.2665 - val_loss: 2.4174 - val_accuracy: 0.1364\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.4168 - accuracy: 0.4847 - val_loss: 2.2371 - val_accuracy: 0.1682\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2003 - accuracy: 0.5673 - val_loss: 2.2109 - val_accuracy: 0.2013\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0964 - accuracy: 0.5984 - val_loss: 2.3085 - val_accuracy: 0.1294\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0362 - accuracy: 0.6244 - val_loss: 2.2099 - val_accuracy: 0.1940\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9872 - accuracy: 0.6423 - val_loss: 2.1793 - val_accuracy: 0.2070\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9168 - accuracy: 0.6615 - val_loss: 2.2045 - val_accuracy: 0.2250\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8830 - accuracy: 0.6776 - val_loss: 2.1739 - val_accuracy: 0.2572\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8292 - accuracy: 0.6970 - val_loss: 2.0770 - val_accuracy: 0.3483\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8012 - accuracy: 0.7096 - val_loss: 2.0150 - val_accuracy: 0.3201\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7609 - accuracy: 0.7275 - val_loss: 2.0314 - val_accuracy: 0.3144\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7168 - accuracy: 0.7393 - val_loss: 1.8819 - val_accuracy: 0.4100\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7155 - accuracy: 0.7408 - val_loss: 2.0401 - val_accuracy: 0.3491\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6748 - accuracy: 0.7527 - val_loss: 1.8915 - val_accuracy: 0.4083\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6436 - accuracy: 0.7653 - val_loss: 1.9573 - val_accuracy: 0.3218\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6481 - accuracy: 0.7637 - val_loss: 1.8725 - val_accuracy: 0.2973\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6007 - accuracy: 0.7800 - val_loss: 1.9136 - val_accuracy: 0.3830\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6070 - accuracy: 0.7792 - val_loss: 1.7224 - val_accuracy: 0.5574\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5501 - accuracy: 0.8021 - val_loss: 1.7270 - val_accuracy: 0.4806\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5249 - accuracy: 0.8105 - val_loss: 1.4116 - val_accuracy: 0.6448\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5390 - accuracy: 0.8045 - val_loss: 1.4723 - val_accuracy: 0.5537\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4911 - accuracy: 0.8243 - val_loss: 1.2917 - val_accuracy: 0.6431\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5707 - accuracy: 0.7967 - val_loss: 1.5768 - val_accuracy: 0.4867\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4877 - accuracy: 0.8255 - val_loss: 1.5730 - val_accuracy: 0.5267\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5019 - accuracy: 0.8167 - val_loss: 1.3636 - val_accuracy: 0.5794\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4765 - accuracy: 0.8276 - val_loss: 1.5612 - val_accuracy: 0.5517\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4128 - accuracy: 0.8492 - val_loss: 1.4625 - val_accuracy: 0.5921\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3912 - accuracy: 0.8632 - val_loss: 1.2622 - val_accuracy: 0.7060\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3924 - accuracy: 0.8589 - val_loss: 1.3772 - val_accuracy: 0.5786\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3437 - accuracy: 0.8784 - val_loss: 1.2366 - val_accuracy: 0.6513\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3376 - accuracy: 0.8812 - val_loss: 1.2857 - val_accuracy: 0.6721\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3477 - accuracy: 0.8752 - val_loss: 0.9411 - val_accuracy: 0.7795\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3177 - accuracy: 0.8863 - val_loss: 1.2383 - val_accuracy: 0.6991\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2820 - accuracy: 0.8997 - val_loss: 1.2112 - val_accuracy: 0.6541\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3196 - accuracy: 0.8851 - val_loss: 1.0057 - val_accuracy: 0.7264\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2684 - accuracy: 0.9067 - val_loss: 1.3003 - val_accuracy: 0.6019\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2525 - accuracy: 0.9090 - val_loss: 1.1671 - val_accuracy: 0.6721\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2682 - accuracy: 0.9073 - val_loss: 1.1786 - val_accuracy: 0.6129\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2834 - accuracy: 0.8989 - val_loss: 0.9672 - val_accuracy: 0.6701\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2606 - accuracy: 0.9084 - val_loss: 1.2259 - val_accuracy: 0.5247\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2282 - accuracy: 0.9210 - val_loss: 1.0389 - val_accuracy: 0.6362\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2534 - accuracy: 0.9119 - val_loss: 1.0025 - val_accuracy: 0.7068\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2451 - accuracy: 0.9180 - val_loss: 0.9803 - val_accuracy: 0.7117\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2098 - accuracy: 0.9294 - val_loss: 1.2715 - val_accuracy: 0.5664\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2058 - accuracy: 0.9288 - val_loss: 1.1579 - val_accuracy: 0.6448\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2066 - accuracy: 0.9320 - val_loss: 1.1458 - val_accuracy: 0.6178\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2295 - accuracy: 0.9181 - val_loss: 0.8851 - val_accuracy: 0.7146\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1624 - accuracy: 0.9456 - val_loss: 0.7459 - val_accuracy: 0.7566\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1718 - accuracy: 0.9387 - val_loss: 0.9451 - val_accuracy: 0.6513\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1580 - accuracy: 0.9442 - val_loss: 0.9383 - val_accuracy: 0.7097\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1793 - accuracy: 0.9406 - val_loss: 0.7637 - val_accuracy: 0.7497\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1695 - accuracy: 0.9415 - val_loss: 0.7847 - val_accuracy: 0.7436\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9339 - val_loss: 0.7618 - val_accuracy: 0.7379\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1405 - accuracy: 0.9541 - val_loss: 0.7091 - val_accuracy: 0.7717\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9338 - val_loss: 0.6345 - val_accuracy: 0.7877\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2312 - accuracy: 0.9194 - val_loss: 0.7703 - val_accuracy: 0.7219\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1590 - accuracy: 0.9456 - val_loss: 0.7446 - val_accuracy: 0.7309\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1441 - accuracy: 0.9507 - val_loss: 0.5274 - val_accuracy: 0.8432\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1632 - accuracy: 0.9456 - val_loss: 0.5175 - val_accuracy: 0.8567\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2056 - accuracy: 0.9271 - val_loss: 0.6276 - val_accuracy: 0.7995\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1641 - accuracy: 0.9424 - val_loss: 0.8499 - val_accuracy: 0.7101\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1516 - accuracy: 0.9478 - val_loss: 0.7482 - val_accuracy: 0.7477\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1532 - accuracy: 0.9505 - val_loss: 1.0770 - val_accuracy: 0.6064\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1517 - accuracy: 0.9478 - val_loss: 0.8127 - val_accuracy: 0.7595\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1229 - accuracy: 0.9570 - val_loss: 0.7297 - val_accuracy: 0.7530\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1231 - accuracy: 0.9587 - val_loss: 0.7578 - val_accuracy: 0.7395\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1001 - accuracy: 0.9669 - val_loss: 0.7103 - val_accuracy: 0.7971\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1925 - accuracy: 0.9376 - val_loss: 0.5847 - val_accuracy: 0.8567\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1192 - accuracy: 0.9609 - val_loss: 0.9126 - val_accuracy: 0.6778\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1304 - accuracy: 0.9544 - val_loss: 1.1513 - val_accuracy: 0.5651\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1379 - accuracy: 0.9532 - val_loss: 0.7853 - val_accuracy: 0.7240\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1722 - accuracy: 0.9425 - val_loss: 0.6491 - val_accuracy: 0.8011\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1445 - accuracy: 0.9547 - val_loss: 0.6508 - val_accuracy: 0.7967\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1460 - accuracy: 0.9506 - val_loss: 0.8276 - val_accuracy: 0.7187\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1159 - accuracy: 0.9631 - val_loss: 1.0473 - val_accuracy: 0.6497\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1091 - accuracy: 0.9627 - val_loss: 0.7892 - val_accuracy: 0.7358\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1200 - accuracy: 0.9583 - val_loss: 0.8749 - val_accuracy: 0.7187\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1647 - accuracy: 0.9438 - val_loss: 0.5626 - val_accuracy: 0.8387\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0973 - accuracy: 0.9686 - val_loss: 0.4416 - val_accuracy: 0.8840\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0809 - accuracy: 0.9730 - val_loss: 0.5241 - val_accuracy: 0.8395\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0966 - accuracy: 0.9657 - val_loss: 0.4249 - val_accuracy: 0.8591\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1201 - accuracy: 0.9589 - val_loss: 0.4521 - val_accuracy: 0.8636\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1543 - accuracy: 0.9480 - val_loss: 0.6901 - val_accuracy: 0.7660\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1299 - accuracy: 0.9574 - val_loss: 0.5866 - val_accuracy: 0.8212\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0919 - accuracy: 0.9698 - val_loss: 0.7164 - val_accuracy: 0.7726\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0929 - accuracy: 0.9683 - val_loss: 0.6413 - val_accuracy: 0.7905\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0874 - accuracy: 0.9689 - val_loss: 0.6522 - val_accuracy: 0.7701\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1250 - accuracy: 0.9590 - val_loss: 0.6110 - val_accuracy: 0.7934\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0928 - accuracy: 0.9696 - val_loss: 0.7299 - val_accuracy: 0.7546\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1291 - accuracy: 0.9550 - val_loss: 0.6892 - val_accuracy: 0.7579\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0750 - accuracy: 0.9757 - val_loss: 0.7023 - val_accuracy: 0.7628\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0879 - accuracy: 0.9702 - val_loss: 1.0145 - val_accuracy: 0.6586\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1317 - accuracy: 0.9571 - val_loss: 0.9595 - val_accuracy: 0.7011\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1344 - accuracy: 0.9553 - val_loss: 0.7560 - val_accuracy: 0.8097\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0775 - accuracy: 0.9731 - val_loss: 0.7368 - val_accuracy: 0.7918\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1357 - accuracy: 0.9553 - val_loss: 0.7343 - val_accuracy: 0.7709\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1352 - accuracy: 0.9531 - val_loss: 0.5628 - val_accuracy: 0.8440\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0808 - accuracy: 0.9730 - val_loss: 0.4368 - val_accuracy: 0.9371\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0690 - accuracy: 0.9778 - val_loss: 0.5124 - val_accuracy: 0.8526\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0764 - accuracy: 0.9752 - val_loss: 0.7018 - val_accuracy: 0.7975\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7018 - accuracy: 0.7975\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[2 5 2 ... 2 4 1]\n",
      "Confusion matrix\n",
      " [[214   3  13   5   0   5   7   0   0   0]\n",
      " [  0 251   8   2   0   0   1   0   0   0]\n",
      " [  1   9 275   1   0   0   0   0   0   1]\n",
      " [  0  15   9 236   0   0   0   0   0   0]\n",
      " [  9   0   4   0 203  23  24   0   2   1]\n",
      " [  0   0   2   0   0 184   0   0   0   0]\n",
      " [  0   0   4   3   0  32 216   0   0   0]\n",
      " [  0   0  12  14   0  61   8 147   0   1]\n",
      " [  1  44  68  17   0  10  10   0  99   2]\n",
      " [  0   0  22   0   6  28   5   0   3 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       247\n",
      "           1       0.78      0.96      0.86       262\n",
      "           2       0.66      0.96      0.78       287\n",
      "           3       0.85      0.91      0.88       260\n",
      "           4       0.97      0.76      0.85       266\n",
      "           5       0.54      0.99      0.70       186\n",
      "           6       0.80      0.85      0.82       255\n",
      "           7       1.00      0.60      0.75       243\n",
      "           8       0.95      0.39      0.56       251\n",
      "           9       0.96      0.67      0.79       192\n",
      "\n",
      "    accuracy                           0.80      2449\n",
      "   macro avg       0.85      0.80      0.79      2449\n",
      "weighted avg       0.85      0.80      0.79      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed_modified\", \"*\", \"*.csv\"))\n",
    "# # print(raw_angle_files_1)\n",
    "# all_filenames = [i for i in raw_angle_files_1]\n",
    "# df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "# data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\tree_classifier_extracted.csv\")\n",
    "# print(df.shape)\n",
    "# df = df.sample(frac = 1)\n",
    "# # df.iloc[1:10,:]\n",
    "\n",
    "\n",
    "list_train_accuracy=[]\n",
    "list_val_accuracy=[]\n",
    "for pool in range(2,5):\n",
    "    data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_extended_modified.csv\")\n",
    "    data=pd.get_dummies(data,columns=['target'])\n",
    "    data=data.to_numpy()\n",
    "    x,y=split_sequences(data,20)\n",
    "    # x=x[None:]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # data=data.to_numpy()\n",
    "    print(data.shape)\n",
    "\n",
    "    # y=data.iloc[:,26:]\n",
    "    # x=data.iloc[:,:26]\n",
    "    # # x=data[-1:26]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # x=x.to_numpy()\n",
    "    # y=y.to_numpy()\n",
    "    print(\"pool size \",pool)\n",
    "    x_train,x_test,y_train,y_test= train_test_split(x, y, test_size = 0.2)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(type(y_train))\n",
    "    # print(\" kernel size : \",filter_size)\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, activation='relu', input_shape=(20,60)))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.MaxPooling1D(pool_size=pool))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    # # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # print(y_train)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # # print(x.shape)\n",
    "    history = model.fit(x_train, y_train,batch_size=config.BATCH_SIZE,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    y_pred=model.predict(x_test)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    y_test=np.argmax(y_test,axis=1)\n",
    "    list_train_accuracy.append(history.history[\"accuracy\"][-1])\n",
    "    list_val_accuracy.append(history.history[\"val_accuracy\"][-1])\n",
    "    print(y_pred)\n",
    "    cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "    print('Confusion matrix\\n',cf_matrix)\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9782475233078003,\n",
       " 0.9796772599220276,\n",
       " 0.9620097875595093,\n",
       " 0.9535334706306458,\n",
       " 0.9728349447250366]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_train_accuracy\n",
    "# list_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.plot( range( 2, 7 ),list_train_accuracy, marker = 's' )\n",
    "plt.plot( range( 2, 7),list_val_accuracy, marker = 's' )\n",
    "plt.xlabel( 'pool Size', fontsize = 17 )\n",
    "plt.ylabel( 'Accuracy', fontsize = 18 )\n",
    "plt.legend(['Train Set', 'Test Set'], loc='best',fontsize = 14)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks( fontsize = 14 )\n",
    "plt.yticks( fontsize = 14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  2\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_98 (Conv1D)          (None, 19, 64)            7744      \n",
      "                                                                 \n",
      " conv1d_99 (Conv1D)          (None, 18, 64)            8256      \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 18, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_49 (MaxPoolin  (None, 9, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_48 (Flatten)        (None, 576)               0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 100)               57700     \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,110\n",
      "Trainable params: 74,910\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.0081 - accuracy: 0.2644 - val_loss: 2.2763 - val_accuracy: 0.1515\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2931 - accuracy: 0.5450 - val_loss: 2.2839 - val_accuracy: 0.1392\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0399 - accuracy: 0.6358 - val_loss: 2.2765 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9163 - accuracy: 0.6734 - val_loss: 2.2476 - val_accuracy: 0.1882\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7937 - accuracy: 0.7191 - val_loss: 2.2617 - val_accuracy: 0.1797\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7321 - accuracy: 0.7421 - val_loss: 2.2349 - val_accuracy: 0.2070\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6626 - accuracy: 0.7603 - val_loss: 2.1886 - val_accuracy: 0.2070\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6218 - accuracy: 0.7780 - val_loss: 2.1671 - val_accuracy: 0.2593\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5621 - accuracy: 0.7987 - val_loss: 2.0658 - val_accuracy: 0.2544\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5199 - accuracy: 0.8153 - val_loss: 1.9895 - val_accuracy: 0.3095\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4879 - accuracy: 0.8248 - val_loss: 2.1026 - val_accuracy: 0.3156\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4724 - accuracy: 0.8270 - val_loss: 1.9554 - val_accuracy: 0.3642\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3873 - accuracy: 0.8577 - val_loss: 1.8935 - val_accuracy: 0.3459\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3690 - accuracy: 0.8645 - val_loss: 1.9016 - val_accuracy: 0.3303\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3202 - accuracy: 0.8828 - val_loss: 2.0686 - val_accuracy: 0.2642\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3234 - accuracy: 0.8856 - val_loss: 1.8114 - val_accuracy: 0.3610\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2743 - accuracy: 0.9027 - val_loss: 1.7449 - val_accuracy: 0.3851\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2575 - accuracy: 0.9066 - val_loss: 1.6205 - val_accuracy: 0.4042\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2711 - accuracy: 0.9051 - val_loss: 1.5692 - val_accuracy: 0.4087\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2213 - accuracy: 0.9239 - val_loss: 1.5735 - val_accuracy: 0.4165\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.2001 - accuracy: 0.9301 - val_loss: 1.4337 - val_accuracy: 0.5153\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1874 - accuracy: 0.9339 - val_loss: 1.5760 - val_accuracy: 0.4986\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1794 - accuracy: 0.9375 - val_loss: 1.4865 - val_accuracy: 0.4700\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1596 - accuracy: 0.9453 - val_loss: 1.6660 - val_accuracy: 0.3985\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1651 - accuracy: 0.9427 - val_loss: 1.5238 - val_accuracy: 0.4569\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1972 - accuracy: 0.9311 - val_loss: 1.3466 - val_accuracy: 0.5860\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1120 - accuracy: 0.9646 - val_loss: 1.5147 - val_accuracy: 0.4447\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1186 - accuracy: 0.9598 - val_loss: 1.6766 - val_accuracy: 0.3728\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1194 - accuracy: 0.9600 - val_loss: 1.3688 - val_accuracy: 0.5480\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1075 - accuracy: 0.9634 - val_loss: 1.1826 - val_accuracy: 0.6403\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1545 - accuracy: 0.9451 - val_loss: 1.1299 - val_accuracy: 0.6113\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1081 - accuracy: 0.9635 - val_loss: 1.0862 - val_accuracy: 0.6333\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1176 - accuracy: 0.9596 - val_loss: 1.2990 - val_accuracy: 0.5484\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0948 - accuracy: 0.9676 - val_loss: 1.0786 - val_accuracy: 0.6419\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0957 - accuracy: 0.9682 - val_loss: 1.1371 - val_accuracy: 0.5753\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0775 - accuracy: 0.9744 - val_loss: 1.3160 - val_accuracy: 0.4982\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0934 - accuracy: 0.9685 - val_loss: 1.2277 - val_accuracy: 0.5312\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1393 - accuracy: 0.9533 - val_loss: 1.3377 - val_accuracy: 0.4847\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0715 - accuracy: 0.9752 - val_loss: 1.3661 - val_accuracy: 0.4728\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0868 - accuracy: 0.9714 - val_loss: 1.2329 - val_accuracy: 0.5365\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0923 - accuracy: 0.9688 - val_loss: 1.4552 - val_accuracy: 0.4132\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1104 - accuracy: 0.9628 - val_loss: 1.3842 - val_accuracy: 0.4524\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0891 - accuracy: 0.9698 - val_loss: 1.1341 - val_accuracy: 0.5753\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1151 - accuracy: 0.9606 - val_loss: 1.3020 - val_accuracy: 0.5304\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 1.2391 - val_accuracy: 0.5492\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0800 - accuracy: 0.9729 - val_loss: 1.2333 - val_accuracy: 0.5325\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0619 - accuracy: 0.9795 - val_loss: 1.3277 - val_accuracy: 0.4904\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1142 - accuracy: 0.9614 - val_loss: 1.2601 - val_accuracy: 0.5267\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0861 - accuracy: 0.9714 - val_loss: 1.4527 - val_accuracy: 0.4132\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0784 - accuracy: 0.9734 - val_loss: 1.1604 - val_accuracy: 0.5149\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.9720 - val_accuracy: 0.6423\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0444 - accuracy: 0.9860 - val_loss: 1.4969 - val_accuracy: 0.3924\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0930 - accuracy: 0.9677 - val_loss: 1.4133 - val_accuracy: 0.4692\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 1.3258 - val_accuracy: 0.5010\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0316 - accuracy: 0.9896 - val_loss: 1.7383 - val_accuracy: 0.3344\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1017 - accuracy: 0.9684 - val_loss: 1.7510 - val_accuracy: 0.3173\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0725 - accuracy: 0.9761 - val_loss: 1.7380 - val_accuracy: 0.3450\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0674 - accuracy: 0.9768 - val_loss: 1.5037 - val_accuracy: 0.4341\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0574 - accuracy: 0.9797 - val_loss: 1.6576 - val_accuracy: 0.3838\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0556 - accuracy: 0.9815 - val_loss: 1.4197 - val_accuracy: 0.4614\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0702 - accuracy: 0.9756 - val_loss: 1.3569 - val_accuracy: 0.4814\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0338 - accuracy: 0.9896 - val_loss: 1.4221 - val_accuracy: 0.4933\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0490 - accuracy: 0.9843 - val_loss: 1.4565 - val_accuracy: 0.4737\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0553 - accuracy: 0.9827 - val_loss: 1.5069 - val_accuracy: 0.4312\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0501 - accuracy: 0.9829 - val_loss: 1.4898 - val_accuracy: 0.4451\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0832 - accuracy: 0.9700 - val_loss: 1.3791 - val_accuracy: 0.4953\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0740 - accuracy: 0.9752 - val_loss: 1.4663 - val_accuracy: 0.4512\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 1.2773 - val_accuracy: 0.5059\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0416 - accuracy: 0.9878 - val_loss: 1.5159 - val_accuracy: 0.4512\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0464 - accuracy: 0.9837 - val_loss: 1.6418 - val_accuracy: 0.3789\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0303 - accuracy: 0.9904 - val_loss: 1.5006 - val_accuracy: 0.4296\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0462 - accuracy: 0.9849 - val_loss: 1.3636 - val_accuracy: 0.4708\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0438 - accuracy: 0.9867 - val_loss: 1.5394 - val_accuracy: 0.3732\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0907 - accuracy: 0.9699 - val_loss: 1.4047 - val_accuracy: 0.4728\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 1.5081 - val_accuracy: 0.3969\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 1.6547 - val_accuracy: 0.3687\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 1.5901 - val_accuracy: 0.3957\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0990 - accuracy: 0.9669 - val_loss: 1.6057 - val_accuracy: 0.3655\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0441 - accuracy: 0.9854 - val_loss: 1.6181 - val_accuracy: 0.3814\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 1.6121 - val_accuracy: 0.3655\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 1.4438 - val_accuracy: 0.4639\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0571 - accuracy: 0.9806 - val_loss: 1.3930 - val_accuracy: 0.4745\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0721 - accuracy: 0.9747 - val_loss: 1.5236 - val_accuracy: 0.4006\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0471 - accuracy: 0.9841 - val_loss: 1.1050 - val_accuracy: 0.5574\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0369 - accuracy: 0.9867 - val_loss: 1.3570 - val_accuracy: 0.4635\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0444 - accuracy: 0.9852 - val_loss: 1.5654 - val_accuracy: 0.3900\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1003 - accuracy: 0.9656 - val_loss: 1.3936 - val_accuracy: 0.4696\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0317 - accuracy: 0.9892 - val_loss: 1.6435 - val_accuracy: 0.3793\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0343 - accuracy: 0.9879 - val_loss: 1.3234 - val_accuracy: 0.4957\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 1.1234 - val_accuracy: 0.5517\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 1.5549 - val_accuracy: 0.3842\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0748 - accuracy: 0.9751 - val_loss: 1.4207 - val_accuracy: 0.4528\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 1.4156 - val_accuracy: 0.4181\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0382 - accuracy: 0.9867 - val_loss: 1.1388 - val_accuracy: 0.5651\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0551 - accuracy: 0.9828 - val_loss: 1.6602 - val_accuracy: 0.3650\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0724 - accuracy: 0.9756 - val_loss: 1.3398 - val_accuracy: 0.4737\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0235 - accuracy: 0.9929 - val_loss: 1.3058 - val_accuracy: 0.4704\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 1.1942 - val_accuracy: 0.5365\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 1.2410 - val_accuracy: 0.5129\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0857 - accuracy: 0.9722 - val_loss: 1.5433 - val_accuracy: 0.4218\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5433 - accuracy: 0.4218\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[6 4 1 ... 6 8 2]\n",
      "Confusion matrix\n",
      " [[ 38   0   2   0  17  13 186   0   7   0]\n",
      " [  0  62   9   0   1  13 151   0   0   3]\n",
      " [  5   0 102   0  11   0 152   0   0   3]\n",
      " [  0   8  29 128   1   8  74   9   0   0]\n",
      " [  0   0   0   0 143  12  91   0   0   0]\n",
      " [  0   0   0   0  10 115  80   0   0   0]\n",
      " [  0   0   0   0   0   0 264   0   0   0]\n",
      " [  3   0   0   0   9  22  91  94   0   0]\n",
      " [  0   0   0   0  13   0 190   2  56   1]\n",
      " [ 11   0   0   0   9   0 167   3   0  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.14      0.24       263\n",
      "           1       0.89      0.26      0.40       239\n",
      "           2       0.72      0.37      0.49       273\n",
      "           3       1.00      0.50      0.66       257\n",
      "           4       0.67      0.58      0.62       246\n",
      "           5       0.63      0.56      0.59       205\n",
      "           6       0.18      1.00      0.31       264\n",
      "           7       0.87      0.43      0.57       219\n",
      "           8       0.89      0.21      0.34       262\n",
      "           9       0.82      0.14      0.24       221\n",
      "\n",
      "    accuracy                           0.42      2449\n",
      "   macro avg       0.73      0.42      0.45      2449\n",
      "weighted avg       0.73      0.42      0.44      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  3\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_100 (Conv1D)         (None, 18, 64)            11584     \n",
      "                                                                 \n",
      " conv1d_101 (Conv1D)         (None, 16, 64)            12352     \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 16, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_50 (MaxPoolin  (None, 8, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_49 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 100)               51300     \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,646\n",
      "Trainable params: 76,446\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 1.9523 - accuracy: 0.3025 - val_loss: 2.2674 - val_accuracy: 0.1711\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2365 - accuracy: 0.5558 - val_loss: 2.2821 - val_accuracy: 0.3618\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0355 - accuracy: 0.6283 - val_loss: 2.2451 - val_accuracy: 0.2879\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9044 - accuracy: 0.6758 - val_loss: 2.2025 - val_accuracy: 0.3116\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8433 - accuracy: 0.7000 - val_loss: 2.4339 - val_accuracy: 0.2315\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7832 - accuracy: 0.7258 - val_loss: 2.2244 - val_accuracy: 0.3279\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7138 - accuracy: 0.7478 - val_loss: 2.2231 - val_accuracy: 0.2095\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6600 - accuracy: 0.7687 - val_loss: 2.2056 - val_accuracy: 0.2483\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6224 - accuracy: 0.7790 - val_loss: 2.1736 - val_accuracy: 0.2642\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5869 - accuracy: 0.7835 - val_loss: 2.1403 - val_accuracy: 0.2332\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5419 - accuracy: 0.8065 - val_loss: 2.1007 - val_accuracy: 0.2466\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4781 - accuracy: 0.8329 - val_loss: 2.1739 - val_accuracy: 0.1817\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4365 - accuracy: 0.8413 - val_loss: 2.1407 - val_accuracy: 0.2409\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4034 - accuracy: 0.8525 - val_loss: 2.0491 - val_accuracy: 0.2523\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3766 - accuracy: 0.8633 - val_loss: 1.8619 - val_accuracy: 0.4116\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3929 - accuracy: 0.8627 - val_loss: 1.7207 - val_accuracy: 0.3704\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3192 - accuracy: 0.8859 - val_loss: 1.8464 - val_accuracy: 0.3655\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2919 - accuracy: 0.8980 - val_loss: 1.8572 - val_accuracy: 0.2846\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3276 - accuracy: 0.8851 - val_loss: 2.7489 - val_accuracy: 0.3083\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2832 - accuracy: 0.9012 - val_loss: 2.3341 - val_accuracy: 0.2670\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2487 - accuracy: 0.9135 - val_loss: 2.1461 - val_accuracy: 0.1833\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2282 - accuracy: 0.9206 - val_loss: 2.1581 - val_accuracy: 0.2127\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9374 - val_loss: 1.8923 - val_accuracy: 0.2679\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1812 - accuracy: 0.9387 - val_loss: 1.9779 - val_accuracy: 0.2107\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1885 - accuracy: 0.9337 - val_loss: 1.8073 - val_accuracy: 0.2785\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2037 - accuracy: 0.9305 - val_loss: 1.7537 - val_accuracy: 0.3720\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1870 - accuracy: 0.9382 - val_loss: 1.7404 - val_accuracy: 0.3642\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1609 - accuracy: 0.9457 - val_loss: 1.6419 - val_accuracy: 0.4818\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1841 - accuracy: 0.9361 - val_loss: 1.5983 - val_accuracy: 0.4304\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1339 - accuracy: 0.9566 - val_loss: 1.2626 - val_accuracy: 0.5525\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1741 - accuracy: 0.9412 - val_loss: 1.3011 - val_accuracy: 0.5651\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1321 - accuracy: 0.9569 - val_loss: 1.2764 - val_accuracy: 0.5651\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1025 - accuracy: 0.9675 - val_loss: 1.5110 - val_accuracy: 0.4443\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1020 - accuracy: 0.9672 - val_loss: 3.0112 - val_accuracy: 0.5263\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 1.4821 - val_accuracy: 0.4120\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1245 - accuracy: 0.9566 - val_loss: 1.5332 - val_accuracy: 0.4479\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1243 - accuracy: 0.9572 - val_loss: 1.5263 - val_accuracy: 0.4153\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0863 - accuracy: 0.9711 - val_loss: 1.6981 - val_accuracy: 0.3606\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0834 - accuracy: 0.9730 - val_loss: 1.5339 - val_accuracy: 0.4104\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1231 - accuracy: 0.9599 - val_loss: 1.4010 - val_accuracy: 0.5276\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1090 - accuracy: 0.9659 - val_loss: 1.3790 - val_accuracy: 0.5325\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0877 - accuracy: 0.9709 - val_loss: 1.3561 - val_accuracy: 0.5125\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0787 - accuracy: 0.9750 - val_loss: 1.4222 - val_accuracy: 0.4385\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1209 - accuracy: 0.9602 - val_loss: 1.4530 - val_accuracy: 0.4733\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0995 - accuracy: 0.9665 - val_loss: 1.3845 - val_accuracy: 0.4206\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0969 - accuracy: 0.9669 - val_loss: 1.3186 - val_accuracy: 0.5774\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0802 - accuracy: 0.9755 - val_loss: 1.3914 - val_accuracy: 0.4790\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0753 - accuracy: 0.9763 - val_loss: 1.4583 - val_accuracy: 0.4586\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0748 - accuracy: 0.9769 - val_loss: 1.4081 - val_accuracy: 0.4802\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0626 - accuracy: 0.9787 - val_loss: 1.4042 - val_accuracy: 0.4455\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 1.4945 - val_accuracy: 0.4757\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1340 - accuracy: 0.9556 - val_loss: 1.0841 - val_accuracy: 0.5753\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0732 - accuracy: 0.9741 - val_loss: 1.1346 - val_accuracy: 0.5713\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0655 - accuracy: 0.9805 - val_loss: 0.8776 - val_accuracy: 0.6938\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 1.2912 - val_accuracy: 0.5721\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0635 - accuracy: 0.9796 - val_loss: 1.0563 - val_accuracy: 0.6247\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0836 - accuracy: 0.9719 - val_loss: 0.9411 - val_accuracy: 0.6737\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0473 - accuracy: 0.9850 - val_loss: 0.8366 - val_accuracy: 0.7452\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0388 - accuracy: 0.9874 - val_loss: 0.8816 - val_accuracy: 0.6737\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0710 - accuracy: 0.9761 - val_loss: 0.8622 - val_accuracy: 0.6880\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1129 - accuracy: 0.9629 - val_loss: 1.0565 - val_accuracy: 0.5770\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0571 - accuracy: 0.9796 - val_loss: 1.0484 - val_accuracy: 0.6027\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0791 - accuracy: 0.9748 - val_loss: 1.0757 - val_accuracy: 0.5774\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 1.3590 - val_accuracy: 0.4581\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0485 - accuracy: 0.9834 - val_loss: 1.4692 - val_accuracy: 0.4316\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0957 - accuracy: 0.9688 - val_loss: 1.5856 - val_accuracy: 0.3422\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0538 - accuracy: 0.9814 - val_loss: 1.3788 - val_accuracy: 0.4165\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0346 - accuracy: 0.9886 - val_loss: 1.4704 - val_accuracy: 0.4275\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0693 - accuracy: 0.9774 - val_loss: 1.5784 - val_accuracy: 0.3708\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0514 - accuracy: 0.9828 - val_loss: 1.3934 - val_accuracy: 0.4577\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1193 - accuracy: 0.9649 - val_loss: 2.0417 - val_accuracy: 0.4353\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.9759 - val_loss: 1.3942 - val_accuracy: 0.4385\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 1.3409 - val_accuracy: 0.4704\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0606 - accuracy: 0.9791 - val_loss: 1.1929 - val_accuracy: 0.5182\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0600 - accuracy: 0.9789 - val_loss: 1.0429 - val_accuracy: 0.6105\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0704 - accuracy: 0.9765 - val_loss: 1.0013 - val_accuracy: 0.6697\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0438 - accuracy: 0.9848 - val_loss: 1.2253 - val_accuracy: 0.6002\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0737 - accuracy: 0.9753 - val_loss: 1.2609 - val_accuracy: 0.6693\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0358 - accuracy: 0.9878 - val_loss: 1.4210 - val_accuracy: 0.6015\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0631 - accuracy: 0.9781 - val_loss: 1.5475 - val_accuracy: 0.5798\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 1.6241 - val_accuracy: 0.5165\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0759 - accuracy: 0.9751 - val_loss: 1.0250 - val_accuracy: 0.6088\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0404 - accuracy: 0.9866 - val_loss: 1.2777 - val_accuracy: 0.5333\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0634 - accuracy: 0.9780 - val_loss: 1.4085 - val_accuracy: 0.4635\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0246 - accuracy: 0.9913 - val_loss: 1.1901 - val_accuracy: 0.5896\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0505 - accuracy: 0.9824 - val_loss: 1.3354 - val_accuracy: 0.5116\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 1.5089 - val_accuracy: 0.4202\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0522 - accuracy: 0.9824 - val_loss: 0.9940 - val_accuracy: 0.6186\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0281 - accuracy: 0.9917 - val_loss: 0.9802 - val_accuracy: 0.6256\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0710 - accuracy: 0.9770 - val_loss: 0.8820 - val_accuracy: 0.6750\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0540 - accuracy: 0.9816 - val_loss: 1.1251 - val_accuracy: 0.5406\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0461 - accuracy: 0.9840 - val_loss: 1.0848 - val_accuracy: 0.5484\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0372 - accuracy: 0.9864 - val_loss: 1.0266 - val_accuracy: 0.6582\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0449 - accuracy: 0.9850 - val_loss: 0.7732 - val_accuracy: 0.7093\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.7145 - val_accuracy: 0.7154\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0459 - accuracy: 0.9837 - val_loss: 0.9984 - val_accuracy: 0.6309\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0793 - accuracy: 0.9735 - val_loss: 0.9546 - val_accuracy: 0.6501\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1044 - accuracy: 0.9644 - val_loss: 1.0550 - val_accuracy: 0.6525\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0226 - accuracy: 0.9918 - val_loss: 0.7211 - val_accuracy: 0.7183\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.9182 - val_accuracy: 0.6693\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9182 - accuracy: 0.6693\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[4 6 6 ... 2 7 6]\n",
      "Confusion matrix\n",
      " [[ 64   0  25   4   1   1 157   0   0   3]\n",
      " [  0 121  13   5   0   0  84  10  12   0]\n",
      " [  0   0 233   5   0   0  18   0   2   0]\n",
      " [  0   0   3 246   0   0  18   0   0   0]\n",
      " [  0   0   5   0 153   0 106   1   2   7]\n",
      " [  0   0   2   0   0 154  58   9   0   0]\n",
      " [  0   0   0   0   0   0 283   0   1   0]\n",
      " [  0   0   0   3   0   0  87 141   0   0]\n",
      " [  0   0   2   1   0   0 135   0 103   0]\n",
      " [  0   0   0   0   0   0  30   0   0 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40       255\n",
      "           1       1.00      0.49      0.66       245\n",
      "           2       0.82      0.90      0.86       258\n",
      "           3       0.93      0.92      0.93       267\n",
      "           4       0.99      0.56      0.71       274\n",
      "           5       0.99      0.69      0.81       223\n",
      "           6       0.29      1.00      0.45       284\n",
      "           7       0.88      0.61      0.72       231\n",
      "           8       0.86      0.43      0.57       241\n",
      "           9       0.93      0.82      0.88       171\n",
      "\n",
      "    accuracy                           0.67      2449\n",
      "   macro avg       0.87      0.67      0.70      2449\n",
      "weighted avg       0.86      0.67      0.69      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  4\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_102 (Conv1D)         (None, 17, 64)            15424     \n",
      "                                                                 \n",
      " conv1d_103 (Conv1D)         (None, 14, 64)            16448     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 14, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_51 (MaxPoolin  (None, 7, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_50 (Flatten)        (None, 448)               0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 100)               44900     \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,182\n",
      "Trainable params: 77,982\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.0682 - accuracy: 0.2586 - val_loss: 2.4736 - val_accuracy: 0.2589\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3822 - accuracy: 0.5063 - val_loss: 2.2724 - val_accuracy: 0.1205\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1489 - accuracy: 0.5906 - val_loss: 2.2654 - val_accuracy: 0.1090\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0132 - accuracy: 0.6432 - val_loss: 2.2601 - val_accuracy: 0.1098\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8981 - accuracy: 0.6766 - val_loss: 2.2498 - val_accuracy: 0.1180\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8264 - accuracy: 0.7033 - val_loss: 2.2498 - val_accuracy: 0.1102\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7903 - accuracy: 0.7159 - val_loss: 2.2292 - val_accuracy: 0.1221\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7402 - accuracy: 0.7346 - val_loss: 2.2260 - val_accuracy: 0.1364\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7323 - accuracy: 0.7336 - val_loss: 2.1317 - val_accuracy: 0.1413\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6641 - accuracy: 0.7628 - val_loss: 2.0201 - val_accuracy: 0.1821\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5939 - accuracy: 0.7839 - val_loss: 1.9817 - val_accuracy: 0.1997\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5777 - accuracy: 0.7921 - val_loss: 2.0560 - val_accuracy: 0.1948\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5279 - accuracy: 0.8097 - val_loss: 1.9995 - val_accuracy: 0.2107\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4883 - accuracy: 0.8239 - val_loss: 2.0427 - val_accuracy: 0.1874\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4775 - accuracy: 0.8303 - val_loss: 2.0239 - val_accuracy: 0.2115\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4704 - accuracy: 0.8321 - val_loss: 1.9607 - val_accuracy: 0.2127\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4053 - accuracy: 0.8527 - val_loss: 1.7959 - val_accuracy: 0.2952\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3987 - accuracy: 0.8559 - val_loss: 1.7701 - val_accuracy: 0.3450\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3824 - accuracy: 0.8638 - val_loss: 1.6758 - val_accuracy: 0.3577\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3342 - accuracy: 0.8812 - val_loss: 1.7818 - val_accuracy: 0.3630\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3033 - accuracy: 0.8904 - val_loss: 1.7460 - val_accuracy: 0.3397\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3190 - accuracy: 0.8873 - val_loss: 1.6304 - val_accuracy: 0.3773\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2432 - accuracy: 0.9152 - val_loss: 1.7661 - val_accuracy: 0.3079\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2525 - accuracy: 0.9112 - val_loss: 1.5884 - val_accuracy: 0.3761\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2297 - accuracy: 0.9196 - val_loss: 1.6772 - val_accuracy: 0.3793\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2294 - accuracy: 0.9219 - val_loss: 1.9868 - val_accuracy: 0.3757\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1915 - accuracy: 0.9306 - val_loss: 2.2919 - val_accuracy: 0.3177\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2138 - accuracy: 0.9218 - val_loss: 1.9391 - val_accuracy: 0.4112\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2134 - accuracy: 0.9259 - val_loss: 1.6906 - val_accuracy: 0.3410\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2045 - accuracy: 0.9283 - val_loss: 1.4048 - val_accuracy: 0.4720\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1864 - accuracy: 0.9346 - val_loss: 1.5935 - val_accuracy: 0.3385\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1404 - accuracy: 0.9519 - val_loss: 1.4433 - val_accuracy: 0.4259\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1500 - accuracy: 0.9510 - val_loss: 1.5035 - val_accuracy: 0.5382\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1135 - accuracy: 0.9629 - val_loss: 1.0932 - val_accuracy: 0.6252\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1398 - accuracy: 0.9534 - val_loss: 1.0113 - val_accuracy: 0.6868\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1433 - accuracy: 0.9491 - val_loss: 1.1580 - val_accuracy: 0.6092\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1582 - accuracy: 0.9462 - val_loss: 1.3009 - val_accuracy: 0.4998\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1014 - accuracy: 0.9662 - val_loss: 1.3225 - val_accuracy: 0.4577\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1314 - accuracy: 0.9538 - val_loss: 1.1100 - val_accuracy: 0.6301\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1263 - accuracy: 0.9576 - val_loss: 1.0565 - val_accuracy: 0.6991\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2170 - accuracy: 0.9291 - val_loss: 0.9020 - val_accuracy: 0.7150\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1031 - accuracy: 0.9650 - val_loss: 0.7394 - val_accuracy: 0.7889\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1309 - accuracy: 0.9566 - val_loss: 1.0197 - val_accuracy: 0.6219\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0717 - accuracy: 0.9772 - val_loss: 1.2123 - val_accuracy: 0.5659\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1276 - accuracy: 0.9552 - val_loss: 0.8472 - val_accuracy: 0.6868\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0835 - accuracy: 0.9712 - val_loss: 0.8540 - val_accuracy: 0.6566\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0751 - accuracy: 0.9731 - val_loss: 0.7593 - val_accuracy: 0.7125\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1174 - accuracy: 0.9608 - val_loss: 0.7335 - val_accuracy: 0.7517\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1109 - accuracy: 0.9630 - val_loss: 0.8997 - val_accuracy: 0.6325\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0716 - accuracy: 0.9759 - val_loss: 0.8908 - val_accuracy: 0.6627\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0692 - accuracy: 0.9797 - val_loss: 1.0204 - val_accuracy: 0.6064\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0862 - accuracy: 0.9700 - val_loss: 0.9523 - val_accuracy: 0.6362\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0662 - accuracy: 0.9778 - val_loss: 0.8573 - val_accuracy: 0.6897\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1705 - accuracy: 0.9427 - val_loss: 0.8406 - val_accuracy: 0.7060\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0944 - accuracy: 0.9666 - val_loss: 0.7871 - val_accuracy: 0.7007\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1577 - accuracy: 0.9433 - val_loss: 0.6694 - val_accuracy: 0.7811\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0450 - accuracy: 0.9868 - val_loss: 0.7309 - val_accuracy: 0.7570\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0658 - accuracy: 0.9764 - val_loss: 0.6713 - val_accuracy: 0.7771\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0663 - accuracy: 0.9753 - val_loss: 0.5410 - val_accuracy: 0.8171\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0732 - accuracy: 0.9756 - val_loss: 0.6361 - val_accuracy: 0.7897\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0784 - accuracy: 0.9732 - val_loss: 0.7059 - val_accuracy: 0.7411\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1315 - accuracy: 0.9552 - val_loss: 1.2072 - val_accuracy: 0.5623\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0898 - accuracy: 0.9693 - val_loss: 1.0699 - val_accuracy: 0.5561\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0703 - accuracy: 0.9769 - val_loss: 0.8783 - val_accuracy: 0.6443\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0654 - accuracy: 0.9790 - val_loss: 1.1998 - val_accuracy: 0.5010\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0703 - accuracy: 0.9780 - val_loss: 0.9079 - val_accuracy: 0.6292\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1220 - accuracy: 0.9577 - val_loss: 0.9444 - val_accuracy: 0.6207\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0501 - accuracy: 0.9838 - val_loss: 0.9295 - val_accuracy: 0.6358\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.6950 - val_accuracy: 0.7407\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0531 - accuracy: 0.9839 - val_loss: 0.8264 - val_accuracy: 0.6680\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0642 - accuracy: 0.9785 - val_loss: 0.7203 - val_accuracy: 0.7301\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0545 - accuracy: 0.9821 - val_loss: 0.6959 - val_accuracy: 0.7227\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1141 - accuracy: 0.9612 - val_loss: 0.8976 - val_accuracy: 0.6537\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0895 - accuracy: 0.9693 - val_loss: 0.6623 - val_accuracy: 0.7391\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.5867 - val_accuracy: 0.7799\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0370 - accuracy: 0.9882 - val_loss: 0.8447 - val_accuracy: 0.6754\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0606 - accuracy: 0.9795 - val_loss: 0.8178 - val_accuracy: 0.6880\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1067 - accuracy: 0.9656 - val_loss: 1.5140 - val_accuracy: 0.3797\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1301 - accuracy: 0.9578 - val_loss: 1.1575 - val_accuracy: 0.5108\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0512 - accuracy: 0.9818 - val_loss: 1.4028 - val_accuracy: 0.3855\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0535 - accuracy: 0.9811 - val_loss: 0.9311 - val_accuracy: 0.6537\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 1.0610 - val_accuracy: 0.5855\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0305 - accuracy: 0.9910 - val_loss: 1.2996 - val_accuracy: 0.5027\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0347 - accuracy: 0.9887 - val_loss: 0.8953 - val_accuracy: 0.6341\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0157 - accuracy: 0.9961 - val_loss: 0.6077 - val_accuracy: 0.7689\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0447 - accuracy: 0.9839 - val_loss: 0.9616 - val_accuracy: 0.5929\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1033 - accuracy: 0.9672 - val_loss: 0.8353 - val_accuracy: 0.6725\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1100 - accuracy: 0.9622 - val_loss: 0.8986 - val_accuracy: 0.6235\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0253 - accuracy: 0.9926 - val_loss: 0.9957 - val_accuracy: 0.5823\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0427 - accuracy: 0.9883 - val_loss: 1.1503 - val_accuracy: 0.5316\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0836 - accuracy: 0.9718 - val_loss: 0.8156 - val_accuracy: 0.6770\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 1.0722 - val_accuracy: 0.5223\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0413 - accuracy: 0.9870 - val_loss: 0.9898 - val_accuracy: 0.5664\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1187 - accuracy: 0.9629 - val_loss: 1.1868 - val_accuracy: 0.5488\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.7448 - val_accuracy: 0.7060\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 0.8279 - val_accuracy: 0.6815\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0243 - accuracy: 0.9926 - val_loss: 0.9635 - val_accuracy: 0.6223\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0914 - accuracy: 0.9695 - val_loss: 0.8781 - val_accuracy: 0.6693\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 1.0767 - val_accuracy: 0.5786\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0268 - accuracy: 0.9923 - val_loss: 0.9691 - val_accuracy: 0.6247\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9691 - accuracy: 0.6247\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[6 6 6 ... 2 6 6]\n",
      "Confusion matrix\n",
      " [[155   1   0   0   7   0  86   0  11   0]\n",
      " [  0 217   0   0   0   0  17   1   0   0]\n",
      " [  0   6 205   0   1   3  65   0   0   0]\n",
      " [  0   5  33  61   2   1 156   0   1   0]\n",
      " [  2   0   0   0 151   0  97   0   1   0]\n",
      " [  2   3   0   0   0 111  95   0   0   0]\n",
      " [  0   0   0   0   0   0 267   0   0   0]\n",
      " [  0   0   0   0   3   2 111 119   0   0]\n",
      " [  0   0   1   0  18   0  95   0 144   0]\n",
      " [  0   1   0   0   0   0  89   0   3 100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.60      0.74       260\n",
      "           1       0.93      0.92      0.93       235\n",
      "           2       0.86      0.73      0.79       280\n",
      "           3       1.00      0.24      0.38       259\n",
      "           4       0.83      0.60      0.70       251\n",
      "           5       0.95      0.53      0.68       211\n",
      "           6       0.25      1.00      0.40       267\n",
      "           7       0.99      0.51      0.67       235\n",
      "           8       0.90      0.56      0.69       258\n",
      "           9       1.00      0.52      0.68       193\n",
      "\n",
      "    accuracy                           0.62      2449\n",
      "   macro avg       0.87      0.62      0.67      2449\n",
      "weighted avg       0.86      0.62      0.66      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  5\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_104 (Conv1D)         (None, 16, 64)            19264     \n",
      "                                                                 \n",
      " conv1d_105 (Conv1D)         (None, 12, 64)            20544     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_52 (MaxPoolin  (None, 6, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_51 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 100)               38500     \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,718\n",
      "Trainable params: 79,518\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.0761 - accuracy: 0.2523 - val_loss: 2.3171 - val_accuracy: 0.1160\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3543 - accuracy: 0.5131 - val_loss: 2.2587 - val_accuracy: 0.1258\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1624 - accuracy: 0.5825 - val_loss: 2.2404 - val_accuracy: 0.2050\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0578 - accuracy: 0.6222 - val_loss: 2.1704 - val_accuracy: 0.3091\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9616 - accuracy: 0.6543 - val_loss: 2.1301 - val_accuracy: 0.3344\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8915 - accuracy: 0.6783 - val_loss: 1.9891 - val_accuracy: 0.3973\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8532 - accuracy: 0.6883 - val_loss: 1.8934 - val_accuracy: 0.4610\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8336 - accuracy: 0.6897 - val_loss: 1.8152 - val_accuracy: 0.4622\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7607 - accuracy: 0.7265 - val_loss: 1.7672 - val_accuracy: 0.5063\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7114 - accuracy: 0.7370 - val_loss: 1.8356 - val_accuracy: 0.4541\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6912 - accuracy: 0.7474 - val_loss: 1.7601 - val_accuracy: 0.4594\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6318 - accuracy: 0.7720 - val_loss: 1.5679 - val_accuracy: 0.5394\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6056 - accuracy: 0.7787 - val_loss: 2.1192 - val_accuracy: 0.5296\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5672 - accuracy: 0.7978 - val_loss: 1.8565 - val_accuracy: 0.4924\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5627 - accuracy: 0.8013 - val_loss: 1.8621 - val_accuracy: 0.3940\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5342 - accuracy: 0.8073 - val_loss: 1.7195 - val_accuracy: 0.5002\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4829 - accuracy: 0.8295 - val_loss: 1.5867 - val_accuracy: 0.5096\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4197 - accuracy: 0.8498 - val_loss: 1.6679 - val_accuracy: 0.4345\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4042 - accuracy: 0.8522 - val_loss: 1.4766 - val_accuracy: 0.5374\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3506 - accuracy: 0.8737 - val_loss: 1.4629 - val_accuracy: 0.5361\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3098 - accuracy: 0.8910 - val_loss: 1.5549 - val_accuracy: 0.5349\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3856 - accuracy: 0.8602 - val_loss: 1.5516 - val_accuracy: 0.5006\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3111 - accuracy: 0.8913 - val_loss: 1.4690 - val_accuracy: 0.5398\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2540 - accuracy: 0.9096 - val_loss: 1.4546 - val_accuracy: 0.5255\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2934 - accuracy: 0.8937 - val_loss: 1.2787 - val_accuracy: 0.5655\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2760 - accuracy: 0.9037 - val_loss: 1.1755 - val_accuracy: 0.6243\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2872 - accuracy: 0.9001 - val_loss: 1.1806 - val_accuracy: 0.5945\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2493 - accuracy: 0.9099 - val_loss: 1.1059 - val_accuracy: 0.6068\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2216 - accuracy: 0.9223 - val_loss: 1.2153 - val_accuracy: 0.5664\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1857 - accuracy: 0.9363 - val_loss: 1.1144 - val_accuracy: 0.6443\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2170 - accuracy: 0.9231 - val_loss: 0.9808 - val_accuracy: 0.6701\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1774 - accuracy: 0.9369 - val_loss: 1.1283 - val_accuracy: 0.5941\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1795 - accuracy: 0.9370 - val_loss: 1.0720 - val_accuracy: 0.6178\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 1.0505 - val_accuracy: 0.6182\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1664 - accuracy: 0.9401 - val_loss: 0.8213 - val_accuracy: 0.7444\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2058 - accuracy: 0.9286 - val_loss: 1.0428 - val_accuracy: 0.6766\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2379 - accuracy: 0.9182 - val_loss: 1.1609 - val_accuracy: 0.5394\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1473 - accuracy: 0.9490 - val_loss: 1.3151 - val_accuracy: 0.4851\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1512 - accuracy: 0.9500 - val_loss: 1.1227 - val_accuracy: 0.5917\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1988 - accuracy: 0.9301 - val_loss: 1.1201 - val_accuracy: 0.6207\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1771 - accuracy: 0.9389 - val_loss: 1.1588 - val_accuracy: 0.5308\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1267 - accuracy: 0.9571 - val_loss: 1.1428 - val_accuracy: 0.5378\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1007 - accuracy: 0.9678 - val_loss: 0.9720 - val_accuracy: 0.6309\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.1374 - accuracy: 0.9499 - val_loss: 1.0376 - val_accuracy: 0.5819\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 3s 21ms/step - loss: 0.0958 - accuracy: 0.9670 - val_loss: 1.0200 - val_accuracy: 0.5904\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1267 - accuracy: 0.9555 - val_loss: 0.8269 - val_accuracy: 0.7113\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2023 - accuracy: 0.9276 - val_loss: 0.8486 - val_accuracy: 0.7040\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1425 - accuracy: 0.9542 - val_loss: 0.8529 - val_accuracy: 0.6791\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0954 - accuracy: 0.9670 - val_loss: 0.9266 - val_accuracy: 0.6403\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1604 - accuracy: 0.9454 - val_loss: 0.7629 - val_accuracy: 0.7281\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1149 - accuracy: 0.9602 - val_loss: 0.8215 - val_accuracy: 0.6835\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1031 - accuracy: 0.9646 - val_loss: 0.7352 - val_accuracy: 0.7219\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1008 - accuracy: 0.9683 - val_loss: 0.7297 - val_accuracy: 0.7015\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1143 - accuracy: 0.9587 - val_loss: 0.8120 - val_accuracy: 0.6811\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1313 - accuracy: 0.9546 - val_loss: 0.6011 - val_accuracy: 0.7726\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0887 - accuracy: 0.9693 - val_loss: 0.7091 - val_accuracy: 0.7432\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0941 - accuracy: 0.9663 - val_loss: 0.5955 - val_accuracy: 0.7771\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1460 - accuracy: 0.9530 - val_loss: 0.8850 - val_accuracy: 0.7089\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.1105 - accuracy: 0.9611 - val_loss: 0.7464 - val_accuracy: 0.7003\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1896 - accuracy: 0.9331 - val_loss: 0.5355 - val_accuracy: 0.8236\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0844 - accuracy: 0.9732 - val_loss: 0.5937 - val_accuracy: 0.7918\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0683 - accuracy: 0.9773 - val_loss: 0.5341 - val_accuracy: 0.8085\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0675 - accuracy: 0.9773 - val_loss: 0.5129 - val_accuracy: 0.8093\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1300 - accuracy: 0.9562 - val_loss: 0.7310 - val_accuracy: 0.7195\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0789 - accuracy: 0.9737 - val_loss: 0.4734 - val_accuracy: 0.8305\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0666 - accuracy: 0.9759 - val_loss: 0.6306 - val_accuracy: 0.7477\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0564 - accuracy: 0.9829 - val_loss: 0.3557 - val_accuracy: 0.8644\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0655 - accuracy: 0.9778 - val_loss: 0.7576 - val_accuracy: 0.6978\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1897 - accuracy: 0.9385 - val_loss: 0.5746 - val_accuracy: 0.7848\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0760 - accuracy: 0.9739 - val_loss: 0.5475 - val_accuracy: 0.7840\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.4632 - val_accuracy: 0.8167\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1056 - accuracy: 0.9649 - val_loss: 0.5566 - val_accuracy: 0.7840\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0996 - accuracy: 0.9662 - val_loss: 0.6312 - val_accuracy: 0.7550\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.3959 - val_accuracy: 0.8542\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0760 - accuracy: 0.9737 - val_loss: 1.5171 - val_accuracy: 0.7926\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1466 - accuracy: 0.9508 - val_loss: 0.3402 - val_accuracy: 0.8832\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.3221 - val_accuracy: 0.9069\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0741 - accuracy: 0.9766 - val_loss: 0.3751 - val_accuracy: 0.9053\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0496 - accuracy: 0.9830 - val_loss: 0.4986 - val_accuracy: 0.8473\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0778 - accuracy: 0.9760 - val_loss: 0.5756 - val_accuracy: 0.8093\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1114 - accuracy: 0.9647 - val_loss: 0.6172 - val_accuracy: 0.8060\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0750 - accuracy: 0.9758 - val_loss: 0.5046 - val_accuracy: 0.8216\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0562 - accuracy: 0.9814 - val_loss: 0.4368 - val_accuracy: 0.8501\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0812 - accuracy: 0.9722 - val_loss: 0.4695 - val_accuracy: 0.8444\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0929 - accuracy: 0.9671 - val_loss: 0.6180 - val_accuracy: 0.7783\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0730 - accuracy: 0.9766 - val_loss: 0.5452 - val_accuracy: 0.8118\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0512 - accuracy: 0.9841 - val_loss: 0.5668 - val_accuracy: 0.7840\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0796 - accuracy: 0.9735 - val_loss: 0.8266 - val_accuracy: 0.7048\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0895 - accuracy: 0.9693 - val_loss: 0.4831 - val_accuracy: 0.8354\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0322 - accuracy: 0.9904 - val_loss: 0.4600 - val_accuracy: 0.8322\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0586 - accuracy: 0.9809 - val_loss: 0.3549 - val_accuracy: 0.8828\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1375 - accuracy: 0.9541 - val_loss: 0.6760 - val_accuracy: 0.7313\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1108 - accuracy: 0.9616 - val_loss: 0.4146 - val_accuracy: 0.8436\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.3917 - val_accuracy: 0.8665\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1061 - accuracy: 0.9663 - val_loss: 0.3331 - val_accuracy: 0.8885\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0717 - accuracy: 0.9755 - val_loss: 0.3883 - val_accuracy: 0.8510\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0728 - accuracy: 0.9760 - val_loss: 0.3706 - val_accuracy: 0.8677\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0392 - accuracy: 0.9873 - val_loss: 0.4105 - val_accuracy: 0.8501\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1072 - accuracy: 0.9663 - val_loss: 0.4455 - val_accuracy: 0.8342\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0661 - accuracy: 0.9763 - val_loss: 0.3263 - val_accuracy: 0.8742\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3263 - accuracy: 0.8742\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[5 8 6 ... 3 0 2]\n",
      "Confusion matrix\n",
      " [[244   0  11   0   0   0  16   0   0   0]\n",
      " [  0 195  16   0   0   0  22   0   0   0]\n",
      " [  2   0 255   0   0   0   8   0   0   0]\n",
      " [  0   0  56 211   0   0   0   0   0   0]\n",
      " [  0   0   0   0 180   0  49   0   3  14]\n",
      " [  0   0   0   0   0 152  59   1   0   0]\n",
      " [  0   0   0   0   0   0 262   0   0   0]\n",
      " [  0   0   1   0   0   0  22 208   1   0]\n",
      " [  0   0   0   0   0   0  22   0 236   1]\n",
      " [  0   0   3   0   1   0   0   0   0 198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       271\n",
      "           1       1.00      0.84      0.91       233\n",
      "           2       0.75      0.96      0.84       265\n",
      "           3       1.00      0.79      0.88       267\n",
      "           4       0.99      0.73      0.84       246\n",
      "           5       1.00      0.72      0.84       212\n",
      "           6       0.57      1.00      0.73       262\n",
      "           7       1.00      0.90      0.94       232\n",
      "           8       0.98      0.91      0.95       259\n",
      "           9       0.93      0.98      0.95       202\n",
      "\n",
      "    accuracy                           0.87      2449\n",
      "   macro avg       0.92      0.87      0.88      2449\n",
      "weighted avg       0.92      0.87      0.88      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  6\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_106 (Conv1D)         (None, 15, 64)            23104     \n",
      "                                                                 \n",
      " conv1d_107 (Conv1D)         (None, 10, 64)            24640     \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 10, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_53 (MaxPoolin  (None, 5, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_52 (Flatten)        (None, 320)               0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 100)               32100     \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,254\n",
      "Trainable params: 81,054\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.1447 - accuracy: 0.2299 - val_loss: 2.4462 - val_accuracy: 0.1617\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4984 - accuracy: 0.4617 - val_loss: 2.2877 - val_accuracy: 0.1531\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2136 - accuracy: 0.5639 - val_loss: 2.2700 - val_accuracy: 0.1445\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1080 - accuracy: 0.6039 - val_loss: 2.2440 - val_accuracy: 0.1993\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0772 - accuracy: 0.6154 - val_loss: 2.2194 - val_accuracy: 0.3144\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9806 - accuracy: 0.6511 - val_loss: 2.1954 - val_accuracy: 0.2540\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9307 - accuracy: 0.6649 - val_loss: 2.1708 - val_accuracy: 0.2719\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8700 - accuracy: 0.6854 - val_loss: 2.0935 - val_accuracy: 0.2948\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8359 - accuracy: 0.6975 - val_loss: 2.0965 - val_accuracy: 0.2548\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8094 - accuracy: 0.7075 - val_loss: 2.1205 - val_accuracy: 0.2479\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7916 - accuracy: 0.7195 - val_loss: 2.0411 - val_accuracy: 0.2683\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7426 - accuracy: 0.7349 - val_loss: 1.8241 - val_accuracy: 0.4059\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7132 - accuracy: 0.7420 - val_loss: 1.8228 - val_accuracy: 0.3773\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6819 - accuracy: 0.7571 - val_loss: 1.6667 - val_accuracy: 0.4920\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6562 - accuracy: 0.7608 - val_loss: 1.6846 - val_accuracy: 0.4324\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6176 - accuracy: 0.7784 - val_loss: 1.6069 - val_accuracy: 0.4373\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6251 - accuracy: 0.7747 - val_loss: 1.5751 - val_accuracy: 0.4373\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5753 - accuracy: 0.7975 - val_loss: 1.4813 - val_accuracy: 0.4900\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5733 - accuracy: 0.7925 - val_loss: 1.6789 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5327 - accuracy: 0.8080 - val_loss: 1.4843 - val_accuracy: 0.5557\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5049 - accuracy: 0.8211 - val_loss: 1.2204 - val_accuracy: 0.6145\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5021 - accuracy: 0.8163 - val_loss: 1.4942 - val_accuracy: 0.4867\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4644 - accuracy: 0.8351 - val_loss: 1.1886 - val_accuracy: 0.6296\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4692 - accuracy: 0.8304 - val_loss: 1.6614 - val_accuracy: 0.5047\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4582 - accuracy: 0.8380 - val_loss: 1.9297 - val_accuracy: 0.5627\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4134 - accuracy: 0.8525 - val_loss: 1.8918 - val_accuracy: 0.6529\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4162 - accuracy: 0.8493 - val_loss: 1.3643 - val_accuracy: 0.7485\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3611 - accuracy: 0.8711 - val_loss: 1.4413 - val_accuracy: 0.7044\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3930 - accuracy: 0.8606 - val_loss: 1.0637 - val_accuracy: 0.6697\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3904 - accuracy: 0.8605 - val_loss: 0.8993 - val_accuracy: 0.7464\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3367 - accuracy: 0.8786 - val_loss: 0.8353 - val_accuracy: 0.7697\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2952 - accuracy: 0.8964 - val_loss: 0.7713 - val_accuracy: 0.7971\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3355 - accuracy: 0.8808 - val_loss: 0.8867 - val_accuracy: 0.7497\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2877 - accuracy: 0.8939 - val_loss: 0.8261 - val_accuracy: 0.7546\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2715 - accuracy: 0.9054 - val_loss: 0.7271 - val_accuracy: 0.8428\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 3s 21ms/step - loss: 0.2507 - accuracy: 0.9150 - val_loss: 0.6919 - val_accuracy: 0.8403\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2502 - accuracy: 0.9131 - val_loss: 0.7454 - val_accuracy: 0.7844\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3128 - accuracy: 0.8895 - val_loss: 0.5995 - val_accuracy: 0.8612\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2280 - accuracy: 0.9205 - val_loss: 0.5273 - val_accuracy: 0.8506\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2028 - accuracy: 0.9269 - val_loss: 0.6128 - val_accuracy: 0.8248\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2012 - accuracy: 0.9299 - val_loss: 0.5996 - val_accuracy: 0.8416\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2027 - accuracy: 0.9278 - val_loss: 0.5317 - val_accuracy: 0.8608\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2365 - accuracy: 0.9153 - val_loss: 0.5999 - val_accuracy: 0.8252\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 4s 24ms/step - loss: 0.2027 - accuracy: 0.9309 - val_loss: 0.5915 - val_accuracy: 0.8142\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2016 - accuracy: 0.9291 - val_loss: 0.5698 - val_accuracy: 0.8273\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1767 - accuracy: 0.9415 - val_loss: 0.5805 - val_accuracy: 0.8150\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1534 - accuracy: 0.9489 - val_loss: 0.5980 - val_accuracy: 0.8383\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1462 - accuracy: 0.9504 - val_loss: 0.4891 - val_accuracy: 0.8979\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1771 - accuracy: 0.9376 - val_loss: 0.4325 - val_accuracy: 0.8996\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1560 - accuracy: 0.9471 - val_loss: 0.6805 - val_accuracy: 0.7922\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2047 - accuracy: 0.9269 - val_loss: 0.4968 - val_accuracy: 0.8800\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1658 - accuracy: 0.9432 - val_loss: 0.4205 - val_accuracy: 0.8591\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1442 - accuracy: 0.9504 - val_loss: 0.3345 - val_accuracy: 0.8996\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1444 - accuracy: 0.9519 - val_loss: 0.3792 - val_accuracy: 0.8632\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1893 - accuracy: 0.9321 - val_loss: 0.4007 - val_accuracy: 0.8583\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1258 - accuracy: 0.9569 - val_loss: 0.5129 - val_accuracy: 0.8457\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.1120 - accuracy: 0.9632 - val_loss: 0.5574 - val_accuracy: 0.8305\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1010 - accuracy: 0.9650 - val_loss: 0.4326 - val_accuracy: 0.8930\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1239 - accuracy: 0.9565 - val_loss: 0.6703 - val_accuracy: 0.8314\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1613 - accuracy: 0.9458 - val_loss: 0.4090 - val_accuracy: 0.8689\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1915 - accuracy: 0.9366 - val_loss: 0.6777 - val_accuracy: 0.7542\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1553 - accuracy: 0.9469 - val_loss: 0.4695 - val_accuracy: 0.8350\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.5477 - val_accuracy: 0.8248\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0981 - accuracy: 0.9660 - val_loss: 0.4971 - val_accuracy: 0.8338\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1165 - accuracy: 0.9597 - val_loss: 0.2697 - val_accuracy: 0.9208\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0872 - accuracy: 0.9721 - val_loss: 0.3257 - val_accuracy: 0.8902\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1223 - accuracy: 0.9588 - val_loss: 0.3862 - val_accuracy: 0.8608\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1081 - accuracy: 0.9630 - val_loss: 0.4170 - val_accuracy: 0.8452\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1204 - accuracy: 0.9557 - val_loss: 0.2876 - val_accuracy: 0.9138\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1072 - accuracy: 0.9614 - val_loss: 0.3778 - val_accuracy: 0.8697\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0863 - accuracy: 0.9709 - val_loss: 0.2854 - val_accuracy: 0.8881\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2007 - accuracy: 0.9324 - val_loss: 0.4165 - val_accuracy: 0.8587\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0938 - accuracy: 0.9702 - val_loss: 0.2988 - val_accuracy: 0.9085\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0971 - accuracy: 0.9671 - val_loss: 0.4302 - val_accuracy: 0.8604\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1373 - accuracy: 0.9535 - val_loss: 0.3133 - val_accuracy: 0.8983\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1507 - accuracy: 0.9513 - val_loss: 0.2007 - val_accuracy: 0.9473\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0598 - accuracy: 0.9803 - val_loss: 0.2826 - val_accuracy: 0.9200\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0766 - accuracy: 0.9741 - val_loss: 0.4223 - val_accuracy: 0.8518\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0467 - accuracy: 0.9850 - val_loss: 0.1747 - val_accuracy: 0.9637\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1045 - accuracy: 0.9655 - val_loss: 0.2829 - val_accuracy: 0.9024\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0801 - accuracy: 0.9735 - val_loss: 0.1826 - val_accuracy: 0.9453\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1136 - accuracy: 0.9613 - val_loss: 0.3183 - val_accuracy: 0.8947\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0824 - accuracy: 0.9706 - val_loss: 0.2964 - val_accuracy: 0.9061\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1452 - accuracy: 0.9497 - val_loss: 0.2099 - val_accuracy: 0.9388\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0938 - accuracy: 0.9660 - val_loss: 0.2752 - val_accuracy: 0.9012\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1012 - accuracy: 0.9660 - val_loss: 0.2715 - val_accuracy: 0.9334\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1899 - accuracy: 0.9355 - val_loss: 0.3662 - val_accuracy: 0.8877\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0995 - accuracy: 0.9685 - val_loss: 0.3600 - val_accuracy: 0.9094\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0772 - accuracy: 0.9739 - val_loss: 0.3891 - val_accuracy: 0.8759\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0741 - accuracy: 0.9745 - val_loss: 0.4059 - val_accuracy: 0.8689\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0850 - accuracy: 0.9713 - val_loss: 0.3394 - val_accuracy: 0.8967\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0615 - accuracy: 0.9803 - val_loss: 0.2889 - val_accuracy: 0.9171\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0763 - accuracy: 0.9723 - val_loss: 0.2531 - val_accuracy: 0.9388\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 4s 23ms/step - loss: 0.0630 - accuracy: 0.9782 - val_loss: 0.1794 - val_accuracy: 0.9575\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1117 - accuracy: 0.9640 - val_loss: 0.2079 - val_accuracy: 0.9551\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0878 - accuracy: 0.9699 - val_loss: 0.2344 - val_accuracy: 0.9339\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0864 - accuracy: 0.9712 - val_loss: 0.2016 - val_accuracy: 0.9510\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0805 - accuracy: 0.9726 - val_loss: 0.3571 - val_accuracy: 0.8800\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0953 - accuracy: 0.9669 - val_loss: 0.2208 - val_accuracy: 0.9306\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0406 - accuracy: 0.9872 - val_loss: 0.2766 - val_accuracy: 0.9012\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.9012\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[2 6 4 ... 4 1 2]\n",
      "Confusion matrix\n",
      " [[254   2   0   0   6   0  14   0   0   0]\n",
      " [  2 224   1   0   0   0  11   0   0   0]\n",
      " [  2   0 224   0   0   0  30   0   0   0]\n",
      " [  3   3   3 251   0   0   1   0   0   0]\n",
      " [  0   0   0   0 251   0  17   0   0   0]\n",
      " [  0   0   0   0  16 178  10   0   0   0]\n",
      " [  0   0   0   0   0   0 288   0   0   0]\n",
      " [  0   0   1   0  16   6  50 157   4   0]\n",
      " [  0   0   1   0   1   0  17   0 211   0]\n",
      " [  0   0   0   0   6   0  19   0   0 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95       276\n",
      "           1       0.98      0.94      0.96       238\n",
      "           2       0.97      0.88      0.92       256\n",
      "           3       1.00      0.96      0.98       261\n",
      "           4       0.85      0.94      0.89       268\n",
      "           5       0.97      0.87      0.92       204\n",
      "           6       0.63      1.00      0.77       288\n",
      "           7       1.00      0.67      0.80       234\n",
      "           8       0.98      0.92      0.95       230\n",
      "           9       1.00      0.87      0.93       194\n",
      "\n",
      "    accuracy                           0.90      2449\n",
      "   macro avg       0.94      0.90      0.91      2449\n",
      "weighted avg       0.93      0.90      0.91      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  7\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_108 (Conv1D)         (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_109 (Conv1D)         (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_54 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_53 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 4s 22ms/step - loss: 2.1147 - accuracy: 0.2328 - val_loss: 2.2833 - val_accuracy: 0.2642\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 1.4754 - accuracy: 0.4545 - val_loss: 2.3072 - val_accuracy: 0.3067\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 1.2605 - accuracy: 0.5420 - val_loss: 2.2425 - val_accuracy: 0.1707\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 3s 18ms/step - loss: 1.1319 - accuracy: 0.5828 - val_loss: 2.1950 - val_accuracy: 0.2654\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0450 - accuracy: 0.6183 - val_loss: 2.2032 - val_accuracy: 0.2189\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9962 - accuracy: 0.6427 - val_loss: 2.1742 - val_accuracy: 0.2879\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9645 - accuracy: 0.6498 - val_loss: 2.3601 - val_accuracy: 0.4006\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8645 - accuracy: 0.6855 - val_loss: 2.1489 - val_accuracy: 0.3120\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8268 - accuracy: 0.6992 - val_loss: 2.1852 - val_accuracy: 0.3773\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7758 - accuracy: 0.7227 - val_loss: 2.2873 - val_accuracy: 0.3781\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7617 - accuracy: 0.7272 - val_loss: 1.9086 - val_accuracy: 0.3940\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7131 - accuracy: 0.7411 - val_loss: 1.7664 - val_accuracy: 0.5092\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6803 - accuracy: 0.7530 - val_loss: 1.7241 - val_accuracy: 0.5288\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6550 - accuracy: 0.7631 - val_loss: 1.5573 - val_accuracy: 0.6019\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6181 - accuracy: 0.7737 - val_loss: 1.5115 - val_accuracy: 0.5766\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6045 - accuracy: 0.7810 - val_loss: 1.5161 - val_accuracy: 0.5165\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5896 - accuracy: 0.7900 - val_loss: 1.4459 - val_accuracy: 0.6280\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5528 - accuracy: 0.7993 - val_loss: 1.2515 - val_accuracy: 0.6676\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5158 - accuracy: 0.8134 - val_loss: 1.4114 - val_accuracy: 0.6517\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4992 - accuracy: 0.8214 - val_loss: 1.2589 - val_accuracy: 0.7530\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4733 - accuracy: 0.8264 - val_loss: 1.2586 - val_accuracy: 0.7097\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4443 - accuracy: 0.8395 - val_loss: 1.2182 - val_accuracy: 0.6958\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4145 - accuracy: 0.8498 - val_loss: 1.1042 - val_accuracy: 0.7219\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4301 - accuracy: 0.8481 - val_loss: 1.1443 - val_accuracy: 0.6378\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4552 - accuracy: 0.8356 - val_loss: 0.8436 - val_accuracy: 0.8056\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4122 - accuracy: 0.8549 - val_loss: 0.9288 - val_accuracy: 0.7464\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3829 - accuracy: 0.8589 - val_loss: 0.9433 - val_accuracy: 0.7240\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3322 - accuracy: 0.8831 - val_loss: 0.9307 - val_accuracy: 0.7750\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3256 - accuracy: 0.8854 - val_loss: 1.2248 - val_accuracy: 0.7436\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3151 - accuracy: 0.8905 - val_loss: 1.0643 - val_accuracy: 0.7897\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3088 - accuracy: 0.8863 - val_loss: 0.6947 - val_accuracy: 0.8301\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2930 - accuracy: 0.8962 - val_loss: 0.7421 - val_accuracy: 0.7771\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2784 - accuracy: 0.9000 - val_loss: 0.7253 - val_accuracy: 0.7840\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3028 - accuracy: 0.8906 - val_loss: 0.7376 - val_accuracy: 0.7926\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2304 - accuracy: 0.9203 - val_loss: 0.6808 - val_accuracy: 0.7934\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2552 - accuracy: 0.9098 - val_loss: 0.6645 - val_accuracy: 0.7975\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2656 - accuracy: 0.9075 - val_loss: 0.4954 - val_accuracy: 0.8616\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2332 - accuracy: 0.9178 - val_loss: 0.4297 - val_accuracy: 0.8849\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2345 - accuracy: 0.9197 - val_loss: 0.6535 - val_accuracy: 0.8048\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2472 - accuracy: 0.9112 - val_loss: 0.7845 - val_accuracy: 0.8938\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2159 - accuracy: 0.9231 - val_loss: 0.4816 - val_accuracy: 0.8791\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9199 - val_loss: 0.5054 - val_accuracy: 0.8452\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1905 - accuracy: 0.9338 - val_loss: 0.5604 - val_accuracy: 0.8244\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2353 - accuracy: 0.9172 - val_loss: 0.4331 - val_accuracy: 0.8575\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1760 - accuracy: 0.9372 - val_loss: 0.3493 - val_accuracy: 0.8898\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1880 - accuracy: 0.9325 - val_loss: 0.5511 - val_accuracy: 0.8085\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1761 - accuracy: 0.9386 - val_loss: 0.4120 - val_accuracy: 0.8636\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1655 - accuracy: 0.9415 - val_loss: 0.4866 - val_accuracy: 0.8628\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1705 - accuracy: 0.9406 - val_loss: 0.4561 - val_accuracy: 0.8522\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1781 - accuracy: 0.9382 - val_loss: 0.4452 - val_accuracy: 0.8661\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1608 - accuracy: 0.9435 - val_loss: 0.4864 - val_accuracy: 0.8742\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1968 - accuracy: 0.9330 - val_loss: 0.3669 - val_accuracy: 0.8951\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1375 - accuracy: 0.9532 - val_loss: 0.3516 - val_accuracy: 0.9253\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1170 - accuracy: 0.9603 - val_loss: 0.5166 - val_accuracy: 0.8465\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1546 - accuracy: 0.9464 - val_loss: 0.6108 - val_accuracy: 0.8550\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1822 - accuracy: 0.9370 - val_loss: 0.3735 - val_accuracy: 0.9102\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1224 - accuracy: 0.9583 - val_loss: 0.4118 - val_accuracy: 0.8947\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1615 - accuracy: 0.9449 - val_loss: 0.7492 - val_accuracy: 0.7734\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1730 - accuracy: 0.9424 - val_loss: 0.4652 - val_accuracy: 0.8718\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1535 - accuracy: 0.9481 - val_loss: 0.9266 - val_accuracy: 0.7313\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1322 - accuracy: 0.9532 - val_loss: 0.6831 - val_accuracy: 0.8261\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1261 - accuracy: 0.9598 - val_loss: 0.7365 - val_accuracy: 0.7807\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1725 - accuracy: 0.9450 - val_loss: 0.4612 - val_accuracy: 0.8951\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1265 - accuracy: 0.9577 - val_loss: 0.4147 - val_accuracy: 0.8783\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1522 - accuracy: 0.9471 - val_loss: 0.4309 - val_accuracy: 0.9045\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1410 - accuracy: 0.9507 - val_loss: 1.0139 - val_accuracy: 0.9036\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2061 - accuracy: 0.9315 - val_loss: 0.7794 - val_accuracy: 0.8795\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0998 - accuracy: 0.9672 - val_loss: 1.0194 - val_accuracy: 0.9122\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.1091 - accuracy: 0.9643 - val_loss: 1.1933 - val_accuracy: 0.9196\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 2s 16ms/step - loss: 0.0961 - accuracy: 0.9693 - val_loss: 1.4109 - val_accuracy: 0.9224\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1323 - accuracy: 0.9580 - val_loss: 1.4518 - val_accuracy: 0.8452\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1784 - accuracy: 0.9417 - val_loss: 1.2806 - val_accuracy: 0.9036\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1093 - accuracy: 0.9645 - val_loss: 1.1675 - val_accuracy: 0.8755\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0854 - accuracy: 0.9719 - val_loss: 0.5961 - val_accuracy: 0.8367\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1149 - accuracy: 0.9623 - val_loss: 0.4406 - val_accuracy: 0.9073\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1349 - accuracy: 0.9546 - val_loss: 0.6061 - val_accuracy: 0.8269\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1101 - accuracy: 0.9633 - val_loss: 0.4367 - val_accuracy: 0.9077\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1188 - accuracy: 0.9608 - val_loss: 0.3992 - val_accuracy: 0.9032\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1185 - accuracy: 0.9587 - val_loss: 0.3848 - val_accuracy: 0.8938\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.1531 - accuracy: 0.9489 - val_loss: 0.3911 - val_accuracy: 0.9085\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 2s 13ms/step - loss: 0.1263 - accuracy: 0.9577 - val_loss: 0.4170 - val_accuracy: 0.8975\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0838 - accuracy: 0.9729 - val_loss: 0.6775 - val_accuracy: 0.7775\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1237 - accuracy: 0.9590 - val_loss: 0.3680 - val_accuracy: 0.8718\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1016 - accuracy: 0.9662 - val_loss: 0.3559 - val_accuracy: 0.8820\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0805 - accuracy: 0.9732 - val_loss: 0.7064 - val_accuracy: 0.8289\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0872 - accuracy: 0.9723 - val_loss: 1.0019 - val_accuracy: 0.8060\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0721 - accuracy: 0.9757 - val_loss: 0.9281 - val_accuracy: 0.9183\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0992 - accuracy: 0.9677 - val_loss: 1.2742 - val_accuracy: 0.8853\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1112 - accuracy: 0.9642 - val_loss: 0.9898 - val_accuracy: 0.8587\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0955 - accuracy: 0.9680 - val_loss: 0.8263 - val_accuracy: 0.9163\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0821 - accuracy: 0.9726 - val_loss: 0.2667 - val_accuracy: 0.9175\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0722 - accuracy: 0.9757 - val_loss: 0.4938 - val_accuracy: 0.9089\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0958 - accuracy: 0.9684 - val_loss: 0.5542 - val_accuracy: 0.8869\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.1251 - accuracy: 0.9594 - val_loss: 0.6566 - val_accuracy: 0.8661\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0957 - accuracy: 0.9678 - val_loss: 0.2885 - val_accuracy: 0.9457\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0831 - accuracy: 0.9722 - val_loss: 0.5017 - val_accuracy: 0.8759\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0873 - accuracy: 0.9711 - val_loss: 0.4050 - val_accuracy: 0.9036\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.0978 - accuracy: 0.9673 - val_loss: 0.4267 - val_accuracy: 0.8955\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0909 - accuracy: 0.9694 - val_loss: 0.3004 - val_accuracy: 0.9143\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0880 - accuracy: 0.9702 - val_loss: 0.2857 - val_accuracy: 0.9277\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2857 - accuracy: 0.9277\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[5 1 6 ... 8 3 6]\n",
      "Confusion matrix\n",
      " [[234   0   4   1   0   0  21   0   0   7]\n",
      " [  3 225   7   0   0   1  17   0   0   0]\n",
      " [  2   0 241   0   0   0   5   0   1   0]\n",
      " [  0   0  20 213   0   4   2   2   0   0]\n",
      " [  1   0   0   0 220   0   9   0   3   1]\n",
      " [  0   0   0   0   0 190   8   0   0   2]\n",
      " [  0   0   0   0   0   0 297   0   0   0]\n",
      " [  0   0   1   0   2   3   3 218   1   5]\n",
      " [  0   0   0   0   0   0   1   0 264   0]\n",
      " [  0   0   4   0   1   0  35   0   0 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       267\n",
      "           1       1.00      0.89      0.94       253\n",
      "           2       0.87      0.97      0.92       249\n",
      "           3       1.00      0.88      0.94       241\n",
      "           4       0.99      0.94      0.96       234\n",
      "           5       0.96      0.95      0.95       200\n",
      "           6       0.75      1.00      0.85       297\n",
      "           7       0.99      0.94      0.96       233\n",
      "           8       0.98      1.00      0.99       265\n",
      "           9       0.92      0.81      0.86       210\n",
      "\n",
      "    accuracy                           0.93      2449\n",
      "   macro avg       0.94      0.92      0.93      2449\n",
      "weighted avg       0.94      0.93      0.93      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  8\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_110 (Conv1D)         (None, 13, 64)            30784     \n",
      "                                                                 \n",
      " conv1d_111 (Conv1D)         (None, 6, 64)             32832     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 6, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_55 (MaxPoolin  (None, 3, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_54 (Flatten)        (None, 192)               0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 100)               19300     \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,326\n",
      "Trainable params: 84,126\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.1467 - accuracy: 0.2335 - val_loss: 2.3124 - val_accuracy: 0.1311\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4927 - accuracy: 0.4596 - val_loss: 2.2718 - val_accuracy: 0.1270\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2674 - accuracy: 0.5469 - val_loss: 2.2540 - val_accuracy: 0.1846\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1311 - accuracy: 0.5872 - val_loss: 2.2456 - val_accuracy: 0.1682\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0701 - accuracy: 0.6167 - val_loss: 2.1952 - val_accuracy: 0.1980\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9963 - accuracy: 0.6379 - val_loss: 2.1653 - val_accuracy: 0.2136\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9494 - accuracy: 0.6554 - val_loss: 2.1130 - val_accuracy: 0.2838\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8903 - accuracy: 0.6797 - val_loss: 2.1417 - val_accuracy: 0.2021\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8663 - accuracy: 0.6893 - val_loss: 2.1084 - val_accuracy: 0.2666\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8016 - accuracy: 0.7096 - val_loss: 2.0486 - val_accuracy: 0.3009\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.7151 - val_loss: 2.1506 - val_accuracy: 0.2756\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7460 - accuracy: 0.7297 - val_loss: 2.0573 - val_accuracy: 0.3001\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6999 - accuracy: 0.7459 - val_loss: 1.8959 - val_accuracy: 0.4087\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6919 - accuracy: 0.7497 - val_loss: 1.8504 - val_accuracy: 0.4214\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6540 - accuracy: 0.7613 - val_loss: 1.7068 - val_accuracy: 0.4900\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6156 - accuracy: 0.7795 - val_loss: 1.8746 - val_accuracy: 0.4320\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6133 - accuracy: 0.7770 - val_loss: 1.6101 - val_accuracy: 0.4692\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5689 - accuracy: 0.7939 - val_loss: 1.7021 - val_accuracy: 0.4230\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5698 - accuracy: 0.7969 - val_loss: 1.8093 - val_accuracy: 0.3777\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5297 - accuracy: 0.8099 - val_loss: 1.7879 - val_accuracy: 0.3887\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5190 - accuracy: 0.8106 - val_loss: 1.8185 - val_accuracy: 0.3830\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4880 - accuracy: 0.8278 - val_loss: 1.6571 - val_accuracy: 0.4618\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4924 - accuracy: 0.8254 - val_loss: 1.4527 - val_accuracy: 0.6096\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4757 - accuracy: 0.8327 - val_loss: 1.6464 - val_accuracy: 0.5398\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4020 - accuracy: 0.8539 - val_loss: 1.4653 - val_accuracy: 0.6427\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3937 - accuracy: 0.8598 - val_loss: 1.2588 - val_accuracy: 0.6623\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3992 - accuracy: 0.8552 - val_loss: 1.2603 - val_accuracy: 0.5953\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3908 - accuracy: 0.8617 - val_loss: 1.1934 - val_accuracy: 0.6909\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3813 - accuracy: 0.8626 - val_loss: 1.0868 - val_accuracy: 0.7166\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3733 - accuracy: 0.8628 - val_loss: 1.2657 - val_accuracy: 0.6149\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3398 - accuracy: 0.8756 - val_loss: 1.1920 - val_accuracy: 0.6456\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3485 - accuracy: 0.8743 - val_loss: 0.9978 - val_accuracy: 0.7158\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2871 - accuracy: 0.8970 - val_loss: 0.9778 - val_accuracy: 0.7297\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3279 - accuracy: 0.8826 - val_loss: 0.8716 - val_accuracy: 0.7742\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3044 - accuracy: 0.8953 - val_loss: 0.7266 - val_accuracy: 0.8191\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.2897 - accuracy: 0.8970 - val_loss: 0.8204 - val_accuracy: 0.7656\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.2900 - accuracy: 0.8997 - val_loss: 0.7735 - val_accuracy: 0.8052\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2354 - accuracy: 0.9165 - val_loss: 0.9697 - val_accuracy: 0.7048\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2260 - accuracy: 0.9191 - val_loss: 0.7938 - val_accuracy: 0.7726\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2058 - accuracy: 0.9284 - val_loss: 0.6538 - val_accuracy: 0.8408\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2326 - accuracy: 0.9151 - val_loss: 0.6227 - val_accuracy: 0.8142\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2273 - accuracy: 0.9215 - val_loss: 0.7292 - val_accuracy: 0.7820\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2243 - accuracy: 0.9203 - val_loss: 0.4956 - val_accuracy: 0.8567\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2333 - accuracy: 0.9198 - val_loss: 0.8601 - val_accuracy: 0.7236\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1869 - accuracy: 0.9366 - val_loss: 0.6101 - val_accuracy: 0.8289\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2068 - accuracy: 0.9294 - val_loss: 0.4662 - val_accuracy: 0.8702\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1876 - accuracy: 0.9362 - val_loss: 0.3910 - val_accuracy: 0.9269\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1897 - accuracy: 0.9352 - val_loss: 0.5115 - val_accuracy: 0.8804\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2458 - accuracy: 0.9152 - val_loss: 0.6368 - val_accuracy: 0.8763\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1564 - accuracy: 0.9452 - val_loss: 0.6317 - val_accuracy: 0.8322\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1894 - accuracy: 0.9335 - val_loss: 0.7079 - val_accuracy: 0.7771\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1915 - accuracy: 0.9332 - val_loss: 0.6777 - val_accuracy: 0.8448\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1708 - accuracy: 0.9385 - val_loss: 0.5345 - val_accuracy: 0.8971\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1689 - accuracy: 0.9415 - val_loss: 0.8767 - val_accuracy: 0.7089\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2188 - accuracy: 0.9245 - val_loss: 0.6917 - val_accuracy: 0.8126\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1527 - accuracy: 0.9480 - val_loss: 0.6976 - val_accuracy: 0.7897\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 0.1922 - accuracy: 0.9322 - val_loss: 0.7204 - val_accuracy: 0.7983\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.1595 - accuracy: 0.9453 - val_loss: 0.5900 - val_accuracy: 0.8256\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1542 - accuracy: 0.9463 - val_loss: 0.8390 - val_accuracy: 0.7362\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1334 - accuracy: 0.9540 - val_loss: 1.0150 - val_accuracy: 0.7611\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.1805 - accuracy: 0.9346 - val_loss: 0.7599 - val_accuracy: 0.8232\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1060 - accuracy: 0.9644 - val_loss: 0.7800 - val_accuracy: 0.7877\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1414 - accuracy: 0.9511 - val_loss: 0.7613 - val_accuracy: 0.7636\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1378 - accuracy: 0.9536 - val_loss: 0.6660 - val_accuracy: 0.7991\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1404 - accuracy: 0.9513 - val_loss: 0.6556 - val_accuracy: 0.7685\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1184 - accuracy: 0.9599 - val_loss: 0.6190 - val_accuracy: 0.8305\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1419 - accuracy: 0.9516 - val_loss: 0.5149 - val_accuracy: 0.8677\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1049 - accuracy: 0.9648 - val_loss: 0.5462 - val_accuracy: 0.8506\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1114 - accuracy: 0.9622 - val_loss: 0.5902 - val_accuracy: 0.8212\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2010 - accuracy: 0.9337 - val_loss: 0.7552 - val_accuracy: 0.7219\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1470 - accuracy: 0.9492 - val_loss: 0.9151 - val_accuracy: 0.7064\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1326 - accuracy: 0.9511 - val_loss: 0.7852 - val_accuracy: 0.7587\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1479 - accuracy: 0.9507 - val_loss: 0.5779 - val_accuracy: 0.8359\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0903 - accuracy: 0.9700 - val_loss: 0.7833 - val_accuracy: 0.7264\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1342 - accuracy: 0.9536 - val_loss: 0.9325 - val_accuracy: 0.6742\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1000 - accuracy: 0.9667 - val_loss: 0.6688 - val_accuracy: 0.7987\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0753 - accuracy: 0.9766 - val_loss: 0.5877 - val_accuracy: 0.8199\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1116 - accuracy: 0.9609 - val_loss: 0.7808 - val_accuracy: 0.7550\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1886 - accuracy: 0.9364 - val_loss: 0.6673 - val_accuracy: 0.7889\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1159 - accuracy: 0.9594 - val_loss: 0.6632 - val_accuracy: 0.8007\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 3s 20ms/step - loss: 0.1027 - accuracy: 0.9664 - val_loss: 0.6014 - val_accuracy: 0.8252\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.1045 - accuracy: 0.9663 - val_loss: 0.6001 - val_accuracy: 0.8232\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1043 - accuracy: 0.9656 - val_loss: 0.6618 - val_accuracy: 0.7950\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0985 - accuracy: 0.9671 - val_loss: 0.7444 - val_accuracy: 0.7272\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1374 - accuracy: 0.9516 - val_loss: 0.9316 - val_accuracy: 0.6835\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0857 - accuracy: 0.9721 - val_loss: 0.7839 - val_accuracy: 0.7444\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1048 - accuracy: 0.9648 - val_loss: 1.2421 - val_accuracy: 0.7460\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0941 - accuracy: 0.9703 - val_loss: 0.7573 - val_accuracy: 0.7660\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1229 - accuracy: 0.9564 - val_loss: 1.0645 - val_accuracy: 0.6354\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1262 - accuracy: 0.9580 - val_loss: 0.9737 - val_accuracy: 0.6758\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1534 - accuracy: 0.9471 - val_loss: 0.7955 - val_accuracy: 0.7293\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0961 - accuracy: 0.9684 - val_loss: 0.8078 - val_accuracy: 0.7436\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.0737 - accuracy: 0.9763 - val_loss: 0.7298 - val_accuracy: 0.8097\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0739 - accuracy: 0.9750 - val_loss: 0.5815 - val_accuracy: 0.8375\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1061 - accuracy: 0.9612 - val_loss: 0.5727 - val_accuracy: 0.8097\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1093 - accuracy: 0.9642 - val_loss: 0.5442 - val_accuracy: 0.8469\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0729 - accuracy: 0.9744 - val_loss: 0.5892 - val_accuracy: 0.8432\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0999 - accuracy: 0.9663 - val_loss: 0.5939 - val_accuracy: 0.8216\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0850 - accuracy: 0.9715 - val_loss: 0.5817 - val_accuracy: 0.8236\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1542 - accuracy: 0.9506 - val_loss: 0.5506 - val_accuracy: 0.8481\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5506 - accuracy: 0.8481\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[4 8 8 ... 2 0 4]\n",
      "Confusion matrix\n",
      " [[223   0   0   8   0   1  18   0   0   0]\n",
      " [ 10 195   3   4   0   5  27   0   4   5]\n",
      " [  0   3 265   0   0   1  19   0   4   0]\n",
      " [  0   0   9 150   0  37  25   4   1   1]\n",
      " [  0   0   3   0 206   0  23   1   6   5]\n",
      " [  0   0   0   0   6 207  21   0   1   1]\n",
      " [  0   0   0   0   0   1 307   1   0   0]\n",
      " [  0   0   9   0   1  14  37 151  16   1]\n",
      " [  0   0   1   0   0   0   8   0 228   0]\n",
      " [  0   2   1   0   7   0  10   2   5 145]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       250\n",
      "           1       0.97      0.77      0.86       253\n",
      "           2       0.91      0.91      0.91       292\n",
      "           3       0.93      0.66      0.77       227\n",
      "           4       0.94      0.84      0.89       244\n",
      "           5       0.78      0.88      0.82       236\n",
      "           6       0.62      0.99      0.76       309\n",
      "           7       0.95      0.66      0.78       229\n",
      "           8       0.86      0.96      0.91       237\n",
      "           9       0.92      0.84      0.88       172\n",
      "\n",
      "    accuracy                           0.85      2449\n",
      "   macro avg       0.88      0.84      0.85      2449\n",
      "weighted avg       0.88      0.85      0.85      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  9\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_112 (Conv1D)         (None, 12, 64)            34624     \n",
      "                                                                 \n",
      " conv1d_113 (Conv1D)         (None, 4, 64)             36928     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 4, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_56 (MaxPoolin  (None, 2, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 100)               12900     \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,862\n",
      "Trainable params: 85,662\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2438 - accuracy: 0.1892 - val_loss: 2.2916 - val_accuracy: 0.1290\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6365 - accuracy: 0.3864 - val_loss: 2.3182 - val_accuracy: 0.1376\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3195 - accuracy: 0.5283 - val_loss: 2.2932 - val_accuracy: 0.1258\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1662 - accuracy: 0.5786 - val_loss: 2.2898 - val_accuracy: 0.1266\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0993 - accuracy: 0.6020 - val_loss: 2.2664 - val_accuracy: 0.1813\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0077 - accuracy: 0.6360 - val_loss: 2.2144 - val_accuracy: 0.2311\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9782 - accuracy: 0.6442 - val_loss: 2.1571 - val_accuracy: 0.3499\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9410 - accuracy: 0.6570 - val_loss: 2.0578 - val_accuracy: 0.3524\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8711 - accuracy: 0.6848 - val_loss: 2.0425 - val_accuracy: 0.3397\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8312 - accuracy: 0.6910 - val_loss: 1.9717 - val_accuracy: 0.3940\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7877 - accuracy: 0.7140 - val_loss: 1.8822 - val_accuracy: 0.4594\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8000 - accuracy: 0.7110 - val_loss: 1.8624 - val_accuracy: 0.4357\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7391 - accuracy: 0.7264 - val_loss: 1.8765 - val_accuracy: 0.4777\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7421 - accuracy: 0.7331 - val_loss: 1.7869 - val_accuracy: 0.4230\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6817 - accuracy: 0.7512 - val_loss: 1.8346 - val_accuracy: 0.3593\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6778 - accuracy: 0.7535 - val_loss: 1.7170 - val_accuracy: 0.4463\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6232 - accuracy: 0.7752 - val_loss: 1.6101 - val_accuracy: 0.5035\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6250 - accuracy: 0.7730 - val_loss: 1.5077 - val_accuracy: 0.5970\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6442 - accuracy: 0.7678 - val_loss: 1.6862 - val_accuracy: 0.4500\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6183 - accuracy: 0.7775 - val_loss: 1.5833 - val_accuracy: 0.5116\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5534 - accuracy: 0.8030 - val_loss: 1.5021 - val_accuracy: 0.5770\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5364 - accuracy: 0.8051 - val_loss: 1.3832 - val_accuracy: 0.6733\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5606 - accuracy: 0.7985 - val_loss: 1.4869 - val_accuracy: 0.5210\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5001 - accuracy: 0.8214 - val_loss: 1.3590 - val_accuracy: 0.6243\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5066 - accuracy: 0.8196 - val_loss: 1.5168 - val_accuracy: 0.6002\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4753 - accuracy: 0.8326 - val_loss: 1.6498 - val_accuracy: 0.5586\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5069 - accuracy: 0.8191 - val_loss: 1.6474 - val_accuracy: 0.5749\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4686 - accuracy: 0.8302 - val_loss: 1.5308 - val_accuracy: 0.6252\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4212 - accuracy: 0.8484 - val_loss: 1.4701 - val_accuracy: 0.6207\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4214 - accuracy: 0.8491 - val_loss: 1.4159 - val_accuracy: 0.6060\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4134 - accuracy: 0.8546 - val_loss: 1.3588 - val_accuracy: 0.6203\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3674 - accuracy: 0.8669 - val_loss: 1.2890 - val_accuracy: 0.6235\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4123 - accuracy: 0.8522 - val_loss: 1.3963 - val_accuracy: 0.6313\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3633 - accuracy: 0.8668 - val_loss: 1.3989 - val_accuracy: 0.5982\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3757 - accuracy: 0.8650 - val_loss: 1.4568 - val_accuracy: 0.5141\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3606 - accuracy: 0.8715 - val_loss: 1.3711 - val_accuracy: 0.5590\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3347 - accuracy: 0.8801 - val_loss: 1.2775 - val_accuracy: 0.5864\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3198 - accuracy: 0.8879 - val_loss: 1.3105 - val_accuracy: 0.5802\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3267 - accuracy: 0.8805 - val_loss: 1.2335 - val_accuracy: 0.5974\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2889 - accuracy: 0.8910 - val_loss: 1.0262 - val_accuracy: 0.6709\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2969 - accuracy: 0.8970 - val_loss: 1.0589 - val_accuracy: 0.6460\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3480 - accuracy: 0.8764 - val_loss: 0.8544 - val_accuracy: 0.7599\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2749 - accuracy: 0.9024 - val_loss: 0.7655 - val_accuracy: 0.8085\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3684 - accuracy: 0.8679 - val_loss: 0.7922 - val_accuracy: 0.7615\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2909 - accuracy: 0.8969 - val_loss: 0.8185 - val_accuracy: 0.7468\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2791 - accuracy: 0.8950 - val_loss: 0.8728 - val_accuracy: 0.7591\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2555 - accuracy: 0.9069 - val_loss: 0.8661 - val_accuracy: 0.7301\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2385 - accuracy: 0.9135 - val_loss: 0.7115 - val_accuracy: 0.8077\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2749 - accuracy: 0.9027 - val_loss: 0.6888 - val_accuracy: 0.8024\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2518 - accuracy: 0.9086 - val_loss: 0.7404 - val_accuracy: 0.7534\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2073 - accuracy: 0.9243 - val_loss: 0.5532 - val_accuracy: 0.8587\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2633 - accuracy: 0.9050 - val_loss: 0.6767 - val_accuracy: 0.8310\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2741 - accuracy: 0.9001 - val_loss: 0.7389 - val_accuracy: 0.7762\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2357 - accuracy: 0.9160 - val_loss: 0.7360 - val_accuracy: 0.8101\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2302 - accuracy: 0.9175 - val_loss: 0.8130 - val_accuracy: 0.7677\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2257 - accuracy: 0.9194 - val_loss: 0.7379 - val_accuracy: 0.7758\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2186 - accuracy: 0.9231 - val_loss: 0.7261 - val_accuracy: 0.7689\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2068 - accuracy: 0.9284 - val_loss: 0.5846 - val_accuracy: 0.8506\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2127 - accuracy: 0.9271 - val_loss: 0.5051 - val_accuracy: 0.8820\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2077 - accuracy: 0.9271 - val_loss: 0.6438 - val_accuracy: 0.8755\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2016 - accuracy: 0.9280 - val_loss: 0.5502 - val_accuracy: 0.8726\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9197 - val_loss: 0.7407 - val_accuracy: 0.7575\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2051 - accuracy: 0.9299 - val_loss: 0.6053 - val_accuracy: 0.8301\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2340 - accuracy: 0.9191 - val_loss: 0.5244 - val_accuracy: 0.8424\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2114 - accuracy: 0.9235 - val_loss: 0.6780 - val_accuracy: 0.8085\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1685 - accuracy: 0.9402 - val_loss: 0.5614 - val_accuracy: 0.8722\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1809 - accuracy: 0.9378 - val_loss: 0.5401 - val_accuracy: 0.8628\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1833 - accuracy: 0.9333 - val_loss: 0.5079 - val_accuracy: 0.8693\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1687 - accuracy: 0.9417 - val_loss: 0.4398 - val_accuracy: 0.9057\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1842 - accuracy: 0.9357 - val_loss: 0.3774 - val_accuracy: 0.9032\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2181 - accuracy: 0.9237 - val_loss: 0.3695 - val_accuracy: 0.9036\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1866 - accuracy: 0.9331 - val_loss: 0.4081 - val_accuracy: 0.8800\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1518 - accuracy: 0.9474 - val_loss: 0.3841 - val_accuracy: 0.9008\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1696 - accuracy: 0.9389 - val_loss: 0.3722 - val_accuracy: 0.8910\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2140 - accuracy: 0.9266 - val_loss: 0.5692 - val_accuracy: 0.7979\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.1560 - accuracy: 0.9443 - val_loss: 0.3591 - val_accuracy: 0.9114\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2010 - accuracy: 0.9280 - val_loss: 0.5209 - val_accuracy: 0.8305\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1772 - accuracy: 0.9353 - val_loss: 0.3187 - val_accuracy: 0.9134\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1590 - accuracy: 0.9445 - val_loss: 0.4438 - val_accuracy: 0.8857\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1568 - accuracy: 0.9472 - val_loss: 0.3508 - val_accuracy: 0.9241\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1272 - accuracy: 0.9557 - val_loss: 0.3778 - val_accuracy: 0.9020\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1608 - accuracy: 0.9438 - val_loss: 0.3631 - val_accuracy: 0.9200\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1414 - accuracy: 0.9503 - val_loss: 0.3593 - val_accuracy: 0.9126\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.1884 - accuracy: 0.9325 - val_loss: 0.3092 - val_accuracy: 0.9094\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.1325 - accuracy: 0.9532 - val_loss: 0.2745 - val_accuracy: 0.9306\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2018 - accuracy: 0.9313 - val_loss: 0.3984 - val_accuracy: 0.8767\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1600 - accuracy: 0.9446 - val_loss: 0.2962 - val_accuracy: 0.9245\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1532 - accuracy: 0.9446 - val_loss: 0.2473 - val_accuracy: 0.9359\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1172 - accuracy: 0.9581 - val_loss: 0.3445 - val_accuracy: 0.8979\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1797 - accuracy: 0.9372 - val_loss: 0.5878 - val_accuracy: 0.8036\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1553 - accuracy: 0.9449 - val_loss: 0.3840 - val_accuracy: 0.8763\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1531 - accuracy: 0.9487 - val_loss: 0.3391 - val_accuracy: 0.9004\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1459 - accuracy: 0.9474 - val_loss: 0.2318 - val_accuracy: 0.9518\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9584 - val_loss: 0.3257 - val_accuracy: 0.9040\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.1101 - accuracy: 0.9624 - val_loss: 0.4748 - val_accuracy: 0.8473\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1858 - accuracy: 0.9356 - val_loss: 0.4019 - val_accuracy: 0.8922\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1521 - accuracy: 0.9481 - val_loss: 0.6827 - val_accuracy: 0.8073\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1525 - accuracy: 0.9461 - val_loss: 0.4216 - val_accuracy: 0.8718\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1464 - accuracy: 0.9486 - val_loss: 0.5009 - val_accuracy: 0.8424\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.1615 - accuracy: 0.9469 - val_loss: 0.2991 - val_accuracy: 0.9253\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2991 - accuracy: 0.9253\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "[9 3 3 ... 2 1 2]\n",
      "Confusion matrix\n",
      " [[276  12   1   0   0   0   1   0   0   4]\n",
      " [  1 214   0   0   4   0   8   0   0   0]\n",
      " [ 12   2 221   3   5   0  13   1   1   5]\n",
      " [  1  19  14 204   1   0   5   0   0   1]\n",
      " [  0   0   0   0 252   0   4   0   1   0]\n",
      " [  0   0   5   0   1 177   7   1   4   1]\n",
      " [  0   0   0   0   0   0 278   0   0   0]\n",
      " [  0   0   0   0   3   0  11 222   9   0]\n",
      " [  0   0   0   0   2   0   9   0 232   0]\n",
      " [  0   0   0   0   1   0   2   0   8 190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       294\n",
      "           1       0.87      0.94      0.90       227\n",
      "           2       0.92      0.84      0.88       263\n",
      "           3       0.99      0.83      0.90       245\n",
      "           4       0.94      0.98      0.96       257\n",
      "           5       1.00      0.90      0.95       196\n",
      "           6       0.82      1.00      0.90       278\n",
      "           7       0.99      0.91      0.95       245\n",
      "           8       0.91      0.95      0.93       243\n",
      "           9       0.95      0.95      0.95       201\n",
      "\n",
      "    accuracy                           0.93      2449\n",
      "   macro avg       0.93      0.92      0.93      2449\n",
      "weighted avg       0.93      0.93      0.93      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      " kernel size :  10\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_114 (Conv1D)         (None, 11, 64)            38464     \n",
      "                                                                 \n",
      " conv1d_115 (Conv1D)         (None, 2, 64)             41024     \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 2, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_57 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_56 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 100)               6500      \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,398\n",
      "Trainable params: 87,198\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.2656 - accuracy: 0.1855 - val_loss: 2.2908 - val_accuracy: 0.1433\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.7140 - accuracy: 0.3501 - val_loss: 2.2331 - val_accuracy: 0.1637\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.4216 - accuracy: 0.4770 - val_loss: 2.2878 - val_accuracy: 0.1274\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2551 - accuracy: 0.5436 - val_loss: 2.2896 - val_accuracy: 0.1217\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1297 - accuracy: 0.5890 - val_loss: 2.2922 - val_accuracy: 0.1625\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0533 - accuracy: 0.6219 - val_loss: 2.3112 - val_accuracy: 0.2315\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0177 - accuracy: 0.6301 - val_loss: 2.2976 - val_accuracy: 0.2732\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9903 - accuracy: 0.6486 - val_loss: 2.3521 - val_accuracy: 0.1948\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.9575 - accuracy: 0.6539 - val_loss: 2.3840 - val_accuracy: 0.2679\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.8970 - accuracy: 0.6782 - val_loss: 2.3937 - val_accuracy: 0.2728\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.8808 - accuracy: 0.6832 - val_loss: 2.3695 - val_accuracy: 0.2920\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8352 - accuracy: 0.7054 - val_loss: 2.4301 - val_accuracy: 0.2168\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.8179 - accuracy: 0.7074 - val_loss: 2.3284 - val_accuracy: 0.2470\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8169 - accuracy: 0.7033 - val_loss: 2.3834 - val_accuracy: 0.2548\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7618 - accuracy: 0.7246 - val_loss: 2.4002 - val_accuracy: 0.2340\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8007 - accuracy: 0.7145 - val_loss: 2.4606 - val_accuracy: 0.2470\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7495 - accuracy: 0.7317 - val_loss: 2.4753 - val_accuracy: 0.2458\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.7407 - accuracy: 0.7311 - val_loss: 2.3996 - val_accuracy: 0.2442\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.7019 - accuracy: 0.7432 - val_loss: 2.3665 - val_accuracy: 0.2756\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.7071 - accuracy: 0.7431 - val_loss: 2.2788 - val_accuracy: 0.2458\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6802 - accuracy: 0.7563 - val_loss: 2.1144 - val_accuracy: 0.3026\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6839 - accuracy: 0.7558 - val_loss: 2.0502 - val_accuracy: 0.3099\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6680 - accuracy: 0.7604 - val_loss: 2.0066 - val_accuracy: 0.2989\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6246 - accuracy: 0.7730 - val_loss: 1.9225 - val_accuracy: 0.3855\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6128 - accuracy: 0.7757 - val_loss: 1.9921 - val_accuracy: 0.3454\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6094 - accuracy: 0.7784 - val_loss: 1.8695 - val_accuracy: 0.4034\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.5990 - accuracy: 0.7906 - val_loss: 1.8792 - val_accuracy: 0.4018\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5902 - accuracy: 0.7799 - val_loss: 1.9305 - val_accuracy: 0.3699\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5474 - accuracy: 0.8031 - val_loss: 1.6454 - val_accuracy: 0.4651\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5809 - accuracy: 0.7932 - val_loss: 1.8208 - val_accuracy: 0.3944\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5683 - accuracy: 0.7948 - val_loss: 1.6161 - val_accuracy: 0.5272\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5113 - accuracy: 0.8153 - val_loss: 1.9402 - val_accuracy: 0.4667\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5038 - accuracy: 0.8200 - val_loss: 2.0329 - val_accuracy: 0.4590\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.5101 - accuracy: 0.8181 - val_loss: 2.0869 - val_accuracy: 0.4512\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.4949 - accuracy: 0.8163 - val_loss: 2.2396 - val_accuracy: 0.4198\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.5326 - accuracy: 0.8057 - val_loss: 1.9679 - val_accuracy: 0.4782\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5152 - accuracy: 0.8139 - val_loss: 1.9163 - val_accuracy: 0.5120\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.4725 - accuracy: 0.8324 - val_loss: 1.4560 - val_accuracy: 0.5092\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4923 - accuracy: 0.8247 - val_loss: 1.3156 - val_accuracy: 0.5231\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4637 - accuracy: 0.8331 - val_loss: 1.4171 - val_accuracy: 0.4884\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4435 - accuracy: 0.8376 - val_loss: 1.3420 - val_accuracy: 0.5447\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4649 - accuracy: 0.8309 - val_loss: 1.1985 - val_accuracy: 0.6644\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4450 - accuracy: 0.8372 - val_loss: 1.3187 - val_accuracy: 0.6460\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4205 - accuracy: 0.8467 - val_loss: 1.5492 - val_accuracy: 0.5190\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4165 - accuracy: 0.8487 - val_loss: 1.3168 - val_accuracy: 0.6137\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 3s 21ms/step - loss: 0.4188 - accuracy: 0.8496 - val_loss: 1.3928 - val_accuracy: 0.6113\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3880 - accuracy: 0.8591 - val_loss: 1.1732 - val_accuracy: 0.6635\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4605 - accuracy: 0.8337 - val_loss: 1.6126 - val_accuracy: 0.5500\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3928 - accuracy: 0.8584 - val_loss: 1.2250 - val_accuracy: 0.6893\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3728 - accuracy: 0.8646 - val_loss: 1.2381 - val_accuracy: 0.6439\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3768 - accuracy: 0.8665 - val_loss: 1.2356 - val_accuracy: 0.6092\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3857 - accuracy: 0.8645 - val_loss: 1.2601 - val_accuracy: 0.5353\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3949 - accuracy: 0.8603 - val_loss: 1.3518 - val_accuracy: 0.5161\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.4032 - accuracy: 0.8564 - val_loss: 1.2250 - val_accuracy: 0.5676\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3770 - accuracy: 0.8642 - val_loss: 1.2254 - val_accuracy: 0.5704\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3922 - accuracy: 0.8577 - val_loss: 1.4532 - val_accuracy: 0.4663\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3185 - accuracy: 0.8862 - val_loss: 1.2166 - val_accuracy: 0.5382\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3416 - accuracy: 0.8797 - val_loss: 1.2605 - val_accuracy: 0.5296\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.4165 - accuracy: 0.8491 - val_loss: 1.2718 - val_accuracy: 0.5349\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3559 - accuracy: 0.8698 - val_loss: 1.4373 - val_accuracy: 0.4916\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3755 - accuracy: 0.8683 - val_loss: 1.1865 - val_accuracy: 0.5815\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3444 - accuracy: 0.8790 - val_loss: 1.2403 - val_accuracy: 0.5541\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3245 - accuracy: 0.8819 - val_loss: 1.4604 - val_accuracy: 0.4394\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3628 - accuracy: 0.8702 - val_loss: 1.4352 - val_accuracy: 0.4835\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3525 - accuracy: 0.8747 - val_loss: 1.7679 - val_accuracy: 0.3936\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3212 - accuracy: 0.8851 - val_loss: 1.7018 - val_accuracy: 0.3949\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.3387 - accuracy: 0.8785 - val_loss: 1.6064 - val_accuracy: 0.4304\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.3405 - accuracy: 0.8814 - val_loss: 1.4856 - val_accuracy: 0.4626\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3140 - accuracy: 0.8887 - val_loss: 1.3929 - val_accuracy: 0.5161\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3407 - accuracy: 0.8764 - val_loss: 1.3600 - val_accuracy: 0.5010\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3265 - accuracy: 0.8848 - val_loss: 1.4120 - val_accuracy: 0.5235\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3086 - accuracy: 0.8898 - val_loss: 1.5330 - val_accuracy: 0.5096\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2791 - accuracy: 0.8999 - val_loss: 1.3871 - val_accuracy: 0.5463\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2982 - accuracy: 0.8950 - val_loss: 1.3499 - val_accuracy: 0.5982\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2907 - accuracy: 0.8971 - val_loss: 1.5469 - val_accuracy: 0.5876\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3155 - accuracy: 0.8883 - val_loss: 2.0951 - val_accuracy: 0.5745\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3012 - accuracy: 0.8911 - val_loss: 2.3359 - val_accuracy: 0.5782\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3173 - accuracy: 0.8869 - val_loss: 2.1651 - val_accuracy: 0.6754\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3020 - accuracy: 0.8920 - val_loss: 1.5322 - val_accuracy: 0.6141\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2762 - accuracy: 0.9024 - val_loss: 1.8513 - val_accuracy: 0.6011\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.3234 - accuracy: 0.8842 - val_loss: 1.6824 - val_accuracy: 0.6072\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2740 - accuracy: 0.9032 - val_loss: 1.2779 - val_accuracy: 0.6133\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2770 - accuracy: 0.9005 - val_loss: 1.1974 - val_accuracy: 0.5704\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.3039 - accuracy: 0.8913 - val_loss: 1.3234 - val_accuracy: 0.5321\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2553 - accuracy: 0.9121 - val_loss: 1.1581 - val_accuracy: 0.5860\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2523 - accuracy: 0.9125 - val_loss: 1.2049 - val_accuracy: 0.5725\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.2825 - accuracy: 0.9018 - val_loss: 1.1689 - val_accuracy: 0.5958\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2889 - accuracy: 0.8968 - val_loss: 1.0063 - val_accuracy: 0.6795\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2460 - accuracy: 0.9144 - val_loss: 0.9529 - val_accuracy: 0.7285\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2871 - accuracy: 0.8990 - val_loss: 0.9087 - val_accuracy: 0.6799\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2740 - accuracy: 0.9002 - val_loss: 1.4801 - val_accuracy: 0.5182\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.3524 - accuracy: 0.8760 - val_loss: 1.2869 - val_accuracy: 0.5537\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.2650 - accuracy: 0.9070 - val_loss: 1.4211 - val_accuracy: 0.5672\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2717 - accuracy: 0.9010 - val_loss: 1.1427 - val_accuracy: 0.5929\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2636 - accuracy: 0.9100 - val_loss: 0.9618 - val_accuracy: 0.6950\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2973 - accuracy: 0.8942 - val_loss: 1.0229 - val_accuracy: 0.7460\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2977 - accuracy: 0.8942 - val_loss: 0.8946 - val_accuracy: 0.7713\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.2565 - accuracy: 0.9082 - val_loss: 1.0334 - val_accuracy: 0.7236\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.2111 - accuracy: 0.9277 - val_loss: 0.8593 - val_accuracy: 0.7632\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.2292 - accuracy: 0.9198 - val_loss: 0.8305 - val_accuracy: 0.7762\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.8305 - accuracy: 0.7762\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "[6 8 7 ... 3 6 8]\n",
      "Confusion matrix\n",
      " [[193   0   2   1   5   0  63   0   0   1]\n",
      " [ 21  55   2   6   0   0 158   0   0   0]\n",
      " [  0   0 210   7   3   3  25   0   3   0]\n",
      " [  4   0  10 241   0   0   8   0   1   0]\n",
      " [  0   0   0   0 229   0  26   0   2   0]\n",
      " [  0   0   0   0  29 143  41   1   0   0]\n",
      " [  0   2   0   0   0   0 257   0   0   0]\n",
      " [  0   0   0   0   2   8  31 207  11   0]\n",
      " [  0   0   0   0   3   0  27   1 211   0]\n",
      " [  0   0   0   0   5   0   5   0  31 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80       265\n",
      "           1       0.96      0.23      0.37       242\n",
      "           2       0.94      0.84      0.88       251\n",
      "           3       0.95      0.91      0.93       264\n",
      "           4       0.83      0.89      0.86       257\n",
      "           5       0.93      0.67      0.78       214\n",
      "           6       0.40      0.99      0.57       259\n",
      "           7       0.99      0.80      0.88       259\n",
      "           8       0.81      0.87      0.84       242\n",
      "           9       0.99      0.79      0.88       196\n",
      "\n",
      "    accuracy                           0.78      2449\n",
      "   macro avg       0.87      0.77      0.78      2449\n",
      "weighted avg       0.86      0.78      0.78      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed_modified\", \"*\", \"*.csv\"))\n",
    "# # print(raw_angle_files_1)\n",
    "# all_filenames = [i for i in raw_angle_files_1]\n",
    "# df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "# data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\tree_classifier_extracted.csv\")\n",
    "# print(df.shape)\n",
    "# df = df.sample(frac = 1)\n",
    "# # df.iloc[1:10,:]\n",
    "\n",
    "\n",
    "list_train_accuracy=[]\n",
    "list_val_accuracy=[]\n",
    "for filter_size in range(2,11):\n",
    "    data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_extended_modified.csv\")\n",
    "    data=pd.get_dummies(data,columns=['target'])\n",
    "    data=data.to_numpy()\n",
    "    x,y=split_sequences(data,20)\n",
    "    # x=x[None:]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # data=data.to_numpy()\n",
    "    print(data.shape)\n",
    "\n",
    "    # y=data.iloc[:,26:]\n",
    "    # x=data.iloc[:,:26]\n",
    "    # # x=data[-1:26]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # x=x.to_numpy()\n",
    "    # y=y.to_numpy()\n",
    "    x_train,x_test,y_train,y_test= train_test_split(x, y, test_size = 0.2)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(type(y_train))\n",
    "    print(\" kernel size : \",filter_size)\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=filter_size, activation='relu', input_shape=(20,60)))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=filter_size, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    # # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # print(y_train)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # # print(x.shape)\n",
    "    history = model.fit(x_train, y_train,batch_size=config.BATCH_SIZE,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    y_pred=model.predict(x_test)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    y_test=np.argmax(y_test,axis=1)\n",
    "    list_train_accuracy.append(history.history[\"accuracy\"][-1])\n",
    "    list_val_accuracy.append(history.history[\"val_accuracy\"][-1])\n",
    "    print(y_pred)\n",
    "    cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "    print('Confusion matrix\\n',cf_matrix)\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9886642098426819, 0.983047366142273, 0.9720180034637451, 0.9810048937797546, 0.9881535768508911, 0.9895833134651184, 0.969464898109436, 0.9534313678741455, 0.9097222089767456]\n",
      "[0.15720702707767487, 0.36831361055374146, 0.8542262315750122, 0.813393235206604, 0.9093507528305054, 0.6002449989318848, 0.8844426274299622, 0.9526337385177612, 0.8178848624229431]\n"
     ]
    }
   ],
   "source": [
    "print(list_train_accuracy)\n",
    "print(list_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.plot( range( 2, 11 ),list_train_accuracy, marker = 's' )\n",
    "plt.plot( range( 2, 11),list_val_accuracy, marker = 's' )\n",
    "plt.xlabel( 'Kernel Size', fontsize = 17 )\n",
    "plt.ylabel( 'Accuracy', fontsize = 18 )\n",
    "plt.legend(['Train Set', 'Test Set'], loc='best',fontsize = 14)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks( fontsize = 14 )\n",
    "plt.yticks( fontsize = 14 )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[178   0   1   5  11   0  51   0   0   1]\n",
      " [  1 142   9  33   5   5  21   0  17   2]\n",
      " [  2   0 206   2  10   0  11   6  11   7]\n",
      " [  0   0  11 237   0   4   3   0   0   2]\n",
      " [  0   0   0   2 224   0  11   0   3   0]\n",
      " [  0   0   0   0  11 157  28   2   5   4]\n",
      " [  0   1   0   0   1   0 297   0   5   1]\n",
      " [  0   0   2   0   2   0  28 173  10  20]\n",
      " [  0   0   0   0   7   0   7   0 243  10]\n",
      " [  0   0   0   0   6   1  12   2   4 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.72      0.83       247\n",
      "           1       0.99      0.60      0.75       235\n",
      "           2       0.90      0.81      0.85       255\n",
      "           3       0.85      0.92      0.88       257\n",
      "           4       0.81      0.93      0.87       240\n",
      "           5       0.94      0.76      0.84       207\n",
      "           6       0.63      0.97      0.77       305\n",
      "           7       0.95      0.74      0.83       235\n",
      "           8       0.82      0.91      0.86       267\n",
      "           9       0.79      0.88      0.83       201\n",
      "\n",
      "    accuracy                           0.83      2449\n",
      "   macro avg       0.87      0.82      0.83      2449\n",
      "weighted avg       0.86      0.83      0.83      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix\\n',cf_matrix)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimisers=[]\n",
    "list_learning_rate=[]\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.01)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.02)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.02)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.03)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.03)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.04)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.04)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.05)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.05)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.06)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.06)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.07)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.07)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.08)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.08)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.09)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.09)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.10)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.10)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.11)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.11)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.12)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.12)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.13)\n",
    "adam_optimisers.append(opt)\n",
    "list_learning_rate.append(0.13)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_28 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.1490 - accuracy: 0.1676 - val_loss: 2.4959 - val_accuracy: 0.1241\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6342 - accuracy: 0.3679 - val_loss: 13.3524 - val_accuracy: 0.1074\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3377 - accuracy: 0.5071 - val_loss: 6.4518 - val_accuracy: 0.1690\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2306 - accuracy: 0.5485 - val_loss: 4.0573 - val_accuracy: 0.2650\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1836 - accuracy: 0.5579 - val_loss: 1.9485 - val_accuracy: 0.2793\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1071 - accuracy: 0.5930 - val_loss: 1.6271 - val_accuracy: 0.4720\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0456 - accuracy: 0.6105 - val_loss: 1.6046 - val_accuracy: 0.4528\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9967 - accuracy: 0.6353 - val_loss: 1.5879 - val_accuracy: 0.4214\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0597 - accuracy: 0.6080 - val_loss: 1.4820 - val_accuracy: 0.5517\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9779 - accuracy: 0.6447 - val_loss: 1.9566 - val_accuracy: 0.3377\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9202 - accuracy: 0.6648 - val_loss: 1.6327 - val_accuracy: 0.4875\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9053 - accuracy: 0.6722 - val_loss: 1.5367 - val_accuracy: 0.5092\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8992 - accuracy: 0.6733 - val_loss: 1.3723 - val_accuracy: 0.5451\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8682 - accuracy: 0.6815 - val_loss: 1.4822 - val_accuracy: 0.5174\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8451 - accuracy: 0.6896 - val_loss: 1.4031 - val_accuracy: 0.5394\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8274 - accuracy: 0.7003 - val_loss: 1.4156 - val_accuracy: 0.5288\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8392 - accuracy: 0.6938 - val_loss: 1.3005 - val_accuracy: 0.5602\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8166 - accuracy: 0.7039 - val_loss: 1.2898 - val_accuracy: 0.6358\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7759 - accuracy: 0.7168 - val_loss: 1.2245 - val_accuracy: 0.6227\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2121 - accuracy: 0.5717 - val_loss: 1.6921 - val_accuracy: 0.4198\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1557 - accuracy: 0.5769 - val_loss: 2.0016 - val_accuracy: 0.3728\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.9200 - accuracy: 0.3153 - val_loss: 2.2437 - val_accuracy: 0.1066\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.5913 - accuracy: 0.4223 - val_loss: 2.4285 - val_accuracy: 0.1290\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3313 - accuracy: 0.5128 - val_loss: 2.2664 - val_accuracy: 0.1809\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2226 - accuracy: 0.5541 - val_loss: 2.3044 - val_accuracy: 0.2332\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1707 - accuracy: 0.5750 - val_loss: 1.9687 - val_accuracy: 0.2928\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1409 - accuracy: 0.5772 - val_loss: 1.7925 - val_accuracy: 0.3757\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0835 - accuracy: 0.6021 - val_loss: 1.6628 - val_accuracy: 0.3606\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0343 - accuracy: 0.6236 - val_loss: 1.5579 - val_accuracy: 0.4145\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9964 - accuracy: 0.6313 - val_loss: 1.5542 - val_accuracy: 0.4108\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9982 - accuracy: 0.6360 - val_loss: 1.4316 - val_accuracy: 0.4961\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9510 - accuracy: 0.6495 - val_loss: 1.4516 - val_accuracy: 0.4549\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9542 - accuracy: 0.6470 - val_loss: 1.3667 - val_accuracy: 0.5039\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9330 - accuracy: 0.6601 - val_loss: 1.6570 - val_accuracy: 0.3928\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9192 - accuracy: 0.6550 - val_loss: 1.2369 - val_accuracy: 0.6129\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8825 - accuracy: 0.6752 - val_loss: 1.5976 - val_accuracy: 0.4026\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8760 - accuracy: 0.6820 - val_loss: 1.2472 - val_accuracy: 0.5725\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8427 - accuracy: 0.6886 - val_loss: 1.2743 - val_accuracy: 0.5492\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8554 - accuracy: 0.6852 - val_loss: 2.0713 - val_accuracy: 0.4210\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8705 - accuracy: 0.6894 - val_loss: 1.6708 - val_accuracy: 0.3209\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8234 - accuracy: 0.3578 - val_loss: 2.0705 - val_accuracy: 0.2225\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4501 - accuracy: 0.4842 - val_loss: 1.9596 - val_accuracy: 0.3234\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.7898 - accuracy: 0.3395 - val_loss: 2.1503 - val_accuracy: 0.2617\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.5446 - accuracy: 0.4185 - val_loss: 2.4153 - val_accuracy: 0.2115\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.5315 - accuracy: 0.4226 - val_loss: 2.2362 - val_accuracy: 0.2054\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.4292 - accuracy: 0.4554 - val_loss: 1.9294 - val_accuracy: 0.3005\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3809 - accuracy: 0.4754 - val_loss: 1.7503 - val_accuracy: 0.3589\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3365 - accuracy: 0.4842 - val_loss: 1.6729 - val_accuracy: 0.3793\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2634 - accuracy: 0.5142 - val_loss: 1.6208 - val_accuracy: 0.3961\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1821 - accuracy: 0.5590 - val_loss: 1.6431 - val_accuracy: 0.3185\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1401 - accuracy: 0.5778 - val_loss: 1.7929 - val_accuracy: 0.3009\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0916 - accuracy: 0.5916 - val_loss: 1.5577 - val_accuracy: 0.4381\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0517 - accuracy: 0.6112 - val_loss: 1.5370 - val_accuracy: 0.4287\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0125 - accuracy: 0.6206 - val_loss: 1.3639 - val_accuracy: 0.5031\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0385 - accuracy: 0.6209 - val_loss: 1.5393 - val_accuracy: 0.4390\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9950 - accuracy: 0.6285 - val_loss: 1.4104 - val_accuracy: 0.5439\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9833 - accuracy: 0.6297 - val_loss: 1.2103 - val_accuracy: 0.5998\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9766 - accuracy: 0.6408 - val_loss: 1.1892 - val_accuracy: 0.5880\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9509 - accuracy: 0.6478 - val_loss: 1.2546 - val_accuracy: 0.5578\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9593 - accuracy: 0.6412 - val_loss: 1.2807 - val_accuracy: 0.5194\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9106 - accuracy: 0.6599 - val_loss: 1.2160 - val_accuracy: 0.5480\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9079 - accuracy: 0.6631 - val_loss: 0.9936 - val_accuracy: 0.6317\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9140 - accuracy: 0.6597 - val_loss: 0.9073 - val_accuracy: 0.6970\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8997 - accuracy: 0.6623 - val_loss: 0.9473 - val_accuracy: 0.6648\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9018 - accuracy: 0.6667 - val_loss: 1.5845 - val_accuracy: 0.4704\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8790 - accuracy: 0.6733 - val_loss: 1.9425 - val_accuracy: 0.4006\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8811 - accuracy: 0.6693 - val_loss: 1.3290 - val_accuracy: 0.5333\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8295 - accuracy: 0.6977 - val_loss: 1.1707 - val_accuracy: 0.5749\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8291 - accuracy: 0.6948 - val_loss: 1.1672 - val_accuracy: 0.5986\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8129 - accuracy: 0.6978 - val_loss: 0.9910 - val_accuracy: 0.6672\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8350 - accuracy: 0.6940 - val_loss: 1.1848 - val_accuracy: 0.5692\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8163 - accuracy: 0.6960 - val_loss: 1.0566 - val_accuracy: 0.6562\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8053 - accuracy: 0.7006 - val_loss: 0.8856 - val_accuracy: 0.7125\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7720 - accuracy: 0.7158 - val_loss: 0.9624 - val_accuracy: 0.6676\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7925 - accuracy: 0.7031 - val_loss: 1.0252 - val_accuracy: 0.6501\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8139 - accuracy: 0.7066 - val_loss: 0.9368 - val_accuracy: 0.6615\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7413 - accuracy: 0.7251 - val_loss: 0.8939 - val_accuracy: 0.6946\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7681 - accuracy: 0.7138 - val_loss: 0.8757 - val_accuracy: 0.6848\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7424 - accuracy: 0.7218 - val_loss: 0.8410 - val_accuracy: 0.7101\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7571 - accuracy: 0.7151 - val_loss: 0.8524 - val_accuracy: 0.6909\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7172 - accuracy: 0.7300 - val_loss: 0.9021 - val_accuracy: 0.6746\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7400 - accuracy: 0.7219 - val_loss: 0.8840 - val_accuracy: 0.6901\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7157 - accuracy: 0.7310 - val_loss: 0.8454 - val_accuracy: 0.6974\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6958 - accuracy: 0.7459 - val_loss: 0.8104 - val_accuracy: 0.7215\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7141 - accuracy: 0.7331 - val_loss: 0.8061 - val_accuracy: 0.7370\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6923 - accuracy: 0.7398 - val_loss: 0.7344 - val_accuracy: 0.7505\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6938 - accuracy: 0.7453 - val_loss: 0.8734 - val_accuracy: 0.6721\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6800 - accuracy: 0.7456 - val_loss: 0.8331 - val_accuracy: 0.7080\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6825 - accuracy: 0.7483 - val_loss: 0.8045 - val_accuracy: 0.6925\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7082 - accuracy: 0.7353 - val_loss: 0.8649 - val_accuracy: 0.6884\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6940 - accuracy: 0.7432 - val_loss: 0.7744 - val_accuracy: 0.7632\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7088 - accuracy: 0.7423 - val_loss: 1.1278 - val_accuracy: 0.6035\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6846 - accuracy: 0.7443 - val_loss: 0.8057 - val_accuracy: 0.7244\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6842 - accuracy: 0.7469 - val_loss: 0.6848 - val_accuracy: 0.7828\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1724 - accuracy: 0.5812 - val_loss: 1.2438 - val_accuracy: 0.5643\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0863 - accuracy: 0.6004 - val_loss: 8.8833 - val_accuracy: 0.1143\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 2.0422 - accuracy: 0.2866 - val_loss: 2.1809 - val_accuracy: 0.1952\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6010 - accuracy: 0.4350 - val_loss: 2.5219 - val_accuracy: 0.1609\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2816 - accuracy: 0.5514 - val_loss: 2.1134 - val_accuracy: 0.3144\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1263 - accuracy: 0.5999 - val_loss: 1.7742 - val_accuracy: 0.4414\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7742 - accuracy: 0.4414\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[6 6 6 ... 6 9 6]\n",
      "Confusion matrix\n",
      " [[ 97   6   1   6   0  13 129   0   0   0]\n",
      " [  0  89   0   2   0  13 139   0   0   2]\n",
      " [  0   0 108   2   0   0 148   1   4   5]\n",
      " [  0   0  25  71   0   1 125  21   3   1]\n",
      " [  0   0   0   0 107   2 155   2   1   0]\n",
      " [  0   0   0   0   3  77 132   7   0   0]\n",
      " [  0   0   0   0   0   2 250  12   0   0]\n",
      " [  0   0   0   0   2   7 119  81  17   0]\n",
      " [  0   0   0   0   3   0 126   1 112   0]\n",
      " [  0   0   0   0   2   0 109   1  18  89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.38      0.56       252\n",
      "           1       0.94      0.36      0.52       245\n",
      "           2       0.81      0.40      0.54       268\n",
      "           3       0.88      0.29      0.43       247\n",
      "           4       0.91      0.40      0.56       267\n",
      "           5       0.67      0.35      0.46       219\n",
      "           6       0.17      0.95      0.29       264\n",
      "           7       0.64      0.36      0.46       226\n",
      "           8       0.72      0.46      0.56       242\n",
      "           9       0.92      0.41      0.56       219\n",
      "\n",
      "    accuracy                           0.44      2449\n",
      "   macro avg       0.77      0.44      0.50      2449\n",
      "weighted avg       0.76      0.44      0.49      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_30 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.0587 - accuracy: 0.2024 - val_loss: 2.5028 - val_accuracy: 0.1225\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.6544 - accuracy: 0.3563 - val_loss: 3.9508 - val_accuracy: 0.1119\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4541 - accuracy: 0.4583 - val_loss: 2.5168 - val_accuracy: 0.1192\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2701 - accuracy: 0.5325 - val_loss: 2.2215 - val_accuracy: 0.2189\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1801 - accuracy: 0.5645 - val_loss: 1.9048 - val_accuracy: 0.3589\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1128 - accuracy: 0.5914 - val_loss: 1.6615 - val_accuracy: 0.3977\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1217 - accuracy: 0.5833 - val_loss: 1.6042 - val_accuracy: 0.4610\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0635 - accuracy: 0.6107 - val_loss: 1.5746 - val_accuracy: 0.4569\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9885 - accuracy: 0.6342 - val_loss: 1.3573 - val_accuracy: 0.4982\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9779 - accuracy: 0.6341 - val_loss: 1.4697 - val_accuracy: 0.4777\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9064 - accuracy: 0.6634 - val_loss: 1.4541 - val_accuracy: 0.4978\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8983 - accuracy: 0.6709 - val_loss: 1.4500 - val_accuracy: 0.4924\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8725 - accuracy: 0.6763 - val_loss: 1.1373 - val_accuracy: 0.6407\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8578 - accuracy: 0.6843 - val_loss: 1.2987 - val_accuracy: 0.5427\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8412 - accuracy: 0.6925 - val_loss: 1.1119 - val_accuracy: 0.6260\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7747 - accuracy: 0.7137 - val_loss: 1.1554 - val_accuracy: 0.5953\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9814 - accuracy: 0.6583 - val_loss: 2.5234 - val_accuracy: 0.2148\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5162 - accuracy: 0.4618 - val_loss: 7.4023 - val_accuracy: 0.2899\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1855 - accuracy: 0.5725 - val_loss: 1.6707 - val_accuracy: 0.4324\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0695 - accuracy: 0.6138 - val_loss: 1.5419 - val_accuracy: 0.4332\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0352 - accuracy: 0.6225 - val_loss: 1.4606 - val_accuracy: 0.4733\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0013 - accuracy: 0.6408 - val_loss: 1.4226 - val_accuracy: 0.5521\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9578 - accuracy: 0.6461 - val_loss: 1.3466 - val_accuracy: 0.5610\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9504 - accuracy: 0.6535 - val_loss: 1.1842 - val_accuracy: 0.5847\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9377 - accuracy: 0.6599 - val_loss: 1.3551 - val_accuracy: 0.5512\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8888 - accuracy: 0.6798 - val_loss: 1.6174 - val_accuracy: 0.4202\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8888 - accuracy: 0.6761 - val_loss: 1.4604 - val_accuracy: 0.4749\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8674 - accuracy: 0.6780 - val_loss: 1.3589 - val_accuracy: 0.5333\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8242 - accuracy: 0.6939 - val_loss: 1.3206 - val_accuracy: 0.5125\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8225 - accuracy: 0.7005 - val_loss: 1.2854 - val_accuracy: 0.5672\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7988 - accuracy: 0.7089 - val_loss: 1.2788 - val_accuracy: 0.5766\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7935 - accuracy: 0.7074 - val_loss: 1.2279 - val_accuracy: 0.5815\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7668 - accuracy: 0.7159 - val_loss: 1.5807 - val_accuracy: 0.5745\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7656 - accuracy: 0.7170 - val_loss: 1.2247 - val_accuracy: 0.5684\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7255 - accuracy: 0.7316 - val_loss: 1.4878 - val_accuracy: 0.5145\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7391 - accuracy: 0.7281 - val_loss: 1.0625 - val_accuracy: 0.6423\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7078 - accuracy: 0.7416 - val_loss: 0.9324 - val_accuracy: 0.6827\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7066 - accuracy: 0.7373 - val_loss: 1.0831 - val_accuracy: 0.6390\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6912 - accuracy: 0.7493 - val_loss: 1.4016 - val_accuracy: 0.5811\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6724 - accuracy: 0.7549 - val_loss: 1.0468 - val_accuracy: 0.6595\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7062 - accuracy: 0.7416 - val_loss: 1.4975 - val_accuracy: 0.4888\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6716 - accuracy: 0.7529 - val_loss: 1.5342 - val_accuracy: 0.5557\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6537 - accuracy: 0.7600 - val_loss: 1.4303 - val_accuracy: 0.5149\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6588 - accuracy: 0.7599 - val_loss: 1.3320 - val_accuracy: 0.5606\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6554 - accuracy: 0.7626 - val_loss: 1.3597 - val_accuracy: 0.5949\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6035 - accuracy: 0.7798 - val_loss: 1.4713 - val_accuracy: 0.5190\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6386 - accuracy: 0.7689 - val_loss: 0.9908 - val_accuracy: 0.6668\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9724 - accuracy: 0.6518 - val_loss: 1.0001 - val_accuracy: 0.6403\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6698 - accuracy: 0.7495 - val_loss: 0.9123 - val_accuracy: 0.6766\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6033 - accuracy: 0.7841 - val_loss: 1.0508 - val_accuracy: 0.6390\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6381 - accuracy: 0.7696 - val_loss: 1.1059 - val_accuracy: 0.6635\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5835 - accuracy: 0.7917 - val_loss: 0.9118 - val_accuracy: 0.6864\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5353 - accuracy: 0.8043 - val_loss: 0.9749 - val_accuracy: 0.6525\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5625 - accuracy: 0.7965 - val_loss: 1.1205 - val_accuracy: 0.5741\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5697 - accuracy: 0.7976 - val_loss: 0.9671 - val_accuracy: 0.6942\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5405 - accuracy: 0.8087 - val_loss: 0.7689 - val_accuracy: 0.7481\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5379 - accuracy: 0.8035 - val_loss: 0.9586 - val_accuracy: 0.7272\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5440 - accuracy: 0.8078 - val_loss: 0.9942 - val_accuracy: 0.6590\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5356 - accuracy: 0.8030 - val_loss: 0.8458 - val_accuracy: 0.7187\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5297 - accuracy: 0.8029 - val_loss: 0.7354 - val_accuracy: 0.7460\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5493 - accuracy: 0.8098 - val_loss: 0.7801 - val_accuracy: 0.7227\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5303 - accuracy: 0.8054 - val_loss: 0.7282 - val_accuracy: 0.7203\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4775 - accuracy: 0.8300 - val_loss: 0.7657 - val_accuracy: 0.7321\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6536 - accuracy: 0.7685 - val_loss: 0.7087 - val_accuracy: 0.7628\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4813 - accuracy: 0.8252 - val_loss: 0.6616 - val_accuracy: 0.8044\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4864 - accuracy: 0.8266 - val_loss: 0.7310 - val_accuracy: 0.7505\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4444 - accuracy: 0.8350 - val_loss: 0.7674 - val_accuracy: 0.7125\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4692 - accuracy: 0.8323 - val_loss: 0.9914 - val_accuracy: 0.6737\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4063 - accuracy: 0.8524 - val_loss: 0.8867 - val_accuracy: 0.6929\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4504 - accuracy: 0.8415 - val_loss: 0.6584 - val_accuracy: 0.7848\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4591 - accuracy: 0.8376 - val_loss: 0.7701 - val_accuracy: 0.7134\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4383 - accuracy: 0.8399 - val_loss: 1.4801 - val_accuracy: 0.5733\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4587 - accuracy: 0.8347 - val_loss: 0.8408 - val_accuracy: 0.7027\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4329 - accuracy: 0.8468 - val_loss: 0.6707 - val_accuracy: 0.7738\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3816 - accuracy: 0.8647 - val_loss: 0.6353 - val_accuracy: 0.7632\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4097 - accuracy: 0.8556 - val_loss: 0.8976 - val_accuracy: 0.6962\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3923 - accuracy: 0.8589 - val_loss: 0.7283 - val_accuracy: 0.7460\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4471 - accuracy: 0.8411 - val_loss: 0.8665 - val_accuracy: 0.6893\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4141 - accuracy: 0.8583 - val_loss: 0.9100 - val_accuracy: 0.6864\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4162 - accuracy: 0.8540 - val_loss: 1.0802 - val_accuracy: 0.6117\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3796 - accuracy: 0.8699 - val_loss: 0.9531 - val_accuracy: 0.6423\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3835 - accuracy: 0.8663 - val_loss: 1.0443 - val_accuracy: 0.6301\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4350 - accuracy: 0.8455 - val_loss: 1.2788 - val_accuracy: 0.5917\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3448 - accuracy: 0.8797 - val_loss: 1.0293 - val_accuracy: 0.6435\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3468 - accuracy: 0.8758 - val_loss: 1.4767 - val_accuracy: 0.5255\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3789 - accuracy: 0.8643 - val_loss: 1.0208 - val_accuracy: 0.6460\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3560 - accuracy: 0.8740 - val_loss: 1.2780 - val_accuracy: 0.5921\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3771 - accuracy: 0.8659 - val_loss: 0.8811 - val_accuracy: 0.6729\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3783 - accuracy: 0.8646 - val_loss: 0.6190 - val_accuracy: 0.7856\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3428 - accuracy: 0.8778 - val_loss: 0.6281 - val_accuracy: 0.7746\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3974 - accuracy: 0.8606 - val_loss: 1.0916 - val_accuracy: 0.6145\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3263 - accuracy: 0.8832 - val_loss: 0.9904 - val_accuracy: 0.6366\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3577 - accuracy: 0.8735 - val_loss: 1.1668 - val_accuracy: 0.5802\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3264 - accuracy: 0.8867 - val_loss: 0.8353 - val_accuracy: 0.6889\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3442 - accuracy: 0.8754 - val_loss: 0.8675 - val_accuracy: 0.7113\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3096 - accuracy: 0.8902 - val_loss: 0.6460 - val_accuracy: 0.7807\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3807 - accuracy: 0.8645 - val_loss: 1.0418 - val_accuracy: 0.6080\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3284 - accuracy: 0.8882 - val_loss: 0.7940 - val_accuracy: 0.7403\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3805 - accuracy: 0.8657 - val_loss: 0.7043 - val_accuracy: 0.7419\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.2979 - accuracy: 0.8924 - val_loss: 0.6017 - val_accuracy: 0.7779\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.7779\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[6 6 3 ... 4 2 7]\n",
      "Confusion matrix\n",
      " [[155   0   3   0   0   0  92   0   0   0]\n",
      " [  0 132   7   5   0   2  77   0   0   0]\n",
      " [  4   0 246   0   0   2  21   8   0   0]\n",
      " [ 14   3  37 187   0   0  11   3   3   0]\n",
      " [  0   0   0   0 159   6  62  15   2   0]\n",
      " [  0   0   0   0   0 169  55   0   0   0]\n",
      " [  0   0   0   0   0   0 251   1   0   0]\n",
      " [  2   0   2   0   0   1  37 217   0   2]\n",
      " [  0   0   2   0   5   0  28   6 222   1]\n",
      " [  1   0   0   0   1   0  12   3   8 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.62      0.73       250\n",
      "           1       0.98      0.59      0.74       223\n",
      "           2       0.83      0.88      0.85       281\n",
      "           3       0.97      0.72      0.83       258\n",
      "           4       0.96      0.65      0.78       244\n",
      "           5       0.94      0.75      0.84       224\n",
      "           6       0.39      1.00      0.56       252\n",
      "           7       0.86      0.83      0.84       261\n",
      "           8       0.94      0.84      0.89       264\n",
      "           9       0.98      0.87      0.92       192\n",
      "\n",
      "    accuracy                           0.78      2449\n",
      "   macro avg       0.87      0.78      0.80      2449\n",
      "weighted avg       0.87      0.78      0.80      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_32 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_33 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.0763 - accuracy: 0.1816 - val_loss: 2.3307 - val_accuracy: 0.0768\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7124 - accuracy: 0.3325 - val_loss: 2.3799 - val_accuracy: 0.1372\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.4698 - accuracy: 0.4513 - val_loss: 8.7660 - val_accuracy: 0.1911\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3122 - accuracy: 0.5153 - val_loss: 2.4572 - val_accuracy: 0.2033\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2720 - accuracy: 0.5335 - val_loss: 1.9412 - val_accuracy: 0.3238\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1677 - accuracy: 0.5665 - val_loss: 1.8479 - val_accuracy: 0.4161\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1231 - accuracy: 0.5851 - val_loss: 1.7823 - val_accuracy: 0.3944\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0551 - accuracy: 0.6089 - val_loss: 1.7433 - val_accuracy: 0.4271\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0327 - accuracy: 0.6122 - val_loss: 2.2477 - val_accuracy: 0.3793\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0314 - accuracy: 0.6206 - val_loss: 1.7643 - val_accuracy: 0.4373\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9296 - accuracy: 0.6518 - val_loss: 1.7987 - val_accuracy: 0.5345\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9384 - accuracy: 0.6509 - val_loss: 2.1663 - val_accuracy: 0.3597\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9673 - accuracy: 0.6397 - val_loss: 1.7463 - val_accuracy: 0.3859\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8842 - accuracy: 0.6718 - val_loss: 1.6547 - val_accuracy: 0.4287\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 1.4211 - accuracy: 0.5030 - val_loss: 1.9835 - val_accuracy: 0.2785\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3790 - accuracy: 0.4952 - val_loss: 1.9159 - val_accuracy: 0.2781\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2407 - accuracy: 0.5432 - val_loss: 1.8983 - val_accuracy: 0.3169\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1846 - accuracy: 0.5725 - val_loss: 1.9248 - val_accuracy: 0.3638\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1337 - accuracy: 0.5880 - val_loss: 2.0424 - val_accuracy: 0.3610\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0888 - accuracy: 0.6031 - val_loss: 1.6964 - val_accuracy: 0.3965\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1081 - accuracy: 0.5956 - val_loss: 1.7453 - val_accuracy: 0.3895\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9914 - accuracy: 0.6369 - val_loss: 1.6865 - val_accuracy: 0.4177\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0075 - accuracy: 0.6338 - val_loss: 1.4905 - val_accuracy: 0.5533\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9806 - accuracy: 0.6411 - val_loss: 1.3597 - val_accuracy: 0.5508\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9492 - accuracy: 0.6537 - val_loss: 1.5217 - val_accuracy: 0.4426\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8942 - accuracy: 0.6729 - val_loss: 1.6751 - val_accuracy: 0.3667\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9165 - accuracy: 0.6629 - val_loss: 1.4803 - val_accuracy: 0.5272\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8627 - accuracy: 0.6831 - val_loss: 1.2221 - val_accuracy: 0.5802\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8307 - accuracy: 0.6957 - val_loss: 1.1820 - val_accuracy: 0.6149\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8145 - accuracy: 0.7013 - val_loss: 1.1909 - val_accuracy: 0.5819\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8499 - accuracy: 0.6866 - val_loss: 1.3948 - val_accuracy: 0.5631\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8611 - accuracy: 0.6836 - val_loss: 1.2473 - val_accuracy: 0.5835\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8331 - accuracy: 0.6968 - val_loss: 1.1099 - val_accuracy: 0.6243\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7672 - accuracy: 0.7157 - val_loss: 1.1317 - val_accuracy: 0.6027\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7760 - accuracy: 0.7141 - val_loss: 1.0595 - val_accuracy: 0.6141\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8000 - accuracy: 0.7036 - val_loss: 0.9730 - val_accuracy: 0.6550\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7428 - accuracy: 0.7291 - val_loss: 0.9198 - val_accuracy: 0.6656\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7682 - accuracy: 0.7228 - val_loss: 0.9161 - val_accuracy: 0.6827\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7263 - accuracy: 0.7392 - val_loss: 1.0275 - val_accuracy: 0.6590\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7146 - accuracy: 0.7431 - val_loss: 0.8661 - val_accuracy: 0.6933\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6867 - accuracy: 0.7502 - val_loss: 0.9852 - val_accuracy: 0.6460\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7070 - accuracy: 0.7444 - val_loss: 1.0714 - val_accuracy: 0.6680\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7067 - accuracy: 0.7473 - val_loss: 0.8914 - val_accuracy: 0.7080\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6864 - accuracy: 0.7539 - val_loss: 1.0354 - val_accuracy: 0.6256\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6366 - accuracy: 0.7678 - val_loss: 0.9278 - val_accuracy: 0.7015\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6655 - accuracy: 0.7571 - val_loss: 0.9101 - val_accuracy: 0.6737\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6314 - accuracy: 0.7745 - val_loss: 1.3880 - val_accuracy: 0.6701\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6557 - accuracy: 0.7613 - val_loss: 0.8459 - val_accuracy: 0.7256\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6204 - accuracy: 0.7754 - val_loss: 1.4529 - val_accuracy: 0.7354\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5971 - accuracy: 0.7822 - val_loss: 0.9744 - val_accuracy: 0.6884\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6230 - accuracy: 0.7773 - val_loss: 0.8049 - val_accuracy: 0.7117\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5521 - accuracy: 0.7989 - val_loss: 1.0719 - val_accuracy: 0.6750\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6041 - accuracy: 0.7831 - val_loss: 0.7913 - val_accuracy: 0.7125\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5845 - accuracy: 0.7895 - val_loss: 0.7533 - val_accuracy: 0.7477\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5937 - accuracy: 0.7912 - val_loss: 0.9557 - val_accuracy: 0.6889\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5532 - accuracy: 0.8004 - val_loss: 0.5951 - val_accuracy: 0.8040\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5658 - accuracy: 0.7980 - val_loss: 0.7140 - val_accuracy: 0.7468\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5483 - accuracy: 0.8018 - val_loss: 0.6961 - val_accuracy: 0.7734\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5716 - accuracy: 0.7984 - val_loss: 0.7174 - val_accuracy: 0.7477\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5817 - accuracy: 0.7939 - val_loss: 0.7507 - val_accuracy: 0.7399\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4985 - accuracy: 0.8176 - val_loss: 0.6510 - val_accuracy: 0.7660\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5478 - accuracy: 0.8014 - val_loss: 0.8538 - val_accuracy: 0.7044\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5295 - accuracy: 0.8088 - val_loss: 0.7650 - val_accuracy: 0.7395\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5608 - accuracy: 0.8006 - val_loss: 0.9974 - val_accuracy: 0.6709\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4938 - accuracy: 0.8216 - val_loss: 0.8956 - val_accuracy: 0.6868\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4905 - accuracy: 0.8193 - val_loss: 0.7709 - val_accuracy: 0.7276\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4883 - accuracy: 0.8300 - val_loss: 0.6637 - val_accuracy: 0.7689\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4994 - accuracy: 0.8227 - val_loss: 0.7457 - val_accuracy: 0.7603\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4421 - accuracy: 0.8438 - val_loss: 0.8758 - val_accuracy: 0.7178\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4742 - accuracy: 0.8384 - val_loss: 0.9061 - val_accuracy: 0.7080\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 3s 18ms/step - loss: 0.4361 - accuracy: 0.8427 - val_loss: 0.6259 - val_accuracy: 0.7909\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4635 - accuracy: 0.8373 - val_loss: 1.1251 - val_accuracy: 0.6623\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5289 - accuracy: 0.8196 - val_loss: 0.8507 - val_accuracy: 0.7317\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4479 - accuracy: 0.8388 - val_loss: 0.6809 - val_accuracy: 0.7722\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4531 - accuracy: 0.8381 - val_loss: 0.9170 - val_accuracy: 0.6717\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4912 - accuracy: 0.8269 - val_loss: 0.9641 - val_accuracy: 0.6207\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4508 - accuracy: 0.8375 - val_loss: 0.8411 - val_accuracy: 0.6599\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4427 - accuracy: 0.8381 - val_loss: 1.0122 - val_accuracy: 0.6554\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3983 - accuracy: 0.8569 - val_loss: 0.7002 - val_accuracy: 0.7599\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4274 - accuracy: 0.8515 - val_loss: 0.8204 - val_accuracy: 0.6901\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4135 - accuracy: 0.8583 - val_loss: 0.9633 - val_accuracy: 0.6546\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3745 - accuracy: 0.8653 - val_loss: 0.7774 - val_accuracy: 0.7391\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4209 - accuracy: 0.8515 - val_loss: 0.7106 - val_accuracy: 0.7526\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3899 - accuracy: 0.8639 - val_loss: 0.5956 - val_accuracy: 0.8003\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3830 - accuracy: 0.8622 - val_loss: 0.7468 - val_accuracy: 0.7448\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3655 - accuracy: 0.8719 - val_loss: 0.5887 - val_accuracy: 0.7995\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8621 - val_loss: 0.6949 - val_accuracy: 0.7836\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3881 - accuracy: 0.8637 - val_loss: 0.7280 - val_accuracy: 0.7566\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3598 - accuracy: 0.8752 - val_loss: 0.5277 - val_accuracy: 0.8220\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3480 - accuracy: 0.8768 - val_loss: 0.4631 - val_accuracy: 0.8416\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3921 - accuracy: 0.8619 - val_loss: 0.6533 - val_accuracy: 0.7820\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3505 - accuracy: 0.8814 - val_loss: 0.5476 - val_accuracy: 0.8073\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3661 - accuracy: 0.8760 - val_loss: 0.6624 - val_accuracy: 0.7570\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3395 - accuracy: 0.8837 - val_loss: 0.5448 - val_accuracy: 0.7983\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3922 - accuracy: 0.8659 - val_loss: 0.7576 - val_accuracy: 0.7219\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3517 - accuracy: 0.8755 - val_loss: 0.5685 - val_accuracy: 0.7930\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3327 - accuracy: 0.8846 - val_loss: 0.6115 - val_accuracy: 0.7987\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3414 - accuracy: 0.8827 - val_loss: 0.5982 - val_accuracy: 0.7991\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3520 - accuracy: 0.8782 - val_loss: 0.6402 - val_accuracy: 0.7685\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3497 - accuracy: 0.8811 - val_loss: 0.7926 - val_accuracy: 0.7432\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7926 - accuracy: 0.7432\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[4 4 7 ... 5 3 5]\n",
      "Confusion matrix\n",
      " [[177  20   7   4  15  19  33   0   0   0]\n",
      " [  0 206   0   0   0   6  34   0   0   0]\n",
      " [  2   8 165   0   4  39  29   4   0   0]\n",
      " [  2  31  12 170   4   6  16   1   2   0]\n",
      " [  0   0   0   0 219  14  15   0   0   0]\n",
      " [  0   1   0   0   4 169  11   0   0   0]\n",
      " [  0   2   0   0   0  13 284   0   0   0]\n",
      " [  0   0   1   0  20   8  43 177   0   0]\n",
      " [  0   0   0   0  31   9  39  10 174   0]\n",
      " [  0   0   1   0  43  11   8  45   2  79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.64      0.78       275\n",
      "           1       0.77      0.84      0.80       246\n",
      "           2       0.89      0.66      0.76       251\n",
      "           3       0.98      0.70      0.81       244\n",
      "           4       0.64      0.88      0.74       248\n",
      "           5       0.57      0.91      0.71       185\n",
      "           6       0.55      0.95      0.70       299\n",
      "           7       0.75      0.71      0.73       249\n",
      "           8       0.98      0.66      0.79       263\n",
      "           9       1.00      0.42      0.59       189\n",
      "\n",
      "    accuracy                           0.74      2449\n",
      "   macro avg       0.81      0.74      0.74      2449\n",
      "weighted avg       0.81      0.74      0.74      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_34 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_17 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 9ms/step - loss: 2.1001 - accuracy: 0.1878 - val_loss: 2.6838 - val_accuracy: 0.1041\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.9986 - accuracy: 0.2191 - val_loss: 2.2429 - val_accuracy: 0.1298\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.8788 - accuracy: 0.2568 - val_loss: 2.3764 - val_accuracy: 0.1094\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7448 - accuracy: 0.3157 - val_loss: 2.1512 - val_accuracy: 0.1895\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.6615 - accuracy: 0.3386 - val_loss: 1.9708 - val_accuracy: 0.2213\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.6131 - accuracy: 0.3654 - val_loss: 2.2417 - val_accuracy: 0.1764\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5302 - accuracy: 0.4118 - val_loss: 1.8909 - val_accuracy: 0.3148\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.4298 - accuracy: 0.4531 - val_loss: 1.6196 - val_accuracy: 0.3499\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.4514 - accuracy: 0.4499 - val_loss: 1.5647 - val_accuracy: 0.3887\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3389 - accuracy: 0.4943 - val_loss: 1.5060 - val_accuracy: 0.4296\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2770 - accuracy: 0.5200 - val_loss: 1.8191 - val_accuracy: 0.3356\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2251 - accuracy: 0.5413 - val_loss: 1.4332 - val_accuracy: 0.4251\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1949 - accuracy: 0.5619 - val_loss: 1.3593 - val_accuracy: 0.5080\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1784 - accuracy: 0.5678 - val_loss: 1.2779 - val_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1112 - accuracy: 0.5874 - val_loss: 1.6372 - val_accuracy: 0.5116\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0972 - accuracy: 0.5908 - val_loss: 1.6720 - val_accuracy: 0.4663\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0826 - accuracy: 0.6043 - val_loss: 1.0494 - val_accuracy: 0.6333\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0410 - accuracy: 0.6111 - val_loss: 1.0674 - val_accuracy: 0.6333\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0196 - accuracy: 0.6177 - val_loss: 1.1449 - val_accuracy: 0.5745\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0071 - accuracy: 0.6323 - val_loss: 1.5717 - val_accuracy: 0.4545\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0074 - accuracy: 0.6283 - val_loss: 1.3342 - val_accuracy: 0.5525\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9403 - accuracy: 0.6516 - val_loss: 0.8699 - val_accuracy: 0.6835\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9422 - accuracy: 0.6471 - val_loss: 1.3182 - val_accuracy: 0.5263\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9132 - accuracy: 0.6661 - val_loss: 0.9105 - val_accuracy: 0.6962\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9198 - accuracy: 0.6609 - val_loss: 1.0765 - val_accuracy: 0.6386\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8769 - accuracy: 0.6768 - val_loss: 1.0155 - val_accuracy: 0.6427\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8795 - accuracy: 0.6769 - val_loss: 1.1751 - val_accuracy: 0.5745\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9076 - accuracy: 0.6673 - val_loss: 1.2023 - val_accuracy: 0.6207\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8672 - accuracy: 0.6754 - val_loss: 0.9267 - val_accuracy: 0.6701\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8472 - accuracy: 0.6885 - val_loss: 1.1432 - val_accuracy: 0.5978\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8432 - accuracy: 0.6924 - val_loss: 1.2212 - val_accuracy: 0.5815\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0021 - accuracy: 0.6339 - val_loss: 1.4306 - val_accuracy: 0.5182\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8840 - accuracy: 0.6765 - val_loss: 0.8737 - val_accuracy: 0.7346\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8359 - accuracy: 0.6952 - val_loss: 0.9504 - val_accuracy: 0.6546\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8162 - accuracy: 0.7014 - val_loss: 1.0510 - val_accuracy: 0.6301\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7922 - accuracy: 0.7135 - val_loss: 1.0557 - val_accuracy: 0.6390\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8581 - accuracy: 0.7005 - val_loss: 18.4910 - val_accuracy: 0.1013\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 2.0182 - accuracy: 0.2588 - val_loss: 3.2151 - val_accuracy: 0.0988\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.6235 - accuracy: 0.4195 - val_loss: 2.2061 - val_accuracy: 0.2846\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3781 - accuracy: 0.5068 - val_loss: 2.3088 - val_accuracy: 0.2389\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2533 - accuracy: 0.5518 - val_loss: 1.9874 - val_accuracy: 0.2838\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1656 - accuracy: 0.5835 - val_loss: 1.6269 - val_accuracy: 0.4541\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1635 - accuracy: 0.5810 - val_loss: 1.6873 - val_accuracy: 0.4087\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1052 - accuracy: 0.6016 - val_loss: 1.5404 - val_accuracy: 0.4667\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0811 - accuracy: 0.6091 - val_loss: 1.2691 - val_accuracy: 0.5517\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0438 - accuracy: 0.6166 - val_loss: 1.3306 - val_accuracy: 0.5341\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0084 - accuracy: 0.6289 - val_loss: 1.2499 - val_accuracy: 0.5655\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9805 - accuracy: 0.6417 - val_loss: 1.0488 - val_accuracy: 0.6460\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9703 - accuracy: 0.6416 - val_loss: 1.1126 - val_accuracy: 0.5921\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9817 - accuracy: 0.6455 - val_loss: 1.1682 - val_accuracy: 0.5745\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0940 - accuracy: 0.6087 - val_loss: 1.3419 - val_accuracy: 0.4973\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0404 - accuracy: 0.6147 - val_loss: 1.2275 - val_accuracy: 0.5500\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9905 - accuracy: 0.6379 - val_loss: 1.0943 - val_accuracy: 0.5966\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9647 - accuracy: 0.6447 - val_loss: 1.0620 - val_accuracy: 0.6305\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9985 - accuracy: 0.6420 - val_loss: 1.0865 - val_accuracy: 0.6247\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9147 - accuracy: 0.6584 - val_loss: 1.0604 - val_accuracy: 0.6105\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9377 - accuracy: 0.6516 - val_loss: 0.9806 - val_accuracy: 0.6521\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8625 - accuracy: 0.6839 - val_loss: 1.2279 - val_accuracy: 0.6166\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8642 - accuracy: 0.6813 - val_loss: 2.0502 - val_accuracy: 0.6546\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8621 - accuracy: 0.6848 - val_loss: 1.0865 - val_accuracy: 0.6480\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8448 - accuracy: 0.6915 - val_loss: 1.0152 - val_accuracy: 0.6676\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8412 - accuracy: 0.6865 - val_loss: 0.9810 - val_accuracy: 0.6439\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8742 - accuracy: 0.6808 - val_loss: 1.0948 - val_accuracy: 0.6276\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8425 - accuracy: 0.6965 - val_loss: 0.9582 - val_accuracy: 0.6709\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8163 - accuracy: 0.6969 - val_loss: 0.8559 - val_accuracy: 0.7011\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8242 - accuracy: 0.6953 - val_loss: 1.0274 - val_accuracy: 0.6194\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8744 - accuracy: 0.6865 - val_loss: 1.4426 - val_accuracy: 0.5729\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7931 - accuracy: 0.7122 - val_loss: 1.1905 - val_accuracy: 0.6084\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8284 - accuracy: 0.6974 - val_loss: 1.1663 - val_accuracy: 0.5794\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8158 - accuracy: 0.7038 - val_loss: 1.0102 - val_accuracy: 0.6574\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8352 - accuracy: 0.7004 - val_loss: 1.0293 - val_accuracy: 0.6550\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7604 - accuracy: 0.7220 - val_loss: 0.9445 - val_accuracy: 0.6974\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7773 - accuracy: 0.7133 - val_loss: 0.8615 - val_accuracy: 0.7178\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7458 - accuracy: 0.7294 - val_loss: 1.1672 - val_accuracy: 0.6521\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7828 - accuracy: 0.7169 - val_loss: 1.0805 - val_accuracy: 0.6456\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7875 - accuracy: 0.7102 - val_loss: 0.8797 - val_accuracy: 0.6856\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7454 - accuracy: 0.7264 - val_loss: 0.9038 - val_accuracy: 0.6815\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7336 - accuracy: 0.7328 - val_loss: 1.1019 - val_accuracy: 0.6145\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7257 - accuracy: 0.7356 - val_loss: 0.9031 - val_accuracy: 0.7301\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7263 - accuracy: 0.7387 - val_loss: 1.0993 - val_accuracy: 0.6247\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7494 - accuracy: 0.7271 - val_loss: 0.9881 - val_accuracy: 0.6309\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7260 - accuracy: 0.7352 - val_loss: 0.8988 - val_accuracy: 0.6807\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6867 - accuracy: 0.7533 - val_loss: 0.8277 - val_accuracy: 0.7542\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4416 - accuracy: 0.4977 - val_loss: 2.1386 - val_accuracy: 0.1788\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1768 - accuracy: 0.5667 - val_loss: 2.1193 - val_accuracy: 0.2348\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0309 - accuracy: 0.6244 - val_loss: 1.9418 - val_accuracy: 0.2944\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9709 - accuracy: 0.6413 - val_loss: 1.7344 - val_accuracy: 0.3348\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9286 - accuracy: 0.6590 - val_loss: 1.3789 - val_accuracy: 0.4618\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8739 - accuracy: 0.6812 - val_loss: 1.5233 - val_accuracy: 0.4802\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8547 - accuracy: 0.6850 - val_loss: 1.5454 - val_accuracy: 0.4006\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8540 - accuracy: 0.6866 - val_loss: 1.2943 - val_accuracy: 0.5484\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8102 - accuracy: 0.7072 - val_loss: 1.2424 - val_accuracy: 0.5272\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8298 - accuracy: 0.6911 - val_loss: 1.0644 - val_accuracy: 0.6411\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7811 - accuracy: 0.7137 - val_loss: 1.0910 - val_accuracy: 0.6309\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7768 - accuracy: 0.7173 - val_loss: 1.2736 - val_accuracy: 0.5202\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7675 - accuracy: 0.7137 - val_loss: 1.3200 - val_accuracy: 0.5496\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8230 - accuracy: 0.7005 - val_loss: 1.2603 - val_accuracy: 0.5602\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7445 - accuracy: 0.7280 - val_loss: 0.9643 - val_accuracy: 0.6521\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7639 - accuracy: 0.7191 - val_loss: 1.3494 - val_accuracy: 0.4978\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7776 - accuracy: 0.7150 - val_loss: 1.0728 - val_accuracy: 0.5909\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0728 - accuracy: 0.5909\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[5 8 5 ... 6 0 6]\n",
      "Confusion matrix\n",
      " [[211  57   0   0   0   0   0   0   0   0]\n",
      " [  0 238   0   0   0   0   0   0   0   0]\n",
      " [  4 148  84   0   0   0   0   0   0   0]\n",
      " [  2 215   0  42   0   0   0   0   0   0]\n",
      " [ 25   7   0   0 169   5  61   0   3   0]\n",
      " [ 17  11   0   0   9 132  45   0   0   0]\n",
      " [ 23  16   3   0   0   2 232   0   0   0]\n",
      " [ 27  27   3   0  19   9  20  63  66   2]\n",
      " [  9  25   5   0   9   0  33   5 161   1]\n",
      " [ 23   8  11   0   0   1  27   6  13 115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69       268\n",
      "           1       0.32      1.00      0.48       238\n",
      "           2       0.79      0.36      0.49       236\n",
      "           3       1.00      0.16      0.28       259\n",
      "           4       0.82      0.63      0.71       270\n",
      "           5       0.89      0.62      0.73       214\n",
      "           6       0.56      0.84      0.67       276\n",
      "           7       0.85      0.27      0.41       236\n",
      "           8       0.66      0.65      0.66       248\n",
      "           9       0.97      0.56      0.71       204\n",
      "\n",
      "    accuracy                           0.59      2449\n",
      "   macro avg       0.75      0.59      0.58      2449\n",
      "weighted avg       0.74      0.59      0.58      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_36 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_18 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.1418 - accuracy: 0.1669 - val_loss: 2.2845 - val_accuracy: 0.1225\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9029 - accuracy: 0.2272 - val_loss: 2.3765 - val_accuracy: 0.0931\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6839 - accuracy: 0.3393 - val_loss: 2.2937 - val_accuracy: 0.1098\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.5548 - accuracy: 0.4123 - val_loss: 2.3332 - val_accuracy: 0.1690\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4214 - accuracy: 0.4773 - val_loss: 3.0555 - val_accuracy: 0.1327\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3093 - accuracy: 0.5228 - val_loss: 1.8024 - val_accuracy: 0.3548\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2080 - accuracy: 0.5711 - val_loss: 1.6133 - val_accuracy: 0.4189\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1792 - accuracy: 0.5702 - val_loss: 1.5964 - val_accuracy: 0.4267\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1364 - accuracy: 0.5891 - val_loss: 1.3693 - val_accuracy: 0.5365\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0902 - accuracy: 0.6062 - val_loss: 1.5124 - val_accuracy: 0.5459\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0598 - accuracy: 0.6175 - val_loss: 1.4497 - val_accuracy: 0.5145\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0443 - accuracy: 0.6208 - val_loss: 1.4335 - val_accuracy: 0.4655\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9821 - accuracy: 0.6472 - val_loss: 1.2451 - val_accuracy: 0.5900\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9621 - accuracy: 0.6499 - val_loss: 1.2097 - val_accuracy: 0.5876\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9545 - accuracy: 0.6565 - val_loss: 2.1265 - val_accuracy: 0.3748\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9266 - accuracy: 0.6621 - val_loss: 1.6634 - val_accuracy: 0.3683\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9372 - accuracy: 0.6595 - val_loss: 1.2562 - val_accuracy: 0.5574\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8675 - accuracy: 0.6812 - val_loss: 1.2709 - val_accuracy: 0.5876\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8622 - accuracy: 0.6894 - val_loss: 1.2048 - val_accuracy: 0.5966\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8751 - accuracy: 0.6839 - val_loss: 1.3483 - val_accuracy: 0.5590\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8115 - accuracy: 0.7081 - val_loss: 1.1640 - val_accuracy: 0.5945\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0847 - accuracy: 0.6143 - val_loss: 2.5548 - val_accuracy: 0.1768\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5413 - accuracy: 0.4112 - val_loss: 1.7059 - val_accuracy: 0.3842\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3301 - accuracy: 0.5034 - val_loss: 2.4799 - val_accuracy: 0.1952\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1989 - accuracy: 0.5608 - val_loss: 2.2426 - val_accuracy: 0.2768\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2777 - accuracy: 0.5448 - val_loss: 2.2459 - val_accuracy: 0.2544\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3688 - accuracy: 0.4965 - val_loss: 1.7565 - val_accuracy: 0.4324\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1817 - accuracy: 0.5599 - val_loss: 1.6959 - val_accuracy: 0.3838\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0957 - accuracy: 0.5909 - val_loss: 1.6277 - val_accuracy: 0.3940\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0015 - accuracy: 0.6302 - val_loss: 1.3202 - val_accuracy: 0.5459\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9637 - accuracy: 0.6484 - val_loss: 1.4259 - val_accuracy: 0.4941\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9547 - accuracy: 0.6462 - val_loss: 1.3046 - val_accuracy: 0.5541\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9326 - accuracy: 0.6623 - val_loss: 1.2235 - val_accuracy: 0.5627\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8304 - accuracy: 0.6970 - val_loss: 1.2866 - val_accuracy: 0.5410\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9302 - accuracy: 0.6602 - val_loss: 0.9982 - val_accuracy: 0.6660\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8234 - accuracy: 0.7016 - val_loss: 1.2351 - val_accuracy: 0.5251\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8379 - accuracy: 0.7036 - val_loss: 1.2276 - val_accuracy: 0.5811\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8008 - accuracy: 0.7071 - val_loss: 1.1252 - val_accuracy: 0.5970\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7589 - accuracy: 0.7185 - val_loss: 1.1215 - val_accuracy: 0.6778\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7228 - accuracy: 0.7365 - val_loss: 1.0447 - val_accuracy: 0.5994\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7448 - accuracy: 0.7335 - val_loss: 1.0448 - val_accuracy: 0.5982\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7450 - accuracy: 0.7312 - val_loss: 0.9477 - val_accuracy: 0.6680\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6977 - accuracy: 0.7488 - val_loss: 1.0550 - val_accuracy: 0.6329\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7343 - accuracy: 0.7403 - val_loss: 1.1394 - val_accuracy: 0.6092\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7398 - accuracy: 0.7345 - val_loss: 1.1903 - val_accuracy: 0.6211\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7034 - accuracy: 0.7507 - val_loss: 1.1026 - val_accuracy: 0.6435\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6502 - accuracy: 0.7638 - val_loss: 0.8276 - val_accuracy: 0.7044\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6687 - accuracy: 0.7652 - val_loss: 0.8675 - val_accuracy: 0.7436\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6741 - accuracy: 0.7605 - val_loss: 0.9097 - val_accuracy: 0.6852\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6529 - accuracy: 0.7691 - val_loss: 0.9734 - val_accuracy: 0.6827\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6348 - accuracy: 0.7733 - val_loss: 0.9211 - val_accuracy: 0.6970\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6636 - accuracy: 0.7653 - val_loss: 1.1712 - val_accuracy: 0.5937\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6801 - accuracy: 0.7636 - val_loss: 1.1239 - val_accuracy: 0.6194\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6538 - accuracy: 0.7640 - val_loss: 0.8853 - val_accuracy: 0.7134\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6081 - accuracy: 0.7796 - val_loss: 0.8131 - val_accuracy: 0.7223\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6022 - accuracy: 0.7888 - val_loss: 1.1626 - val_accuracy: 0.6394\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6274 - accuracy: 0.7787 - val_loss: 1.9610 - val_accuracy: 0.4006\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5722 - accuracy: 0.8013 - val_loss: 1.0306 - val_accuracy: 0.6260\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5673 - accuracy: 0.7994 - val_loss: 0.8480 - val_accuracy: 0.7211\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5796 - accuracy: 0.7975 - val_loss: 0.7547 - val_accuracy: 0.7595\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5663 - accuracy: 0.8011 - val_loss: 1.1498 - val_accuracy: 0.6558\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5927 - accuracy: 0.7926 - val_loss: 0.6930 - val_accuracy: 0.7546\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5821 - accuracy: 0.7990 - val_loss: 0.7798 - val_accuracy: 0.7170\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5691 - accuracy: 0.8036 - val_loss: 0.8559 - val_accuracy: 0.6970\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5571 - accuracy: 0.8037 - val_loss: 0.7194 - val_accuracy: 0.7481\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5567 - accuracy: 0.8063 - val_loss: 0.7707 - val_accuracy: 0.7379\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5833 - accuracy: 0.7963 - val_loss: 0.7815 - val_accuracy: 0.7289\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5582 - accuracy: 0.8042 - val_loss: 0.6531 - val_accuracy: 0.7840\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5514 - accuracy: 0.8080 - val_loss: 0.6477 - val_accuracy: 0.7856\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5527 - accuracy: 0.8049 - val_loss: 0.8356 - val_accuracy: 0.7203\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5324 - accuracy: 0.8184 - val_loss: 0.7812 - val_accuracy: 0.7668\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5114 - accuracy: 0.8228 - val_loss: 1.0533 - val_accuracy: 0.7289\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5498 - accuracy: 0.8048 - val_loss: 0.7383 - val_accuracy: 0.7448\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5241 - accuracy: 0.8192 - val_loss: 0.6534 - val_accuracy: 0.7995\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5521 - accuracy: 0.8119 - val_loss: 0.7195 - val_accuracy: 0.7407\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4981 - accuracy: 0.8232 - val_loss: 1.5509 - val_accuracy: 0.5525\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7739 - accuracy: 0.7323 - val_loss: 0.6849 - val_accuracy: 0.7697\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5075 - accuracy: 0.8247 - val_loss: 0.9496 - val_accuracy: 0.6537\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5036 - accuracy: 0.8215 - val_loss: 1.0140 - val_accuracy: 0.6827\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5027 - accuracy: 0.8295 - val_loss: 0.9254 - val_accuracy: 0.6652\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1734 - accuracy: 0.6159 - val_loss: 1.1788 - val_accuracy: 0.6145\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8482 - accuracy: 0.7008 - val_loss: 1.0966 - val_accuracy: 0.5978\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6845 - accuracy: 0.7583 - val_loss: 1.0253 - val_accuracy: 0.6627\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6204 - accuracy: 0.7783 - val_loss: 1.3098 - val_accuracy: 0.6129\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6147 - accuracy: 0.7824 - val_loss: 1.0600 - val_accuracy: 0.6325\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6057 - accuracy: 0.7907 - val_loss: 0.9543 - val_accuracy: 0.6558\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5651 - accuracy: 0.8018 - val_loss: 0.9285 - val_accuracy: 0.6713\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5695 - accuracy: 0.8059 - val_loss: 0.9195 - val_accuracy: 0.6733\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5965 - accuracy: 0.7918 - val_loss: 0.9264 - val_accuracy: 0.6823\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5847 - accuracy: 0.7973 - val_loss: 0.8653 - val_accuracy: 0.7089\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5556 - accuracy: 0.8045 - val_loss: 0.7921 - val_accuracy: 0.7015\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5844 - accuracy: 0.7940 - val_loss: 0.7869 - val_accuracy: 0.7374\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6184 - accuracy: 0.7906 - val_loss: 0.7406 - val_accuracy: 0.7546\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5154 - accuracy: 0.8186 - val_loss: 0.7990 - val_accuracy: 0.7305\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5301 - accuracy: 0.8133 - val_loss: 0.7693 - val_accuracy: 0.7468\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5241 - accuracy: 0.8161 - val_loss: 0.5885 - val_accuracy: 0.8007\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5460 - accuracy: 0.8094 - val_loss: 0.6564 - val_accuracy: 0.7346\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5169 - accuracy: 0.8209 - val_loss: 0.6146 - val_accuracy: 0.7873\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5414 - accuracy: 0.8115 - val_loss: 0.7607 - val_accuracy: 0.7509\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4782 - accuracy: 0.8308 - val_loss: 0.8497 - val_accuracy: 0.7121\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8497 - accuracy: 0.7121\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[2 1 0 ... 9 0 2]\n",
      "Confusion matrix\n",
      " [[219  36   0   0   0   0   5   0   0   0]\n",
      " [  1 229   0   0   0   0   0   0   0   0]\n",
      " [ 12  66 216   0   0   0   0   0   0   1]\n",
      " [  0  94  15 136   0   0   0   0   0   0]\n",
      " [ 21   5   0   1 129   1   4   0   3  92]\n",
      " [ 27   7  10   0   0 130   9   0   0  25]\n",
      " [ 21  25  16   0   0   0 192   1   0  22]\n",
      " [ 15   4  14   0   0   0   3 158   3  52]\n",
      " [ 10   7   9   0   0   0   1   2 149  50]\n",
      " [ 11   0   0   0   0   0   2   1   1 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.84      0.73       260\n",
      "           1       0.48      1.00      0.65       230\n",
      "           2       0.77      0.73      0.75       295\n",
      "           3       0.99      0.56      0.71       245\n",
      "           4       1.00      0.50      0.67       256\n",
      "           5       0.99      0.62      0.77       208\n",
      "           6       0.89      0.69      0.78       277\n",
      "           7       0.98      0.63      0.77       249\n",
      "           8       0.96      0.65      0.78       228\n",
      "           9       0.43      0.93      0.59       201\n",
      "\n",
      "    accuracy                           0.71      2449\n",
      "   macro avg       0.81      0.72      0.72      2449\n",
      "weighted avg       0.82      0.71      0.72      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_38 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_39 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_19 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_19 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2609 - accuracy: 0.1439 - val_loss: 2.3368 - val_accuracy: 0.0902\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.9662 - accuracy: 0.1916 - val_loss: 2.3475 - val_accuracy: 0.1135\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8821 - accuracy: 0.2447 - val_loss: 2.7073 - val_accuracy: 0.1111\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.7360 - accuracy: 0.3124 - val_loss: 2.2549 - val_accuracy: 0.1200\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.6363 - accuracy: 0.3626 - val_loss: 2.7128 - val_accuracy: 0.2813\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.5713 - accuracy: 0.4164 - val_loss: 2.1101 - val_accuracy: 0.2168\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4269 - accuracy: 0.4709 - val_loss: 1.9071 - val_accuracy: 0.3528\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3297 - accuracy: 0.5133 - val_loss: 1.5277 - val_accuracy: 0.4892\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3008 - accuracy: 0.5279 - val_loss: 1.5877 - val_accuracy: 0.4777\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2065 - accuracy: 0.5703 - val_loss: 1.7306 - val_accuracy: 0.4512\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1488 - accuracy: 0.5829 - val_loss: 1.9864 - val_accuracy: 0.4157\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.3929 - accuracy: 0.4951 - val_loss: 1.5185 - val_accuracy: 0.5035\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1804 - accuracy: 0.5758 - val_loss: 1.4652 - val_accuracy: 0.5035\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1605 - accuracy: 0.5825 - val_loss: 2.0594 - val_accuracy: 0.4561\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0947 - accuracy: 0.6062 - val_loss: 1.3332 - val_accuracy: 0.5884\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0507 - accuracy: 0.6154 - val_loss: 1.3575 - val_accuracy: 0.4810\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0295 - accuracy: 0.6237 - val_loss: 1.6893 - val_accuracy: 0.5308\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9977 - accuracy: 0.6377 - val_loss: 1.0923 - val_accuracy: 0.6121\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9794 - accuracy: 0.6380 - val_loss: 2.3274 - val_accuracy: 0.4851\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9859 - accuracy: 0.6447 - val_loss: 1.3822 - val_accuracy: 0.4639\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9447 - accuracy: 0.6558 - val_loss: 2.1515 - val_accuracy: 0.3385\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9508 - accuracy: 0.6560 - val_loss: 1.6618 - val_accuracy: 0.4059\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9120 - accuracy: 0.6660 - val_loss: 1.7369 - val_accuracy: 0.4304\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9012 - accuracy: 0.6689 - val_loss: 2.3908 - val_accuracy: 0.3626\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9074 - accuracy: 0.6733 - val_loss: 1.9474 - val_accuracy: 0.4492\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8181 - accuracy: 0.7042 - val_loss: 1.6220 - val_accuracy: 0.4806\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8399 - accuracy: 0.6973 - val_loss: 1.6908 - val_accuracy: 0.4692\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8313 - accuracy: 0.7038 - val_loss: 1.9152 - val_accuracy: 0.3908\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7799 - accuracy: 0.7132 - val_loss: 1.4665 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8132 - accuracy: 0.7077 - val_loss: 1.5037 - val_accuracy: 0.5688\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8184 - accuracy: 0.7151 - val_loss: 1.3565 - val_accuracy: 0.6060\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7212 - accuracy: 0.7384 - val_loss: 1.0278 - val_accuracy: 0.6341\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7481 - accuracy: 0.7309 - val_loss: 1.2442 - val_accuracy: 0.5835\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.7043 - accuracy: 0.7456 - val_loss: 1.0392 - val_accuracy: 0.6243\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7113 - accuracy: 0.7459 - val_loss: 1.0732 - val_accuracy: 0.6443\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7182 - accuracy: 0.7402 - val_loss: 1.2873 - val_accuracy: 0.6746\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6931 - accuracy: 0.7504 - val_loss: 1.0541 - val_accuracy: 0.7583\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6724 - accuracy: 0.7534 - val_loss: 1.3257 - val_accuracy: 0.6035\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7693 - accuracy: 0.7319 - val_loss: 1.5469 - val_accuracy: 0.6007\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6587 - accuracy: 0.7666 - val_loss: 1.2953 - val_accuracy: 0.5925\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6816 - accuracy: 0.7567 - val_loss: 1.5948 - val_accuracy: 0.5904\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6615 - accuracy: 0.7652 - val_loss: 1.0144 - val_accuracy: 0.6484\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6676 - accuracy: 0.7607 - val_loss: 0.8361 - val_accuracy: 0.7289\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6030 - accuracy: 0.7781 - val_loss: 0.6779 - val_accuracy: 0.7521\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6569 - accuracy: 0.7680 - val_loss: 0.8842 - val_accuracy: 0.6974\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6071 - accuracy: 0.7842 - val_loss: 1.0413 - val_accuracy: 0.6599\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5880 - accuracy: 0.7884 - val_loss: 0.7277 - val_accuracy: 0.7444\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.6020 - accuracy: 0.7859 - val_loss: 2.0358 - val_accuracy: 0.6680\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5682 - accuracy: 0.7945 - val_loss: 1.6104 - val_accuracy: 0.6966\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5942 - accuracy: 0.7900 - val_loss: 0.7654 - val_accuracy: 0.7526\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5415 - accuracy: 0.8081 - val_loss: 0.7121 - val_accuracy: 0.7913\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5739 - accuracy: 0.7964 - val_loss: 1690.3159 - val_accuracy: 0.7893\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5442 - accuracy: 0.8055 - val_loss: 0.8021 - val_accuracy: 0.7379\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5336 - accuracy: 0.8106 - val_loss: 0.8349 - val_accuracy: 0.7105\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5444 - accuracy: 0.8030 - val_loss: 0.5302 - val_accuracy: 0.8118\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5758 - accuracy: 0.7947 - val_loss: 0.6970 - val_accuracy: 0.7538\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5287 - accuracy: 0.8152 - val_loss: 0.5678 - val_accuracy: 0.8142\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5258 - accuracy: 0.8159 - val_loss: 0.8536 - val_accuracy: 0.6942\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5367 - accuracy: 0.8111 - val_loss: 0.4281 - val_accuracy: 0.8599\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5147 - accuracy: 0.8201 - val_loss: 1.3012 - val_accuracy: 0.6076\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5668 - accuracy: 0.8047 - val_loss: 0.7979 - val_accuracy: 0.7346\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4715 - accuracy: 0.8324 - val_loss: 0.6556 - val_accuracy: 0.7909\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4975 - accuracy: 0.8217 - val_loss: 0.7997 - val_accuracy: 0.7807\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4935 - accuracy: 0.8234 - val_loss: 1.5578 - val_accuracy: 0.7950\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.8248 - val_loss: 3.6408 - val_accuracy: 0.7558\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4748 - accuracy: 0.8365 - val_loss: 0.5124 - val_accuracy: 0.8130\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4871 - accuracy: 0.8330 - val_loss: 0.5795 - val_accuracy: 0.7877\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4908 - accuracy: 0.8340 - val_loss: 0.5178 - val_accuracy: 0.8424\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4676 - accuracy: 0.8402 - val_loss: 0.7122 - val_accuracy: 0.7587\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4779 - accuracy: 0.8343 - val_loss: 0.8299 - val_accuracy: 0.7146\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.5371 - accuracy: 0.8165 - val_loss: 0.7147 - val_accuracy: 0.7685\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4937 - accuracy: 0.8343 - val_loss: 0.6494 - val_accuracy: 0.7685\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4350 - accuracy: 0.8474 - val_loss: 0.4405 - val_accuracy: 0.8628\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4081 - accuracy: 0.8574 - val_loss: 0.9569 - val_accuracy: 0.6786\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4482 - accuracy: 0.8498 - val_loss: 0.7749 - val_accuracy: 0.7415\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5030 - accuracy: 0.8334 - val_loss: 0.7472 - val_accuracy: 0.7493\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4664 - accuracy: 0.8408 - val_loss: 1.2506 - val_accuracy: 0.7583\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4303 - accuracy: 0.8527 - val_loss: 1.2108 - val_accuracy: 0.6933\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4788 - accuracy: 0.8374 - val_loss: 1.2338 - val_accuracy: 0.8085\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4282 - accuracy: 0.8482 - val_loss: 0.8245 - val_accuracy: 0.7297\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4078 - accuracy: 0.8577 - val_loss: 1.2859 - val_accuracy: 0.7644\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4354 - accuracy: 0.8536 - val_loss: 5.5106 - val_accuracy: 0.8391\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.4354 - accuracy: 0.8517 - val_loss: 6.9175 - val_accuracy: 0.8240\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4297 - accuracy: 0.8526 - val_loss: 0.5025 - val_accuracy: 0.8056\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8704 - val_loss: 0.6201 - val_accuracy: 0.7652\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3808 - accuracy: 0.8697 - val_loss: 0.7015 - val_accuracy: 0.7607\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3979 - accuracy: 0.8694 - val_loss: 0.8186 - val_accuracy: 0.7248\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3933 - accuracy: 0.8650 - val_loss: 0.7559 - val_accuracy: 0.7281\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4298 - accuracy: 0.8604 - val_loss: 1.1297 - val_accuracy: 0.6554\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3929 - accuracy: 0.8648 - val_loss: 0.7176 - val_accuracy: 0.7489\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4026 - accuracy: 0.8646 - val_loss: 0.7902 - val_accuracy: 0.7591\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8666 - val_loss: 0.6075 - val_accuracy: 0.7791\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3591 - accuracy: 0.8751 - val_loss: 0.6596 - val_accuracy: 0.7791\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3971 - accuracy: 0.8668 - val_loss: 0.5875 - val_accuracy: 0.8150\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3552 - accuracy: 0.8770 - val_loss: 0.3354 - val_accuracy: 0.8914\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4127 - accuracy: 0.8607 - val_loss: 0.5257 - val_accuracy: 0.8142\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4424 - accuracy: 0.8565 - val_loss: 0.6469 - val_accuracy: 0.7701\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.4539 - accuracy: 0.8544 - val_loss: 0.6310 - val_accuracy: 0.7660\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.3820 - accuracy: 0.8716 - val_loss: 0.4861 - val_accuracy: 0.8212\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.3446 - accuracy: 0.8805 - val_loss: 0.5669 - val_accuracy: 0.7885\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5669 - accuracy: 0.7885\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[5 4 0 ... 4 2 1]\n",
      "Confusion matrix\n",
      " [[250   0   3   0   0   0  21   0   0   0]\n",
      " [  0 140   0   0   0   0  45  34   0   0]\n",
      " [ 14   0 189   0   0   2  38  21   0   1]\n",
      " [  8   0  18 151   0   6   4  66   2   0]\n",
      " [  0   0   0   0 160   6  69   9   0   1]\n",
      " [  1   0   0   0   0 167  42   7   0   0]\n",
      " [  0   0   0   0   0   0 286   0   0   0]\n",
      " [  0   0   0   0   0   6  10 201   1   0]\n",
      " [  0   0   1   0   0   0  33  20 207   1]\n",
      " [  5   0   4   0   0   0   3   8   8 180]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91       274\n",
      "           1       1.00      0.64      0.78       219\n",
      "           2       0.88      0.71      0.79       265\n",
      "           3       1.00      0.59      0.74       255\n",
      "           4       1.00      0.65      0.79       245\n",
      "           5       0.89      0.77      0.83       217\n",
      "           6       0.52      1.00      0.68       286\n",
      "           7       0.55      0.92      0.69       218\n",
      "           8       0.95      0.79      0.86       262\n",
      "           9       0.98      0.87      0.92       208\n",
      "\n",
      "    accuracy                           0.79      2449\n",
      "   macro avg       0.87      0.79      0.80      2449\n",
      "weighted avg       0.86      0.79      0.80      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_40 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_41 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_20 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_20 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2324 - accuracy: 0.1452 - val_loss: 2.3684 - val_accuracy: 0.1164\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9962 - accuracy: 0.1980 - val_loss: 2.3545 - val_accuracy: 0.1021\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9017 - accuracy: 0.2355 - val_loss: 2.6467 - val_accuracy: 0.1025\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7759 - accuracy: 0.2972 - val_loss: 2.5085 - val_accuracy: 0.1180\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.6442 - accuracy: 0.3662 - val_loss: 2.4238 - val_accuracy: 0.1588\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5454 - accuracy: 0.4101 - val_loss: 2.1717 - val_accuracy: 0.2099\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4030 - accuracy: 0.4771 - val_loss: 1.9335 - val_accuracy: 0.2952\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3276 - accuracy: 0.5073 - val_loss: 1.7939 - val_accuracy: 0.3536\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2413 - accuracy: 0.5451 - val_loss: 1.7029 - val_accuracy: 0.3222\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.2370 - accuracy: 0.5423 - val_loss: 1.6875 - val_accuracy: 0.4149\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1631 - accuracy: 0.5733 - val_loss: 1.5360 - val_accuracy: 0.4324\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.1276 - accuracy: 0.5930 - val_loss: 1.3952 - val_accuracy: 0.4859\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1112 - accuracy: 0.5997 - val_loss: 1.1874 - val_accuracy: 0.5692\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 1.0672 - accuracy: 0.6169 - val_loss: 1.3068 - val_accuracy: 0.5292\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0690 - accuracy: 0.6150 - val_loss: 1.5216 - val_accuracy: 0.4328\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0487 - accuracy: 0.6245 - val_loss: 1.6522 - val_accuracy: 0.3789\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0018 - accuracy: 0.6385 - val_loss: 1.4051 - val_accuracy: 0.4745\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9542 - accuracy: 0.6591 - val_loss: 1.4077 - val_accuracy: 0.4990\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9868 - accuracy: 0.6487 - val_loss: 1.1299 - val_accuracy: 0.6296\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9344 - accuracy: 0.6665 - val_loss: 1.3419 - val_accuracy: 0.5178\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9128 - accuracy: 0.6669 - val_loss: 1.0974 - val_accuracy: 0.5884\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.9238 - accuracy: 0.6737 - val_loss: 1.1347 - val_accuracy: 0.6537\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8671 - accuracy: 0.6922 - val_loss: 1.3123 - val_accuracy: 0.5651\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8820 - accuracy: 0.6829 - val_loss: 1.0474 - val_accuracy: 0.5860\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.8886 - accuracy: 0.6808 - val_loss: 1.6549 - val_accuracy: 0.4002\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8315 - accuracy: 0.7038 - val_loss: 1.1934 - val_accuracy: 0.5733\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9008 - accuracy: 0.6896 - val_loss: 1.5676 - val_accuracy: 0.5349\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8580 - accuracy: 0.6961 - val_loss: 1.5243 - val_accuracy: 0.4867\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8547 - accuracy: 0.7001 - val_loss: 1.3750 - val_accuracy: 0.5431\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7977 - accuracy: 0.7147 - val_loss: 1.4814 - val_accuracy: 0.5292\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8064 - accuracy: 0.7116 - val_loss: 1.7293 - val_accuracy: 0.4708\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8076 - accuracy: 0.7095 - val_loss: 1.1053 - val_accuracy: 0.6301\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7803 - accuracy: 0.7251 - val_loss: 1.2626 - val_accuracy: 0.6035\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8060 - accuracy: 0.7133 - val_loss: 1.2217 - val_accuracy: 0.6011\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7959 - accuracy: 0.7138 - val_loss: 1.1666 - val_accuracy: 0.5998\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7650 - accuracy: 0.7275 - val_loss: 1.1374 - val_accuracy: 0.6309\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8130 - accuracy: 0.7180 - val_loss: 1.3180 - val_accuracy: 0.5619\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7680 - accuracy: 0.7270 - val_loss: 1.2041 - val_accuracy: 0.6149\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7502 - accuracy: 0.7299 - val_loss: 1.2983 - val_accuracy: 0.5137\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7493 - accuracy: 0.7355 - val_loss: 1.2189 - val_accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7292 - accuracy: 0.7438 - val_loss: 2.5115 - val_accuracy: 0.4626\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7161 - accuracy: 0.7513 - val_loss: 1.2825 - val_accuracy: 0.5692\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7074 - accuracy: 0.7531 - val_loss: 1.3732 - val_accuracy: 0.5104\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7250 - accuracy: 0.7483 - val_loss: 1.3064 - val_accuracy: 0.5341\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6985 - accuracy: 0.7584 - val_loss: 1.3195 - val_accuracy: 0.5402\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7017 - accuracy: 0.7542 - val_loss: 0.9526 - val_accuracy: 0.6946\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7312 - accuracy: 0.7508 - val_loss: 1.6743 - val_accuracy: 0.4238\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6833 - accuracy: 0.7562 - val_loss: 1.3955 - val_accuracy: 0.5345\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6543 - accuracy: 0.7703 - val_loss: 1.0553 - val_accuracy: 0.6778\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6690 - accuracy: 0.7638 - val_loss: 1.2562 - val_accuracy: 0.5708\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7082 - accuracy: 0.7538 - val_loss: 1.6344 - val_accuracy: 0.5308\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7049 - accuracy: 0.7595 - val_loss: 1.3517 - val_accuracy: 0.5774\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6417 - accuracy: 0.7722 - val_loss: 1.1333 - val_accuracy: 0.6411\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6500 - accuracy: 0.7691 - val_loss: 0.8520 - val_accuracy: 0.6966\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6755 - accuracy: 0.7687 - val_loss: 1.2530 - val_accuracy: 0.6374\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6738 - accuracy: 0.7661 - val_loss: 1.5534 - val_accuracy: 0.5811\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6870 - accuracy: 0.7592 - val_loss: 0.9022 - val_accuracy: 0.7068\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6321 - accuracy: 0.7786 - val_loss: 0.8907 - val_accuracy: 0.6827\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6637 - accuracy: 0.7661 - val_loss: 0.9314 - val_accuracy: 0.6999\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6029 - accuracy: 0.7892 - val_loss: 1.0503 - val_accuracy: 0.6109\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6161 - accuracy: 0.7877 - val_loss: 1.0461 - val_accuracy: 0.6737\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6099 - accuracy: 0.7850 - val_loss: 0.9290 - val_accuracy: 0.7309\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6905 - accuracy: 0.7700 - val_loss: 0.8996 - val_accuracy: 0.7040\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5736 - accuracy: 0.7941 - val_loss: 0.9392 - val_accuracy: 0.6856\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6104 - accuracy: 0.7840 - val_loss: 0.9768 - val_accuracy: 0.6313\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6668 - accuracy: 0.7729 - val_loss: 0.8805 - val_accuracy: 0.6688\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6301 - accuracy: 0.7771 - val_loss: 1.5101 - val_accuracy: 0.4880\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5653 - accuracy: 0.7996 - val_loss: 1.2657 - val_accuracy: 0.6337\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5929 - accuracy: 0.7936 - val_loss: 1.1573 - val_accuracy: 0.6337\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5646 - accuracy: 0.8039 - val_loss: 1.3133 - val_accuracy: 0.6002\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5903 - accuracy: 0.7978 - val_loss: 1.0344 - val_accuracy: 0.6529\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6061 - accuracy: 0.7899 - val_loss: 1.1426 - val_accuracy: 0.6407\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5685 - accuracy: 0.8060 - val_loss: 0.9725 - val_accuracy: 0.7170\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4342 - accuracy: 0.5369 - val_loss: 1.6824 - val_accuracy: 0.3859\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1691 - accuracy: 0.5888 - val_loss: 1.7813 - val_accuracy: 0.3789\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9885 - accuracy: 0.6431 - val_loss: 1.9256 - val_accuracy: 0.4226\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9160 - accuracy: 0.6717 - val_loss: 1.0751 - val_accuracy: 0.6688\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8782 - accuracy: 0.6861 - val_loss: 1.0680 - val_accuracy: 0.6280\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8094 - accuracy: 0.7086 - val_loss: 0.9271 - val_accuracy: 0.6664\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8022 - accuracy: 0.7110 - val_loss: 0.8620 - val_accuracy: 0.6815\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7750 - accuracy: 0.7285 - val_loss: 1.3161 - val_accuracy: 0.6260\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7471 - accuracy: 0.7330 - val_loss: 0.9952 - val_accuracy: 0.6827\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7828 - accuracy: 0.7233 - val_loss: 1.0798 - val_accuracy: 0.6872\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7577 - accuracy: 0.7350 - val_loss: 1.0796 - val_accuracy: 0.6382\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7370 - accuracy: 0.7379 - val_loss: 1.2744 - val_accuracy: 0.6525\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7248 - accuracy: 0.7451 - val_loss: 1.1733 - val_accuracy: 0.6374\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7198 - accuracy: 0.7433 - val_loss: 0.8695 - val_accuracy: 0.7207\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6945 - accuracy: 0.7550 - val_loss: 1.1296 - val_accuracy: 0.6325\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6939 - accuracy: 0.7576 - val_loss: 1.0490 - val_accuracy: 0.6488\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6579 - accuracy: 0.7690 - val_loss: 0.9194 - val_accuracy: 0.6782\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7184 - accuracy: 0.7523 - val_loss: 0.9402 - val_accuracy: 0.7374\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7093 - accuracy: 0.7521 - val_loss: 0.9872 - val_accuracy: 0.6415\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6523 - accuracy: 0.7705 - val_loss: 1.2360 - val_accuracy: 0.6382\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6536 - accuracy: 0.7700 - val_loss: 1.3277 - val_accuracy: 0.5700\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6250 - accuracy: 0.7788 - val_loss: 0.8540 - val_accuracy: 0.7538\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5881 - accuracy: 0.7919 - val_loss: 0.8953 - val_accuracy: 0.7330\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6394 - accuracy: 0.7742 - val_loss: 0.8311 - val_accuracy: 0.7097\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1369 - accuracy: 0.6377 - val_loss: 2.2318 - val_accuracy: 0.3132\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0569 - accuracy: 0.6275 - val_loss: 1.1724 - val_accuracy: 0.6419\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6910 - accuracy: 0.7527 - val_loss: 1.0569 - val_accuracy: 0.6456\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0569 - accuracy: 0.6456\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "[1 1 8 ... 1 2 0]\n",
      "Confusion matrix\n",
      " [[180  62   0   0   0   0   0   0   0   0]\n",
      " [  0 220   0   0   0   0   0   0   0   0]\n",
      " [ 11  97 139  16   0   0   0   0   0   0]\n",
      " [  5  94   6 174   0   0   0   0   0   0]\n",
      " [ 30  22   0   0 172   8  36   1   1   0]\n",
      " [  3  58   0   0   2 136  14   1   0   0]\n",
      " [  1  89   0   1   0   0 176   0   0   0]\n",
      " [ 13  50   8   0   5   2   4 174   0   0]\n",
      " [ 22  35   4   0   5   0   8  43 131   0]\n",
      " [  3  23   2   0   2   4  39  35   3  79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.71       242\n",
      "           1       0.29      1.00      0.45       220\n",
      "           2       0.87      0.53      0.66       263\n",
      "           3       0.91      0.62      0.74       279\n",
      "           4       0.92      0.64      0.75       270\n",
      "           5       0.91      0.64      0.75       214\n",
      "           6       0.64      0.66      0.65       267\n",
      "           7       0.69      0.68      0.68       256\n",
      "           8       0.97      0.53      0.68       248\n",
      "           9       1.00      0.42      0.59       190\n",
      "\n",
      "    accuracy                           0.65      2449\n",
      "   macro avg       0.79      0.65      0.67      2449\n",
      "weighted avg       0.79      0.65      0.67      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_42 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_43 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_21 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2478 - accuracy: 0.1496 - val_loss: 2.4345 - val_accuracy: 0.1078\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 2.0098 - accuracy: 0.1975 - val_loss: 2.4731 - val_accuracy: 0.1078\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8617 - accuracy: 0.2734 - val_loss: 2.5037 - val_accuracy: 0.1376\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7289 - accuracy: 0.3343 - val_loss: 2.3649 - val_accuracy: 0.1058\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5583 - accuracy: 0.4110 - val_loss: 2.2370 - val_accuracy: 0.1058\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4249 - accuracy: 0.4730 - val_loss: 2.2378 - val_accuracy: 0.1245\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3203 - accuracy: 0.5176 - val_loss: 2.0366 - val_accuracy: 0.2209\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2880 - accuracy: 0.5301 - val_loss: 2.5449 - val_accuracy: 0.2123\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1909 - accuracy: 0.5655 - val_loss: 2.1582 - val_accuracy: 0.1895\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1647 - accuracy: 0.5798 - val_loss: 2.4049 - val_accuracy: 0.1788\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1268 - accuracy: 0.5932 - val_loss: 2.2470 - val_accuracy: 0.2017\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2110 - accuracy: 0.5651 - val_loss: 1.9851 - val_accuracy: 0.2352\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1564 - accuracy: 0.5846 - val_loss: 2.1041 - val_accuracy: 0.2487\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0568 - accuracy: 0.6155 - val_loss: 2.2444 - val_accuracy: 0.2993\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0715 - accuracy: 0.6141 - val_loss: 2.3907 - val_accuracy: 0.2785\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0116 - accuracy: 0.6306 - val_loss: 1.6843 - val_accuracy: 0.3765\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9822 - accuracy: 0.6439 - val_loss: 2.2000 - val_accuracy: 0.3418\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9797 - accuracy: 0.6525 - val_loss: 1.9280 - val_accuracy: 0.3830\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0164 - accuracy: 0.6452 - val_loss: 2.1264 - val_accuracy: 0.3250\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9252 - accuracy: 0.6640 - val_loss: 1.5564 - val_accuracy: 0.4373\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9027 - accuracy: 0.6743 - val_loss: 1.3132 - val_accuracy: 0.5337\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8885 - accuracy: 0.6817 - val_loss: 1.6934 - val_accuracy: 0.4410\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8776 - accuracy: 0.6809 - val_loss: 1.6130 - val_accuracy: 0.4308\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0194 - accuracy: 0.6396 - val_loss: 2.4354 - val_accuracy: 0.3169\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8622 - accuracy: 0.6897 - val_loss: 1.3419 - val_accuracy: 0.4716\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8530 - accuracy: 0.6943 - val_loss: 1.1641 - val_accuracy: 0.5962\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8607 - accuracy: 0.6907 - val_loss: 0.9355 - val_accuracy: 0.6811\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7880 - accuracy: 0.7090 - val_loss: 1.6853 - val_accuracy: 0.4406\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8255 - accuracy: 0.7028 - val_loss: 1.2347 - val_accuracy: 0.5557\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8114 - accuracy: 0.7065 - val_loss: 1.3349 - val_accuracy: 0.5512\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8150 - accuracy: 0.7061 - val_loss: 1.1907 - val_accuracy: 0.5937\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8218 - accuracy: 0.7066 - val_loss: 1.7491 - val_accuracy: 0.4786\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8031 - accuracy: 0.7146 - val_loss: 1.8524 - val_accuracy: 0.4100\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7709 - accuracy: 0.7244 - val_loss: 1.1461 - val_accuracy: 0.6162\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7968 - accuracy: 0.7110 - val_loss: 1.4155 - val_accuracy: 0.5557\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7512 - accuracy: 0.7317 - val_loss: 1.1112 - val_accuracy: 0.5958\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7288 - accuracy: 0.7317 - val_loss: 1.7132 - val_accuracy: 0.4630\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6866 - accuracy: 0.7545 - val_loss: 1.6130 - val_accuracy: 0.4300\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7105 - accuracy: 0.7468 - val_loss: 0.9294 - val_accuracy: 0.6774\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7702 - accuracy: 0.7301 - val_loss: 0.8864 - val_accuracy: 0.6648\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7301 - accuracy: 0.7416 - val_loss: 1.5511 - val_accuracy: 0.4508\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7250 - accuracy: 0.7417 - val_loss: 1.4986 - val_accuracy: 0.4524\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7308 - accuracy: 0.7445 - val_loss: 1.4923 - val_accuracy: 0.4773\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6856 - accuracy: 0.7578 - val_loss: 1.4430 - val_accuracy: 0.3989\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6653 - accuracy: 0.7563 - val_loss: 1.4977 - val_accuracy: 0.4786\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6769 - accuracy: 0.7546 - val_loss: 1.5617 - val_accuracy: 0.4161\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6754 - accuracy: 0.7596 - val_loss: 0.9230 - val_accuracy: 0.6492\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6650 - accuracy: 0.7611 - val_loss: 1.1067 - val_accuracy: 0.5872\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7080 - accuracy: 0.7574 - val_loss: 0.8237 - val_accuracy: 0.6995\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6746 - accuracy: 0.7674 - val_loss: 1.5005 - val_accuracy: 0.4606\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6457 - accuracy: 0.7709 - val_loss: 1.6421 - val_accuracy: 0.4826\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6250 - accuracy: 0.7751 - val_loss: 0.9004 - val_accuracy: 0.6484\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 3s 21ms/step - loss: 0.6624 - accuracy: 0.7723 - val_loss: 0.8597 - val_accuracy: 0.6966\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6495 - accuracy: 0.7745 - val_loss: 1.0993 - val_accuracy: 0.6113\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6037 - accuracy: 0.7852 - val_loss: 1.0703 - val_accuracy: 0.6419\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7025 - accuracy: 0.7654 - val_loss: 1.2668 - val_accuracy: 0.7562\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5977 - accuracy: 0.7882 - val_loss: 1.0975 - val_accuracy: 0.6484\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5500 - accuracy: 0.5123 - val_loss: 2.1415 - val_accuracy: 0.2209\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4033 - accuracy: 0.5199 - val_loss: 1.9573 - val_accuracy: 0.2842\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2215 - accuracy: 0.5799 - val_loss: 1.6718 - val_accuracy: 0.3716\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1862 - accuracy: 0.5872 - val_loss: 1.2984 - val_accuracy: 0.5370\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0877 - accuracy: 0.6158 - val_loss: 1.3418 - val_accuracy: 0.4937\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0535 - accuracy: 0.6313 - val_loss: 1.6200 - val_accuracy: 0.4463\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0378 - accuracy: 0.6373 - val_loss: 1.3511 - val_accuracy: 0.5492\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0148 - accuracy: 0.6396 - val_loss: 1.2756 - val_accuracy: 0.5647\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0015 - accuracy: 0.6482 - val_loss: 1.3130 - val_accuracy: 0.5227\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9994 - accuracy: 0.6477 - val_loss: 1.7806 - val_accuracy: 0.4688\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0633 - accuracy: 0.6299 - val_loss: 1.2647 - val_accuracy: 0.5574\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0148 - accuracy: 0.6449 - val_loss: 1.4505 - val_accuracy: 0.5684\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9854 - accuracy: 0.6584 - val_loss: 1.3913 - val_accuracy: 0.5537\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9493 - accuracy: 0.6672 - val_loss: 1.1472 - val_accuracy: 0.6015\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9110 - accuracy: 0.6746 - val_loss: 1.1080 - val_accuracy: 0.5994\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9361 - accuracy: 0.6661 - val_loss: 1.0239 - val_accuracy: 0.6170\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8858 - accuracy: 0.6829 - val_loss: 1.0906 - val_accuracy: 0.6105\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8679 - accuracy: 0.6914 - val_loss: 1.0768 - val_accuracy: 0.6578\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8800 - accuracy: 0.6863 - val_loss: 1.0568 - val_accuracy: 0.6427\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8706 - accuracy: 0.6916 - val_loss: 0.9865 - val_accuracy: 0.6648\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8600 - accuracy: 0.6956 - val_loss: 1.0997 - val_accuracy: 0.6260\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8972 - accuracy: 0.6868 - val_loss: 1.4437 - val_accuracy: 0.5308\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8476 - accuracy: 0.7002 - val_loss: 1.1593 - val_accuracy: 0.5970\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8514 - accuracy: 0.7019 - val_loss: 1.2240 - val_accuracy: 0.6198\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8234 - accuracy: 0.7062 - val_loss: 1.2410 - val_accuracy: 0.6027\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8337 - accuracy: 0.7035 - val_loss: 1.5545 - val_accuracy: 0.5063\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8158 - accuracy: 0.7104 - val_loss: 1.4811 - val_accuracy: 0.5055\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7853 - accuracy: 0.7196 - val_loss: 1.1987 - val_accuracy: 0.6031\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7944 - accuracy: 0.7182 - val_loss: 1.2459 - val_accuracy: 0.5643\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8456 - accuracy: 0.7083 - val_loss: 1.6288 - val_accuracy: 0.5210\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8047 - accuracy: 0.7264 - val_loss: 1.0967 - val_accuracy: 0.6337\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.7253 - val_loss: 1.1228 - val_accuracy: 0.6223\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7358 - accuracy: 0.7317 - val_loss: 1.0040 - val_accuracy: 0.6476\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8234 - accuracy: 0.7247 - val_loss: 1.7916 - val_accuracy: 0.3663\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8045 - accuracy: 0.7217 - val_loss: 1.1120 - val_accuracy: 0.6656\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7434 - accuracy: 0.7375 - val_loss: 1.3417 - val_accuracy: 0.5643\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7362 - val_loss: 1.4054 - val_accuracy: 0.4577\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7255 - accuracy: 0.7458 - val_loss: 1.2686 - val_accuracy: 0.6117\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7019 - accuracy: 0.7513 - val_loss: 0.9936 - val_accuracy: 0.6750\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7669 - accuracy: 0.7358 - val_loss: 1.0518 - val_accuracy: 0.6325\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7717 - accuracy: 0.7427 - val_loss: 1.2672 - val_accuracy: 0.5790\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7940 - accuracy: 0.7279 - val_loss: 1.3951 - val_accuracy: 0.6170\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7231 - accuracy: 0.7498 - val_loss: 1.2151 - val_accuracy: 0.6313\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2151 - accuracy: 0.6313\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[6 5 7 ... 6 6 4]\n",
      "Confusion matrix\n",
      " [[131   0   0   0  15   0 104   0   0   0]\n",
      " [  0 143  13   0   0   0  75   0   0   0]\n",
      " [  0   0 199   0   0   0  83   0   0   0]\n",
      " [  0   7  85  92   1   0  63   0   0   0]\n",
      " [  0   0   0   0 180   6  67   0   0   0]\n",
      " [  0   1   0   0   5 131  75   0   0   0]\n",
      " [  0   0   0   0   0   2 257   0   0   0]\n",
      " [  0   0   3   0  21  12  53 147   3   1]\n",
      " [  0  10   1   0  22   2  53   1 171   1]\n",
      " [  0   4  22   0  10   0  56   8  18  95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.52      0.69       250\n",
      "           1       0.87      0.62      0.72       231\n",
      "           2       0.62      0.71      0.66       282\n",
      "           3       1.00      0.37      0.54       248\n",
      "           4       0.71      0.71      0.71       253\n",
      "           5       0.86      0.62      0.72       212\n",
      "           6       0.29      0.99      0.45       259\n",
      "           7       0.94      0.61      0.74       240\n",
      "           8       0.89      0.66      0.75       261\n",
      "           9       0.98      0.45      0.61       213\n",
      "\n",
      "    accuracy                           0.63      2449\n",
      "   macro avg       0.82      0.63      0.66      2449\n",
      "weighted avg       0.81      0.63      0.66      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_44 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_45 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_22 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_22 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 2.2435 - accuracy: 0.1473 - val_loss: 2.5586 - val_accuracy: 0.1053\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9662 - accuracy: 0.1926 - val_loss: 4.8712 - val_accuracy: 0.1380\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8644 - accuracy: 0.2551 - val_loss: 2.5685 - val_accuracy: 0.0996\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6662 - accuracy: 0.3732 - val_loss: 2.5926 - val_accuracy: 0.1784\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4997 - accuracy: 0.4567 - val_loss: 1.8885 - val_accuracy: 0.2960\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3936 - accuracy: 0.4972 - val_loss: 1.7188 - val_accuracy: 0.3356\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3458 - accuracy: 0.5151 - val_loss: 1.8812 - val_accuracy: 0.3728\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2652 - accuracy: 0.5485 - val_loss: 2.1881 - val_accuracy: 0.3638\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2355 - accuracy: 0.5609 - val_loss: 1.7004 - val_accuracy: 0.3973\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1751 - accuracy: 0.5746 - val_loss: 2.8215 - val_accuracy: 0.2989\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1745 - accuracy: 0.5820 - val_loss: 1.4114 - val_accuracy: 0.5321\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 1.1386 - accuracy: 0.5909 - val_loss: 1.3439 - val_accuracy: 0.5100\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 1.1545 - accuracy: 0.6014 - val_loss: 1.3234 - val_accuracy: 0.5141\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0571 - accuracy: 0.6165 - val_loss: 1.4423 - val_accuracy: 0.5227\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0432 - accuracy: 0.6359 - val_loss: 1.4105 - val_accuracy: 0.5165\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1009 - accuracy: 0.6118 - val_loss: 1.1534 - val_accuracy: 0.5786\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0902 - accuracy: 0.6159 - val_loss: 1.1507 - val_accuracy: 0.5647\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0398 - accuracy: 0.6320 - val_loss: 1.2904 - val_accuracy: 0.5570\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0230 - accuracy: 0.6374 - val_loss: 1.3286 - val_accuracy: 0.5112\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0459 - accuracy: 0.6276 - val_loss: 1.6646 - val_accuracy: 0.5182\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0340 - accuracy: 0.6374 - val_loss: 1.8924 - val_accuracy: 0.4140\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0146 - accuracy: 0.6455 - val_loss: 1.5745 - val_accuracy: 0.4169\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9468 - accuracy: 0.6655 - val_loss: 1.6288 - val_accuracy: 0.4083\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9565 - accuracy: 0.6621 - val_loss: 1.2859 - val_accuracy: 0.5194\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9314 - accuracy: 0.6714 - val_loss: 1.1784 - val_accuracy: 0.6329\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9058 - accuracy: 0.6799 - val_loss: 1.3005 - val_accuracy: 0.5733\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9937 - accuracy: 0.6586 - val_loss: 1.1608 - val_accuracy: 0.5851\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9193 - accuracy: 0.6774 - val_loss: 1.0932 - val_accuracy: 0.5116\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8266 - accuracy: 0.7052 - val_loss: 1.9148 - val_accuracy: 0.3981\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9590 - accuracy: 0.6751 - val_loss: 1.4886 - val_accuracy: 0.4292\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9312 - accuracy: 0.6831 - val_loss: 1.3520 - val_accuracy: 0.5304\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8683 - accuracy: 0.6961 - val_loss: 1.5652 - val_accuracy: 0.4149\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8896 - accuracy: 0.6919 - val_loss: 1.1272 - val_accuracy: 0.5459\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8079 - accuracy: 0.7136 - val_loss: 1.4486 - val_accuracy: 0.5108\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 4s 23ms/step - loss: 0.7918 - accuracy: 0.7158 - val_loss: 1.5173 - val_accuracy: 0.4810\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8412 - accuracy: 0.7023 - val_loss: 1.1270 - val_accuracy: 0.5586\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9595 - accuracy: 0.6843 - val_loss: 2.5725 - val_accuracy: 0.2401\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8454 - accuracy: 0.7045 - val_loss: 1.6370 - val_accuracy: 0.3320\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8157 - accuracy: 0.7146 - val_loss: 2.0485 - val_accuracy: 0.3230\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7854 - accuracy: 0.7220 - val_loss: 1.3821 - val_accuracy: 0.4969\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 3s 18ms/step - loss: 0.8214 - accuracy: 0.7117 - val_loss: 1.4997 - val_accuracy: 0.4679\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7978 - accuracy: 0.7227 - val_loss: 1.6345 - val_accuracy: 0.3822\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8479 - accuracy: 0.7100 - val_loss: 1.5197 - val_accuracy: 0.4573\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8132 - accuracy: 0.7183 - val_loss: 1.2916 - val_accuracy: 0.5659\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7759 - accuracy: 0.7390 - val_loss: 1.0890 - val_accuracy: 0.6121\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7080 - accuracy: 0.7527 - val_loss: 2.1409 - val_accuracy: 0.4806\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.7391 - val_loss: 1.0184 - val_accuracy: 0.6100\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8364 - accuracy: 0.7194 - val_loss: 1.9730 - val_accuracy: 0.3597\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0028 - accuracy: 0.6653 - val_loss: 1.3771 - val_accuracy: 0.4635\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9440 - accuracy: 0.6713 - val_loss: 1.2614 - val_accuracy: 0.5786\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8527 - accuracy: 0.6991 - val_loss: 1.1502 - val_accuracy: 0.5496\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8150 - accuracy: 0.7095 - val_loss: 1.3243 - val_accuracy: 0.5990\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7204 - accuracy: 0.7450 - val_loss: 1.5265 - val_accuracy: 0.5406\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7753 - accuracy: 0.7289 - val_loss: 1.1285 - val_accuracy: 0.5766\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7416 - accuracy: 0.7466 - val_loss: 1.1524 - val_accuracy: 0.5888\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7505 - accuracy: 0.7400 - val_loss: 1.2222 - val_accuracy: 0.5063\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7706 - accuracy: 0.7400 - val_loss: 1.5193 - val_accuracy: 0.4614\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9147 - accuracy: 0.6997 - val_loss: 1.1600 - val_accuracy: 0.6366\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7852 - accuracy: 0.7317 - val_loss: 1.4186 - val_accuracy: 0.4336\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7367 - accuracy: 0.7452 - val_loss: 1.5791 - val_accuracy: 0.4696\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7495 - accuracy: 0.7414 - val_loss: 1.0872 - val_accuracy: 0.5570\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7212 - accuracy: 0.7512 - val_loss: 1.0844 - val_accuracy: 0.5651\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7962 - accuracy: 0.7303 - val_loss: 1.4846 - val_accuracy: 0.5443\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7938 - accuracy: 0.7426 - val_loss: 1.3234 - val_accuracy: 0.5039\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7976 - accuracy: 0.7316 - val_loss: 1.8393 - val_accuracy: 0.4573\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7227 - accuracy: 0.7505 - val_loss: 1.4473 - val_accuracy: 0.5729\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6631 - accuracy: 0.7707 - val_loss: 1.1873 - val_accuracy: 0.6080\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6704 - accuracy: 0.7715 - val_loss: 1.3512 - val_accuracy: 0.5182\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7310 - accuracy: 0.7542 - val_loss: 1.8738 - val_accuracy: 0.3855\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7060 - accuracy: 0.7586 - val_loss: 1.5666 - val_accuracy: 0.4202\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7468 - accuracy: 0.7479 - val_loss: 1.4519 - val_accuracy: 0.4320\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7382 - accuracy: 0.7484 - val_loss: 1.1365 - val_accuracy: 0.5986\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7110 - accuracy: 0.7613 - val_loss: 1.1406 - val_accuracy: 0.6133\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6492 - accuracy: 0.7771 - val_loss: 0.8447 - val_accuracy: 0.6958\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6076 - accuracy: 0.7928 - val_loss: 1.0207 - val_accuracy: 0.6362\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7559 - accuracy: 0.7582 - val_loss: 1.0923 - val_accuracy: 0.5557\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6501 - accuracy: 0.7777 - val_loss: 1.0838 - val_accuracy: 0.6492\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6699 - accuracy: 0.7760 - val_loss: 1.1772 - val_accuracy: 0.5169\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6002 - accuracy: 0.7949 - val_loss: 1.2551 - val_accuracy: 0.5606\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7014 - accuracy: 0.7686 - val_loss: 1.2624 - val_accuracy: 0.5574\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5927 - accuracy: 0.7967 - val_loss: 1.2601 - val_accuracy: 0.6239\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6920 - accuracy: 0.7728 - val_loss: 1.1116 - val_accuracy: 0.6639\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7542 - accuracy: 0.7595 - val_loss: 1.3404 - val_accuracy: 0.5643\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8935 - accuracy: 0.6979 - val_loss: 1.5631 - val_accuracy: 0.5174\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6210 - accuracy: 0.7862 - val_loss: 1.1610 - val_accuracy: 0.5945\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5647 - accuracy: 0.8022 - val_loss: 0.9572 - val_accuracy: 0.7076\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6021 - accuracy: 0.7962 - val_loss: 1.0941 - val_accuracy: 0.6672\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7233 - accuracy: 0.7624 - val_loss: 1.5905 - val_accuracy: 0.4324\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5997 - accuracy: 0.7943 - val_loss: 1.2422 - val_accuracy: 0.5406\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5980 - accuracy: 0.8038 - val_loss: 1.1945 - val_accuracy: 0.5492\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6508 - accuracy: 0.7898 - val_loss: 1.6195 - val_accuracy: 0.4639\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6418 - accuracy: 0.7893 - val_loss: 1.5750 - val_accuracy: 0.5717\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6923 - accuracy: 0.7851 - val_loss: 2.2241 - val_accuracy: 0.4287\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6676 - accuracy: 0.7830 - val_loss: 1.1042 - val_accuracy: 0.6211\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6511 - accuracy: 0.7891 - val_loss: 1.2092 - val_accuracy: 0.5696\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5179 - accuracy: 0.8207 - val_loss: 0.8463 - val_accuracy: 0.7150\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5492 - accuracy: 0.8084 - val_loss: 1.0011 - val_accuracy: 0.7223\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5863 - accuracy: 0.7978 - val_loss: 1.3245 - val_accuracy: 0.6264\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6318 - accuracy: 0.7950 - val_loss: 0.9051 - val_accuracy: 0.6807\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6458 - accuracy: 0.7908 - val_loss: 1.5099 - val_accuracy: 0.5684\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5099 - accuracy: 0.5684\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[7 3 7 ... 0 0 4]\n",
      "Confusion matrix\n",
      " [[226   0   0   9   0   0   0   0   0   0]\n",
      " [ 66 144   0  39   0   0   0   0   0   0]\n",
      " [ 91   0   0 163   0   0   0   0   0   0]\n",
      " [ 15   7   0 239   0   0   0   0   0   0]\n",
      " [ 44   0   0   3 173  31   5   0   4   0]\n",
      " [ 15   0   0   0   0 167  22   4   0   0]\n",
      " [ 29   3   3  17   0  40 175   0   0   0]\n",
      " [ 28   1   0   0   0  49   8 132  10   0]\n",
      " [ 34   0   0  11  23  23   2  35 130   0]\n",
      " [ 20   0   0   7   3  34   3 147   9   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.96      0.56       235\n",
      "           1       0.93      0.58      0.71       249\n",
      "           2       0.00      0.00      0.00       254\n",
      "           3       0.49      0.92      0.64       261\n",
      "           4       0.87      0.67      0.75       260\n",
      "           5       0.49      0.80      0.61       208\n",
      "           6       0.81      0.66      0.73       267\n",
      "           7       0.42      0.58      0.48       228\n",
      "           8       0.85      0.50      0.63       258\n",
      "           9       1.00      0.03      0.05       229\n",
      "\n",
      "    accuracy                           0.57      2449\n",
      "   macro avg       0.63      0.57      0.52      2449\n",
      "weighted avg       0.63      0.57      0.52      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_46 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_47 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_23 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_23 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.2717 - accuracy: 0.1435 - val_loss: 30.3992 - val_accuracy: 0.0882\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 2.0246 - accuracy: 0.1850 - val_loss: 2.3837 - val_accuracy: 0.1107\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 1.9117 - accuracy: 0.2315 - val_loss: 2.4641 - val_accuracy: 0.0927\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 4s 23ms/step - loss: 1.8036 - accuracy: 0.3028 - val_loss: 3.0379 - val_accuracy: 0.1143\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6427 - accuracy: 0.3732 - val_loss: 2.4982 - val_accuracy: 0.1531\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6352 - accuracy: 0.4094 - val_loss: 2.9631 - val_accuracy: 0.1842\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4446 - accuracy: 0.4838 - val_loss: 1.9865 - val_accuracy: 0.2323\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3058 - accuracy: 0.5290 - val_loss: 1.7267 - val_accuracy: 0.3532\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2786 - accuracy: 0.5418 - val_loss: 1.5359 - val_accuracy: 0.4198\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1475 - accuracy: 0.5894 - val_loss: 1.2736 - val_accuracy: 0.5059\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1111 - accuracy: 0.5995 - val_loss: 1.4371 - val_accuracy: 0.4308\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1790 - accuracy: 0.5831 - val_loss: 1.6198 - val_accuracy: 0.4459\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0947 - accuracy: 0.6123 - val_loss: 1.3221 - val_accuracy: 0.4598\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1114 - accuracy: 0.6120 - val_loss: 1.2188 - val_accuracy: 0.5382\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1182 - accuracy: 0.6182 - val_loss: 1.6445 - val_accuracy: 0.4022\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0542 - accuracy: 0.6270 - val_loss: 1.9872 - val_accuracy: 0.4937\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0619 - accuracy: 0.6282 - val_loss: 1.1711 - val_accuracy: 0.6129\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0762 - accuracy: 0.6348 - val_loss: 1.7025 - val_accuracy: 0.3985\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1310 - accuracy: 0.6163 - val_loss: 1.1979 - val_accuracy: 0.5174\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0621 - accuracy: 0.6311 - val_loss: 1.0839 - val_accuracy: 0.6292\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0109 - accuracy: 0.6496 - val_loss: 2.1265 - val_accuracy: 0.4136\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9733 - accuracy: 0.6536 - val_loss: 1.1346 - val_accuracy: 0.5806\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0242 - accuracy: 0.6450 - val_loss: 1.8570 - val_accuracy: 0.2887\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9949 - accuracy: 0.6439 - val_loss: 2.6834 - val_accuracy: 0.2029\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9001 - accuracy: 0.6776 - val_loss: 2.9410 - val_accuracy: 0.2724\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9302 - accuracy: 0.6684 - val_loss: 1.7602 - val_accuracy: 0.3165\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9846 - accuracy: 0.6630 - val_loss: 2.7993 - val_accuracy: 0.2858\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9515 - accuracy: 0.6713 - val_loss: 1.3216 - val_accuracy: 0.4696\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8801 - accuracy: 0.6907 - val_loss: 1.3112 - val_accuracy: 0.5798\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9107 - accuracy: 0.6845 - val_loss: 1.9374 - val_accuracy: 0.4969\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9325 - accuracy: 0.6850 - val_loss: 1.2285 - val_accuracy: 0.5337\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9168 - accuracy: 0.6893 - val_loss: 1.0471 - val_accuracy: 0.6533\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8351 - accuracy: 0.7162 - val_loss: 1.3253 - val_accuracy: 0.5892\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8112 - accuracy: 0.7165 - val_loss: 1.1265 - val_accuracy: 0.5647\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8043 - accuracy: 0.7182 - val_loss: 1.0805 - val_accuracy: 0.6109\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8390 - accuracy: 0.7130 - val_loss: 0.9491 - val_accuracy: 0.6746\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8387 - accuracy: 0.7162 - val_loss: 1.3255 - val_accuracy: 0.4835\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8163 - accuracy: 0.7179 - val_loss: 1.1654 - val_accuracy: 0.5615\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8827 - accuracy: 0.7101 - val_loss: 1.2297 - val_accuracy: 0.6358\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8165 - accuracy: 0.7218 - val_loss: 1.1925 - val_accuracy: 0.6243\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8345 - accuracy: 0.7222 - val_loss: 1.0926 - val_accuracy: 0.6305\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7586 - accuracy: 0.7402 - val_loss: 1.3361 - val_accuracy: 0.5280\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8193 - accuracy: 0.7242 - val_loss: 1.4500 - val_accuracy: 0.5108\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7799 - accuracy: 0.7319 - val_loss: 1.1224 - val_accuracy: 0.6333\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7937 - accuracy: 0.7319 - val_loss: 0.9564 - val_accuracy: 0.6607\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7229 - accuracy: 0.7468 - val_loss: 0.9790 - val_accuracy: 0.6586\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6914 - accuracy: 0.7568 - val_loss: 1.0564 - val_accuracy: 0.6227\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7786 - accuracy: 0.7354 - val_loss: 1.0999 - val_accuracy: 0.6043\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7627 - accuracy: 0.7455 - val_loss: 1.0965 - val_accuracy: 0.5549\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8036 - accuracy: 0.7386 - val_loss: 1.0953 - val_accuracy: 0.6415\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6895 - accuracy: 0.7606 - val_loss: 1.0563 - val_accuracy: 0.6374\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6975 - accuracy: 0.7647 - val_loss: 1.0919 - val_accuracy: 0.5884\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6728 - accuracy: 0.7738 - val_loss: 0.9686 - val_accuracy: 0.6570\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6764 - accuracy: 0.7676 - val_loss: 1.2975 - val_accuracy: 0.5308\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7473 - accuracy: 0.7571 - val_loss: 0.8530 - val_accuracy: 0.6991\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9280 - accuracy: 0.7265 - val_loss: 0.7496 - val_accuracy: 0.7481\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7727 - accuracy: 0.7615 - val_loss: 0.8178 - val_accuracy: 0.7072\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6697 - accuracy: 0.7753 - val_loss: 0.8012 - val_accuracy: 0.7497\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6322 - accuracy: 0.7801 - val_loss: 0.9915 - val_accuracy: 0.5794\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6340 - accuracy: 0.7843 - val_loss: 0.7723 - val_accuracy: 0.7489\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6861 - accuracy: 0.7701 - val_loss: 0.8947 - val_accuracy: 0.6750\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6904 - accuracy: 0.7723 - val_loss: 1.4079 - val_accuracy: 0.5888\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6444 - accuracy: 0.7839 - val_loss: 1.4260 - val_accuracy: 0.5259\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6403 - accuracy: 0.7796 - val_loss: 0.8933 - val_accuracy: 0.7109\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6541 - accuracy: 0.7882 - val_loss: 2.9842 - val_accuracy: 0.4610\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8137 - accuracy: 0.7451 - val_loss: 1.2186 - val_accuracy: 0.5696\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6798 - accuracy: 0.7676 - val_loss: 1.2915 - val_accuracy: 0.5382\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6275 - accuracy: 0.7865 - val_loss: 1.8319 - val_accuracy: 0.3879\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6663 - accuracy: 0.7775 - val_loss: 1.1698 - val_accuracy: 0.5566\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6199 - accuracy: 0.7908 - val_loss: 1.5604 - val_accuracy: 0.4659\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6366 - accuracy: 0.7836 - val_loss: 1.3198 - val_accuracy: 0.5484\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6601 - accuracy: 0.7887 - val_loss: 1.2015 - val_accuracy: 0.5561\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5718 - accuracy: 0.8046 - val_loss: 1.2105 - val_accuracy: 0.5982\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6483 - accuracy: 0.7868 - val_loss: 1.2109 - val_accuracy: 0.5329\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6434 - accuracy: 0.7901 - val_loss: 0.9343 - val_accuracy: 0.6966\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6803 - accuracy: 0.7822 - val_loss: 1.0740 - val_accuracy: 0.6909\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5982 - accuracy: 0.8013 - val_loss: 0.8843 - val_accuracy: 0.6309\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6025 - accuracy: 0.8001 - val_loss: 1.2394 - val_accuracy: 0.5753\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6667 - accuracy: 0.7867 - val_loss: 1.0902 - val_accuracy: 0.6345\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6540 - accuracy: 0.7934 - val_loss: 1.3202 - val_accuracy: 0.6456\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5607 - accuracy: 0.8154 - val_loss: 1.2227 - val_accuracy: 0.6015\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5571 - accuracy: 0.8140 - val_loss: 0.8441 - val_accuracy: 0.7060\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6043 - accuracy: 0.8032 - val_loss: 1.0862 - val_accuracy: 0.6419\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6272 - accuracy: 0.7995 - val_loss: 1.2195 - val_accuracy: 0.6448\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7526 - accuracy: 0.7729 - val_loss: 1.0525 - val_accuracy: 0.6517\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5738 - accuracy: 0.8078 - val_loss: 0.7423 - val_accuracy: 0.7301\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.4825 - accuracy: 0.8339 - val_loss: 0.6936 - val_accuracy: 0.7505\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5254 - accuracy: 0.8300 - val_loss: 0.6534 - val_accuracy: 0.7828\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5375 - accuracy: 0.8168 - val_loss: 0.9637 - val_accuracy: 0.6337\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5211 - accuracy: 0.8335 - val_loss: 1.1655 - val_accuracy: 0.5806\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5439 - accuracy: 0.8235 - val_loss: 0.6968 - val_accuracy: 0.7652\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6352 - accuracy: 0.7970 - val_loss: 1.2891 - val_accuracy: 0.6002\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6304 - accuracy: 0.8008 - val_loss: 1.1531 - val_accuracy: 0.6080\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5322 - accuracy: 0.8264 - val_loss: 1.0515 - val_accuracy: 0.6684\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5709 - accuracy: 0.8138 - val_loss: 0.7161 - val_accuracy: 0.7526\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5834 - accuracy: 0.8177 - val_loss: 0.8255 - val_accuracy: 0.7493\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5270 - accuracy: 0.8261 - val_loss: 0.5794 - val_accuracy: 0.7930\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5425 - accuracy: 0.8214 - val_loss: 0.5503 - val_accuracy: 0.8089\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5417 - accuracy: 0.8248 - val_loss: 0.7207 - val_accuracy: 0.7660\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.5549 - accuracy: 0.8227 - val_loss: 0.5472 - val_accuracy: 0.8060\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5472 - accuracy: 0.8060\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[2 6 7 ... 2 3 4]\n",
      "Confusion matrix\n",
      " [[233   0   0  31   0   0   0   0   0   0]\n",
      " [  9 184   0  54   0   0   0   0   0   0]\n",
      " [ 26   1 193  55   0   0   0   7   0   0]\n",
      " [  1   0   0 268   0   0   0   0   0   0]\n",
      " [ 13   0   0   0 181   2   0  16   9   0]\n",
      " [ 15   0   0   8   0 160  11  21   0   0]\n",
      " [ 21   0   0  14   8   4 176  10   1   0]\n",
      " [  1   0   0   6   0   0   1 224   0   0]\n",
      " [  7   0   0   1  15   0   0  44 203   0]\n",
      " [  9   0   0   7   3   0   0  24  20 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78       264\n",
      "           1       0.99      0.74      0.85       247\n",
      "           2       1.00      0.68      0.81       282\n",
      "           3       0.60      1.00      0.75       269\n",
      "           4       0.87      0.82      0.85       221\n",
      "           5       0.96      0.74      0.84       215\n",
      "           6       0.94      0.75      0.83       234\n",
      "           7       0.65      0.97      0.78       232\n",
      "           8       0.87      0.75      0.81       270\n",
      "           9       1.00      0.71      0.83       215\n",
      "\n",
      "    accuracy                           0.81      2449\n",
      "   macro avg       0.86      0.80      0.81      2449\n",
      "weighted avg       0.85      0.81      0.81      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_48 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_49 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_24 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_24 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.2660 - accuracy: 0.1539 - val_loss: 2.3454 - val_accuracy: 0.1017\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9525 - accuracy: 0.2344 - val_loss: 2.4676 - val_accuracy: 0.0911\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8389 - accuracy: 0.3006 - val_loss: 2.7729 - val_accuracy: 0.1200\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7104 - accuracy: 0.3403 - val_loss: 2.0653 - val_accuracy: 0.1952\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 1.6291 - accuracy: 0.3893 - val_loss: 1.9845 - val_accuracy: 0.2499\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 1.5456 - accuracy: 0.4421 - val_loss: 1.8644 - val_accuracy: 0.3038\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4632 - accuracy: 0.4789 - val_loss: 2.0377 - val_accuracy: 0.3042\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3045 - accuracy: 0.5395 - val_loss: 1.7473 - val_accuracy: 0.4210\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2433 - accuracy: 0.5556 - val_loss: 1.4452 - val_accuracy: 0.4688\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1961 - accuracy: 0.5716 - val_loss: 1.8424 - val_accuracy: 0.3748\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2015 - accuracy: 0.5690 - val_loss: 1.4486 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1814 - accuracy: 0.5819 - val_loss: 1.4225 - val_accuracy: 0.5137\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2853 - accuracy: 0.5648 - val_loss: 2.2770 - val_accuracy: 0.3422\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1445 - accuracy: 0.5951 - val_loss: 2.0474 - val_accuracy: 0.3528\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1415 - accuracy: 0.5959 - val_loss: 1.4547 - val_accuracy: 0.4941\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3498 - accuracy: 0.5363 - val_loss: 3.3479 - val_accuracy: 0.1315\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4426 - accuracy: 0.4988 - val_loss: 2.1830 - val_accuracy: 0.3124\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2145 - accuracy: 0.5742 - val_loss: 1.5281 - val_accuracy: 0.3765\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1829 - accuracy: 0.5898 - val_loss: 2.3731 - val_accuracy: 0.3614\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0354 - accuracy: 0.6280 - val_loss: 2.7201 - val_accuracy: 0.3079\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1244 - accuracy: 0.6125 - val_loss: 2.0026 - val_accuracy: 0.3712\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0478 - accuracy: 0.6312 - val_loss: 1.3202 - val_accuracy: 0.5431\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9740 - accuracy: 0.6468 - val_loss: 1.5910 - val_accuracy: 0.5214\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0566 - accuracy: 0.6406 - val_loss: 1.4059 - val_accuracy: 0.5006\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0012 - accuracy: 0.6477 - val_loss: 1.2927 - val_accuracy: 0.5774\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9655 - accuracy: 0.6569 - val_loss: 1.4376 - val_accuracy: 0.5010\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9435 - accuracy: 0.6687 - val_loss: 1.3045 - val_accuracy: 0.5590\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8765 - accuracy: 0.6847 - val_loss: 2.1728 - val_accuracy: 0.4365\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8821 - accuracy: 0.6848 - val_loss: 1.0016 - val_accuracy: 0.6595\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9844 - accuracy: 0.6698 - val_loss: 1.5341 - val_accuracy: 0.4851\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9246 - accuracy: 0.6805 - val_loss: 1.0792 - val_accuracy: 0.6051\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9413 - accuracy: 0.6817 - val_loss: 1.2495 - val_accuracy: 0.5210\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0494 - accuracy: 0.6641 - val_loss: 1.6938 - val_accuracy: 0.5414\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0193 - accuracy: 0.6601 - val_loss: 1.9146 - val_accuracy: 0.5039\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9094 - accuracy: 0.6871 - val_loss: 0.9904 - val_accuracy: 0.6562\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8633 - accuracy: 0.6977 - val_loss: 1.2191 - val_accuracy: 0.6011\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7975 - accuracy: 0.7159 - val_loss: 1.1013 - val_accuracy: 0.6039\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8835 - accuracy: 0.7052 - val_loss: 1.2791 - val_accuracy: 0.5476\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9764 - accuracy: 0.6896 - val_loss: 1.1324 - val_accuracy: 0.5937\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 0.8722 - accuracy: 0.7008 - val_loss: 1.8807 - val_accuracy: 0.4888\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.7851 - accuracy: 0.7210 - val_loss: 1.2286 - val_accuracy: 0.6031\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8107 - accuracy: 0.7156 - val_loss: 1.0466 - val_accuracy: 0.6546\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7971 - accuracy: 0.7298 - val_loss: 1.4676 - val_accuracy: 0.4973\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7955 - accuracy: 0.7280 - val_loss: 1.5990 - val_accuracy: 0.5022\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 3s 18ms/step - loss: 0.7352 - accuracy: 0.7454 - val_loss: 1.9863 - val_accuracy: 0.4769\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9366 - accuracy: 0.6990 - val_loss: 2.9431 - val_accuracy: 0.3552\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9888 - accuracy: 0.6838 - val_loss: 1.7644 - val_accuracy: 0.5500\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7735 - accuracy: 0.7316 - val_loss: 1.2798 - val_accuracy: 0.5451\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8069 - accuracy: 0.7393 - val_loss: 2.3206 - val_accuracy: 0.3793\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8200 - accuracy: 0.7302 - val_loss: 2.0620 - val_accuracy: 0.4749\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7841 - accuracy: 0.7343 - val_loss: 1.3600 - val_accuracy: 0.5390\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8260 - accuracy: 0.7295 - val_loss: 0.8449 - val_accuracy: 0.6987\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8158 - accuracy: 0.7347 - val_loss: 2.5198 - val_accuracy: 0.3132\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8041 - accuracy: 0.7333 - val_loss: 1.1974 - val_accuracy: 0.5770\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6878 - accuracy: 0.7678 - val_loss: 0.9112 - val_accuracy: 0.6770\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7561 - accuracy: 0.7414 - val_loss: 1.0731 - val_accuracy: 0.6084\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7185 - accuracy: 0.7542 - val_loss: 0.9338 - val_accuracy: 0.6929\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8513 - accuracy: 0.7397 - val_loss: 1.4230 - val_accuracy: 0.5827\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7314 - accuracy: 0.7494 - val_loss: 1.0020 - val_accuracy: 0.6823\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6830 - accuracy: 0.7614 - val_loss: 1.2395 - val_accuracy: 0.6207\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7185 - accuracy: 0.7564 - val_loss: 1.1582 - val_accuracy: 0.6162\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7084 - accuracy: 0.7600 - val_loss: 1.0748 - val_accuracy: 0.6019\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8297 - accuracy: 0.7365 - val_loss: 1.3780 - val_accuracy: 0.5153\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6897 - accuracy: 0.7670 - val_loss: 1.1188 - val_accuracy: 0.6492\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6138 - accuracy: 0.7807 - val_loss: 1.3943 - val_accuracy: 0.4904\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7370 - accuracy: 0.7565 - val_loss: 2.2007 - val_accuracy: 0.4292\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 3s 20ms/step - loss: 0.6720 - accuracy: 0.7680 - val_loss: 1.3388 - val_accuracy: 0.5255\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8438 - accuracy: 0.7198 - val_loss: 1.4192 - val_accuracy: 0.5361\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6676 - accuracy: 0.7740 - val_loss: 1.3149 - val_accuracy: 0.5496\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7456 - accuracy: 0.7571 - val_loss: 1.4659 - val_accuracy: 0.5647\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8044 - accuracy: 0.7521 - val_loss: 1.1604 - val_accuracy: 0.6411\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8828 - accuracy: 0.7350 - val_loss: 1.4016 - val_accuracy: 0.5602\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7043 - accuracy: 0.7678 - val_loss: 1.0724 - val_accuracy: 0.6693\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6434 - accuracy: 0.7869 - val_loss: 1.7226 - val_accuracy: 0.4953\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6277 - accuracy: 0.7868 - val_loss: 1.0522 - val_accuracy: 0.6439\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6499 - accuracy: 0.7836 - val_loss: 1.1939 - val_accuracy: 0.6158\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7367 - accuracy: 0.7681 - val_loss: 1.2010 - val_accuracy: 0.5263\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8103 - accuracy: 0.7526 - val_loss: 1.6309 - val_accuracy: 0.5521\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6579 - accuracy: 0.7836 - val_loss: 1.1601 - val_accuracy: 0.6325\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5589 - accuracy: 0.7976 - val_loss: 1.6383 - val_accuracy: 0.5737\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6074 - accuracy: 0.7910 - val_loss: 0.8509 - val_accuracy: 0.6872\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6169 - accuracy: 0.7934 - val_loss: 1.0169 - val_accuracy: 0.6345\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5711 - accuracy: 0.8038 - val_loss: 1.0195 - val_accuracy: 0.6484\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5805 - accuracy: 0.8055 - val_loss: 0.9390 - val_accuracy: 0.6452\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7243 - accuracy: 0.7713 - val_loss: 2.0701 - val_accuracy: 0.4198\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6073 - accuracy: 0.8002 - val_loss: 1.6416 - val_accuracy: 0.4398\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6713 - accuracy: 0.7884 - val_loss: 2.1568 - val_accuracy: 0.4549\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6504 - accuracy: 0.7892 - val_loss: 2.4500 - val_accuracy: 0.3846\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.7201 - accuracy: 0.7791 - val_loss: 1.5355 - val_accuracy: 0.5370\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.6650 - accuracy: 0.7837 - val_loss: 1.1519 - val_accuracy: 0.6713\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5618 - accuracy: 0.8117 - val_loss: 1.4569 - val_accuracy: 0.5700\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5785 - accuracy: 0.8039 - val_loss: 0.9035 - val_accuracy: 0.6733\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5434 - accuracy: 0.8150 - val_loss: 0.9354 - val_accuracy: 0.6623\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.5269 - accuracy: 0.8188 - val_loss: 1.0126 - val_accuracy: 0.6566\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5741 - accuracy: 0.8088 - val_loss: 0.8751 - val_accuracy: 0.6929\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.5874 - accuracy: 0.8064 - val_loss: 0.8962 - val_accuracy: 0.7036\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6960 - accuracy: 0.7919 - val_loss: 0.7658 - val_accuracy: 0.7456\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7243 - accuracy: 0.7801 - val_loss: 1.1134 - val_accuracy: 0.6635\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6326 - accuracy: 0.8011 - val_loss: 2.2668 - val_accuracy: 0.4483\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6544 - accuracy: 0.7932 - val_loss: 1.7903 - val_accuracy: 0.4635\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7903 - accuracy: 0.4635\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "[4 0 0 ... 0 8 8]\n",
      "Confusion matrix\n",
      " [[274   0   0   0   0   0   0   0   0   0]\n",
      " [164  69   0   0   0   0   0   0   0   0]\n",
      " [ 69   0 178   0   0   0   0   0   0   0]\n",
      " [ 65   0 151  46   0   0   0   0   0   0]\n",
      " [ 38   0   0   0 206   2   0   0   0   0]\n",
      " [ 61   0   0   0  48  87   0   0   0   0]\n",
      " [154   0   1   0 124   7   2   0   0   0]\n",
      " [ 55   0  28   0  41  12   0  44  41   0]\n",
      " [ 48   0  28   0  22   1   0   0 166   0]\n",
      " [ 25   0  19   0  65   4   0   0  41  63]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      1.00      0.45       274\n",
      "           1       1.00      0.30      0.46       233\n",
      "           2       0.44      0.72      0.55       247\n",
      "           3       1.00      0.18      0.30       262\n",
      "           4       0.41      0.84      0.55       246\n",
      "           5       0.77      0.44      0.56       196\n",
      "           6       1.00      0.01      0.01       288\n",
      "           7       1.00      0.20      0.33       221\n",
      "           8       0.67      0.63      0.65       265\n",
      "           9       1.00      0.29      0.45       217\n",
      "\n",
      "    accuracy                           0.46      2449\n",
      "   macro avg       0.76      0.46      0.43      2449\n",
      "weighted avg       0.75      0.46      0.42      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_50 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_51 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_25 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 2.2850 - accuracy: 0.1455 - val_loss: 2.3429 - val_accuracy: 0.1062\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 2.0406 - accuracy: 0.1941 - val_loss: 114.5637 - val_accuracy: 0.1143\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9445 - accuracy: 0.2342 - val_loss: 2.3532 - val_accuracy: 0.1213\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8510 - accuracy: 0.2795 - val_loss: 2.4624 - val_accuracy: 0.1143\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7898 - accuracy: 0.3065 - val_loss: 2.7566 - val_accuracy: 0.1045\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.8219 - accuracy: 0.3113 - val_loss: 2.6197 - val_accuracy: 0.0968\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6718 - accuracy: 0.3717 - val_loss: 2.1736 - val_accuracy: 0.1866\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5923 - accuracy: 0.3939 - val_loss: 2.1346 - val_accuracy: 0.2062\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.8537 - accuracy: 0.3258 - val_loss: 2.2923 - val_accuracy: 0.1119\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.6607 - accuracy: 0.3897 - val_loss: 2.3564 - val_accuracy: 0.1927\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7684 - accuracy: 0.3880 - val_loss: 2.0535 - val_accuracy: 0.2491\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7988 - accuracy: 0.3838 - val_loss: 2.0996 - val_accuracy: 0.2225\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.6510 - accuracy: 0.4187 - val_loss: 4.9656 - val_accuracy: 0.1352\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.9492 - accuracy: 0.2779 - val_loss: 2.4312 - val_accuracy: 0.1037\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.6425 - accuracy: 0.3952 - val_loss: 2.6415 - val_accuracy: 0.1670\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.4573 - accuracy: 0.4644 - val_loss: 6.6849 - val_accuracy: 0.0833\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.4879 - accuracy: 0.4685 - val_loss: 2.3013 - val_accuracy: 0.2152\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.4881 - accuracy: 0.4792 - val_loss: 2.2704 - val_accuracy: 0.1850\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.3319 - accuracy: 0.5134 - val_loss: 1.8232 - val_accuracy: 0.3463\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2388 - accuracy: 0.5516 - val_loss: 1.7453 - val_accuracy: 0.3606\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2969 - accuracy: 0.5416 - val_loss: 1.7397 - val_accuracy: 0.3842\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3687 - accuracy: 0.5200 - val_loss: 1.7911 - val_accuracy: 0.3312\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2424 - accuracy: 0.5604 - val_loss: 1.9268 - val_accuracy: 0.3528\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1732 - accuracy: 0.5799 - val_loss: 1.4513 - val_accuracy: 0.4532\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2852 - accuracy: 0.5555 - val_loss: 1.7275 - val_accuracy: 0.4443\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2888 - accuracy: 0.5531 - val_loss: 1.5345 - val_accuracy: 0.4377\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2527 - accuracy: 0.5772 - val_loss: 1.4821 - val_accuracy: 0.4283\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1279 - accuracy: 0.6047 - val_loss: 1.1624 - val_accuracy: 0.5504\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2026 - accuracy: 0.5909 - val_loss: 1.9331 - val_accuracy: 0.5198\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0796 - accuracy: 0.6190 - val_loss: 1.2050 - val_accuracy: 0.5635\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1229 - accuracy: 0.6020 - val_loss: 1.3383 - val_accuracy: 0.5549\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1250 - accuracy: 0.6165 - val_loss: 1.3097 - val_accuracy: 0.4855\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3031 - accuracy: 0.5879 - val_loss: 2.2881 - val_accuracy: 0.4230\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3317 - accuracy: 0.5877 - val_loss: 1.4073 - val_accuracy: 0.5970\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1509 - accuracy: 0.6127 - val_loss: 1.3111 - val_accuracy: 0.5284\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0055 - accuracy: 0.6473 - val_loss: 0.9608 - val_accuracy: 0.6452\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0133 - accuracy: 0.6385 - val_loss: 1.4401 - val_accuracy: 0.4912\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9604 - accuracy: 0.6570 - val_loss: 1.7056 - val_accuracy: 0.5766\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0579 - accuracy: 0.6399 - val_loss: 1.2445 - val_accuracy: 0.5365\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0284 - accuracy: 0.6467 - val_loss: 1.0781 - val_accuracy: 0.5819\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0338 - accuracy: 0.6482 - val_loss: 1.4737 - val_accuracy: 0.5112\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2324 - accuracy: 0.6088 - val_loss: 1.4057 - val_accuracy: 0.5439\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1363 - accuracy: 0.6256 - val_loss: 0.9996 - val_accuracy: 0.6439\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2063 - accuracy: 0.5932 - val_loss: 1.1401 - val_accuracy: 0.5925\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 0.9958 - accuracy: 0.6443 - val_loss: 1.2241 - val_accuracy: 0.5647\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.9272 - accuracy: 0.6717 - val_loss: 1.2838 - val_accuracy: 0.5619\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9604 - accuracy: 0.6630 - val_loss: 1.3841 - val_accuracy: 0.5145\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0007 - accuracy: 0.6642 - val_loss: 1.5434 - val_accuracy: 0.4994\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9250 - accuracy: 0.6790 - val_loss: 1.6268 - val_accuracy: 0.4541\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9074 - accuracy: 0.6840 - val_loss: 1.8119 - val_accuracy: 0.4602\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0075 - accuracy: 0.6603 - val_loss: 1.5927 - val_accuracy: 0.3953\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8823 - accuracy: 0.6890 - val_loss: 1.1097 - val_accuracy: 0.6590\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9898 - accuracy: 0.6752 - val_loss: 1.5879 - val_accuracy: 0.4708\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1259 - accuracy: 0.6520 - val_loss: 1.9539 - val_accuracy: 0.4826\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9499 - accuracy: 0.6838 - val_loss: 1.0831 - val_accuracy: 0.6219\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8642 - accuracy: 0.7018 - val_loss: 1.1999 - val_accuracy: 0.6366\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9278 - accuracy: 0.6916 - val_loss: 1.2216 - val_accuracy: 0.5900\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1785 - accuracy: 0.6630 - val_loss: 1.3779 - val_accuracy: 0.5774\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8147 - accuracy: 0.7121 - val_loss: 0.9714 - val_accuracy: 0.6431\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8522 - accuracy: 0.7024 - val_loss: 1.0474 - val_accuracy: 0.6027\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8564 - accuracy: 0.7089 - val_loss: 0.8741 - val_accuracy: 0.7097\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8249 - accuracy: 0.7119 - val_loss: 0.9061 - val_accuracy: 0.6872\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8154 - accuracy: 0.7151 - val_loss: 1.2316 - val_accuracy: 0.6366\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9174 - accuracy: 0.6974 - val_loss: 0.9467 - val_accuracy: 0.6607\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9781 - accuracy: 0.6893 - val_loss: 0.8109 - val_accuracy: 0.7174\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8351 - accuracy: 0.7180 - val_loss: 1.0599 - val_accuracy: 0.5962\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9260 - accuracy: 0.6936 - val_loss: 1.0914 - val_accuracy: 0.6509\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7980 - accuracy: 0.7232 - val_loss: 0.9121 - val_accuracy: 0.6950\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9189 - accuracy: 0.7102 - val_loss: 0.8669 - val_accuracy: 0.6774\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0682 - accuracy: 0.6842 - val_loss: 2.2339 - val_accuracy: 0.4696\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0947 - accuracy: 0.6799 - val_loss: 1.0698 - val_accuracy: 0.6080\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8642 - accuracy: 0.7175 - val_loss: 0.9562 - val_accuracy: 0.6815\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7925 - accuracy: 0.7274 - val_loss: 1.5936 - val_accuracy: 0.5753\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8195 - accuracy: 0.7235 - val_loss: 0.9852 - val_accuracy: 0.6595\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7703 - accuracy: 0.7429 - val_loss: 0.9389 - val_accuracy: 0.6754\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9245 - accuracy: 0.7033 - val_loss: 1.2056 - val_accuracy: 0.5500\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0472 - accuracy: 0.6697 - val_loss: 1.3709 - val_accuracy: 0.5651\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8025 - accuracy: 0.7224 - val_loss: 0.7935 - val_accuracy: 0.7256\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7572 - accuracy: 0.7471 - val_loss: 0.9762 - val_accuracy: 0.6476\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7372 - accuracy: 0.7459 - val_loss: 0.9245 - val_accuracy: 0.7080\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8395 - accuracy: 0.7292 - val_loss: 1.0747 - val_accuracy: 0.6823\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 0.9850 - accuracy: 0.7112 - val_loss: 0.8865 - val_accuracy: 0.7048\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 0.7682 - accuracy: 0.7400 - val_loss: 0.8015 - val_accuracy: 0.7419\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.8220 - accuracy: 0.7250 - val_loss: 1.4933 - val_accuracy: 0.5909\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 3s 17ms/step - loss: 0.9755 - accuracy: 0.7003 - val_loss: 1.1108 - val_accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7528 - accuracy: 0.7425 - val_loss: 1.0805 - val_accuracy: 0.6231\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7478 - accuracy: 0.7457 - val_loss: 0.9948 - val_accuracy: 0.6582\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2186 - accuracy: 0.6236 - val_loss: 1.6192 - val_accuracy: 0.4410\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.5328 - accuracy: 0.5193 - val_loss: 1.8077 - val_accuracy: 0.4512\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1199 - accuracy: 0.6073 - val_loss: 2.3124 - val_accuracy: 0.3238\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0154 - accuracy: 0.6373 - val_loss: 1.5642 - val_accuracy: 0.4696\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0180 - accuracy: 0.6391 - val_loss: 1.6127 - val_accuracy: 0.4782\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9659 - accuracy: 0.6627 - val_loss: 1.2751 - val_accuracy: 0.5341\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9579 - accuracy: 0.6616 - val_loss: 1.3366 - val_accuracy: 0.5202\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9980 - accuracy: 0.6544 - val_loss: 1.4430 - val_accuracy: 0.5325\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8681 - accuracy: 0.6859 - val_loss: 1.4379 - val_accuracy: 0.5157\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9067 - accuracy: 0.6823 - val_loss: 1.0926 - val_accuracy: 0.6031\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0537 - accuracy: 0.6552 - val_loss: 1.0069 - val_accuracy: 0.6672\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9899 - accuracy: 0.6701 - val_loss: 1.1997 - val_accuracy: 0.6284\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9223 - accuracy: 0.6889 - val_loss: 0.8170 - val_accuracy: 0.7203\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.8170 - accuracy: 0.7203\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[0 0 4 ... 0 7 2]\n",
      "Confusion matrix\n",
      " [[228   0  11   0   0   0   0   0   0   0]\n",
      " [ 42 155  16   6   0   3   6   2   0   0]\n",
      " [ 29   0 235   2   0   1   0   5   0   5]\n",
      " [ 16   1  39 204   0   0   1   0   1   0]\n",
      " [ 10   0   7   0 137  43  10  14   0  34]\n",
      " [ 22   0   0   0   4 167  12  10   0   3]\n",
      " [ 51   0   2   0   0  23 196   3   0   6]\n",
      " [ 25   0   6   0   0  10   1 157   0  34]\n",
      " [ 15   0  24   0   0   3   0  50 113  46]\n",
      " [  4   0  14   0   0   5   0   8   0 172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.95      0.67       239\n",
      "           1       0.99      0.67      0.80       230\n",
      "           2       0.66      0.85      0.74       277\n",
      "           3       0.96      0.78      0.86       262\n",
      "           4       0.97      0.54      0.69       255\n",
      "           5       0.65      0.77      0.71       218\n",
      "           6       0.87      0.70      0.77       281\n",
      "           7       0.63      0.67      0.65       233\n",
      "           8       0.99      0.45      0.62       251\n",
      "           9       0.57      0.85      0.68       203\n",
      "\n",
      "    accuracy                           0.72      2449\n",
      "   macro avg       0.78      0.72      0.72      2449\n",
      "weighted avg       0.79      0.72      0.72      2449\n",
      "\n",
      "(12260, 70)\n",
      "(9792, 20, 60)\n",
      "(9792, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_52 (Conv1D)          (None, 14, 64)            26944     \n",
      "                                                                 \n",
      " conv1d_53 (Conv1D)          (None, 8, 64)             28736     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 8, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_26 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_26 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,790\n",
      "Trainable params: 82,590\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 2.3104 - accuracy: 0.1546 - val_loss: 2.3142 - val_accuracy: 0.0984\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 2.0280 - accuracy: 0.1952 - val_loss: 3.5054 - val_accuracy: 0.0992\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 3s 22ms/step - loss: 1.9360 - accuracy: 0.2618 - val_loss: 2.5265 - val_accuracy: 0.1009\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9378 - accuracy: 0.3141 - val_loss: 15.2655 - val_accuracy: 0.1082\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.6729 - accuracy: 0.4003 - val_loss: 2.5220 - val_accuracy: 0.1421\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4994 - accuracy: 0.4613 - val_loss: 2.0838 - val_accuracy: 0.2238\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4603 - accuracy: 0.4789 - val_loss: 2.2027 - val_accuracy: 0.2368\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4116 - accuracy: 0.4956 - val_loss: 2.0109 - val_accuracy: 0.1960\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4201 - accuracy: 0.5106 - val_loss: 2.1471 - val_accuracy: 0.1931\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3274 - accuracy: 0.5353 - val_loss: 2.2505 - val_accuracy: 0.2585\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.3042 - accuracy: 0.5299 - val_loss: 2.4712 - val_accuracy: 0.2683\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2747 - accuracy: 0.5507 - val_loss: 2.4655 - val_accuracy: 0.2438\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2811 - accuracy: 0.5544 - val_loss: 2.1953 - val_accuracy: 0.2871\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3189 - accuracy: 0.5513 - val_loss: 2.0811 - val_accuracy: 0.2479\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.3063 - accuracy: 0.5519 - val_loss: 2.2441 - val_accuracy: 0.2487\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2847 - accuracy: 0.5639 - val_loss: 1.8719 - val_accuracy: 0.2801\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2405 - accuracy: 0.5753 - val_loss: 1.8412 - val_accuracy: 0.2626\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2277 - accuracy: 0.5876 - val_loss: 1.8598 - val_accuracy: 0.2940\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2030 - accuracy: 0.5876 - val_loss: 1.8986 - val_accuracy: 0.3079\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1344 - accuracy: 0.6120 - val_loss: 2.1689 - val_accuracy: 0.2058\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1078 - accuracy: 0.6206 - val_loss: 2.2085 - val_accuracy: 0.2854\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0422 - accuracy: 0.6333 - val_loss: 1.8857 - val_accuracy: 0.4042\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0635 - accuracy: 0.6325 - val_loss: 1.5878 - val_accuracy: 0.4136\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0388 - accuracy: 0.6404 - val_loss: 1.8887 - val_accuracy: 0.4467\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 4s 23ms/step - loss: 1.0920 - accuracy: 0.6357 - val_loss: 2.4706 - val_accuracy: 0.4888\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1464 - accuracy: 0.6289 - val_loss: 9.9789 - val_accuracy: 0.1511\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 3s 18ms/step - loss: 2.4077 - accuracy: 0.2018 - val_loss: 2.5370 - val_accuracy: 0.1111\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 1.7533 - accuracy: 0.3899 - val_loss: 2.0334 - val_accuracy: 0.2446\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7495 - accuracy: 0.4329 - val_loss: 2.1513 - val_accuracy: 0.2474\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7330 - accuracy: 0.4277 - val_loss: 5.9252 - val_accuracy: 0.1907\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 2s 16ms/step - loss: 1.5503 - accuracy: 0.4971 - val_loss: 1.9285 - val_accuracy: 0.3083\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 2s 16ms/step - loss: 1.3505 - accuracy: 0.5495 - val_loss: 2.1809 - val_accuracy: 0.2454\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.4507 - accuracy: 0.5261 - val_loss: 1.8579 - val_accuracy: 0.2842\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.3978 - accuracy: 0.5398 - val_loss: 1.7054 - val_accuracy: 0.3585\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2789 - accuracy: 0.5675 - val_loss: 2.1237 - val_accuracy: 0.2969\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2218 - accuracy: 0.5813 - val_loss: 1.9261 - val_accuracy: 0.3312\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1439 - accuracy: 0.6090 - val_loss: 1.4487 - val_accuracy: 0.4745\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1302 - accuracy: 0.6085 - val_loss: 1.6572 - val_accuracy: 0.4855\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1636 - accuracy: 0.6068 - val_loss: 1.4228 - val_accuracy: 0.5349\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2118 - accuracy: 0.5973 - val_loss: 1.4019 - val_accuracy: 0.5141\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1161 - accuracy: 0.6194 - val_loss: 1.5963 - val_accuracy: 0.5080\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.4333 - accuracy: 0.5521 - val_loss: 1.3366 - val_accuracy: 0.5406\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.9746 - accuracy: 0.4301 - val_loss: 3.8420 - val_accuracy: 0.2703\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.7420 - accuracy: 0.4642 - val_loss: 1.7440 - val_accuracy: 0.3953\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.2358 - accuracy: 0.5444 - val_loss: 1.4803 - val_accuracy: 0.4643\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1985 - accuracy: 0.5618 - val_loss: 1.3844 - val_accuracy: 0.4753\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1673 - accuracy: 0.5885 - val_loss: 1.6035 - val_accuracy: 0.4353\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1228 - accuracy: 0.5995 - val_loss: 1.4657 - val_accuracy: 0.5035\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1258 - accuracy: 0.6074 - val_loss: 1.8372 - val_accuracy: 0.4149\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.1198 - accuracy: 0.6114 - val_loss: 1.9514 - val_accuracy: 0.3993\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1161 - accuracy: 0.6139 - val_loss: 1.3324 - val_accuracy: 0.4561\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 3s 19ms/step - loss: 1.1007 - accuracy: 0.6172 - val_loss: 1.4462 - val_accuracy: 0.5713\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0762 - accuracy: 0.6302 - val_loss: 1.5851 - val_accuracy: 0.5333\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2070 - accuracy: 0.5921 - val_loss: 1.4431 - val_accuracy: 0.5500\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0510 - accuracy: 0.6333 - val_loss: 1.1125 - val_accuracy: 0.5949\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0372 - accuracy: 0.6457 - val_loss: 1.3742 - val_accuracy: 0.5202\n",
      "Epoch 57/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2115 - accuracy: 0.6003 - val_loss: 1.5127 - val_accuracy: 0.5337\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1130 - accuracy: 0.6308 - val_loss: 1.2738 - val_accuracy: 0.5386\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0538 - accuracy: 0.6388 - val_loss: 0.9750 - val_accuracy: 0.6815\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.9677 - accuracy: 0.6543 - val_loss: 0.9951 - val_accuracy: 0.6595\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9843 - accuracy: 0.6610 - val_loss: 1.5022 - val_accuracy: 0.5288\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9953 - accuracy: 0.6645 - val_loss: 1.5337 - val_accuracy: 0.5966\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9995 - accuracy: 0.6648 - val_loss: 1.0582 - val_accuracy: 0.6399\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.8770 - accuracy: 0.6852 - val_loss: 0.8688 - val_accuracy: 0.6872\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8908 - accuracy: 0.6936 - val_loss: 1.4276 - val_accuracy: 0.5496\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9629 - accuracy: 0.6791 - val_loss: 1.0036 - val_accuracy: 0.6439\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1371 - accuracy: 0.6528 - val_loss: 1.3500 - val_accuracy: 0.5847\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.3266 - accuracy: 0.6305 - val_loss: 2.0557 - val_accuracy: 0.5235\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.0818 - accuracy: 0.6609 - val_loss: 0.9983 - val_accuracy: 0.6693\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8430 - accuracy: 0.7080 - val_loss: 0.8696 - val_accuracy: 0.6860\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7939 - accuracy: 0.7223 - val_loss: 0.9131 - val_accuracy: 0.6823\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7734 - accuracy: 0.7231 - val_loss: 0.8283 - val_accuracy: 0.7162\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.7189 - val_loss: 1.2643 - val_accuracy: 0.6015\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8200 - accuracy: 0.7166 - val_loss: 0.9378 - val_accuracy: 0.6345\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.0547 - accuracy: 0.6807 - val_loss: 1.7581 - val_accuracy: 0.5410\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.2806 - accuracy: 0.6545 - val_loss: 1.2194 - val_accuracy: 0.6076\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9926 - accuracy: 0.7007 - val_loss: 1.5219 - val_accuracy: 0.5280\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8343 - accuracy: 0.7188 - val_loss: 0.9498 - val_accuracy: 0.7089\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7431 - accuracy: 0.7346 - val_loss: 0.9043 - val_accuracy: 0.6713\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8422 - accuracy: 0.7166 - val_loss: 1.4707 - val_accuracy: 0.5051\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8349 - accuracy: 0.7292 - val_loss: 1.4578 - val_accuracy: 0.4908\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1041 - accuracy: 0.6828 - val_loss: 1.5750 - val_accuracy: 0.4965\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9605 - accuracy: 0.6982 - val_loss: 1.2029 - val_accuracy: 0.6497\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7394 - accuracy: 0.7474 - val_loss: 0.9650 - val_accuracy: 0.6492\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7030 - accuracy: 0.7504 - val_loss: 1.4659 - val_accuracy: 0.5341\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7459 - accuracy: 0.7463 - val_loss: 1.3102 - val_accuracy: 0.5676\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.8297 - accuracy: 0.7317 - val_loss: 1.2274 - val_accuracy: 0.6795\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7817 - accuracy: 0.7385 - val_loss: 1.0574 - val_accuracy: 0.6317\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 1.3235 - accuracy: 0.6588 - val_loss: 23.0485 - val_accuracy: 0.5357\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 1.1208 - accuracy: 0.6998 - val_loss: 0.9156 - val_accuracy: 0.6329\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7266 - accuracy: 0.7528 - val_loss: 0.9437 - val_accuracy: 0.6901\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6595 - accuracy: 0.7684 - val_loss: 0.7481 - val_accuracy: 0.7264\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6849 - accuracy: 0.7674 - val_loss: 0.9501 - val_accuracy: 0.6631\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.6859 - accuracy: 0.7633 - val_loss: 5.0274 - val_accuracy: 0.7285\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6875 - accuracy: 0.7632 - val_loss: 0.7972 - val_accuracy: 0.7060\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7412 - accuracy: 0.7539 - val_loss: 1.0881 - val_accuracy: 0.6219\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9538 - accuracy: 0.7219 - val_loss: 2.0205 - val_accuracy: 0.5108\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.9306 - accuracy: 0.7342 - val_loss: 1.3208 - val_accuracy: 0.5896\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.7672 - accuracy: 0.7515 - val_loss: 0.9800 - val_accuracy: 0.6154\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.7000 - accuracy: 0.7661 - val_loss: 1.1074 - val_accuracy: 0.5802\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1074 - accuracy: 0.5802\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "[8 8 2 ... 2 1 2]\n",
      "Confusion matrix\n",
      " [[140   2  66   0   0   0  55   0   0   0]\n",
      " [  0 150  89   5   0   0   3   0   1   0]\n",
      " [  0   0 220   1   0   0  39   0   1   0]\n",
      " [  0   1  56 199   0   0   1   0   0   0]\n",
      " [  0   0   0   0  82   1  51  10  92   0]\n",
      " [  0   0   0   0   0   0 167   0  18   0]\n",
      " [  0   0   1   0   0   0 299   0   0   0]\n",
      " [  0   0   0   0   0   0  40   2 200   0]\n",
      " [  0   0   0   0   0   0  23   0 219   1]\n",
      " [  0   0   0   0   0   0  24   0  80 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.69       263\n",
      "           1       0.98      0.60      0.75       248\n",
      "           2       0.51      0.84      0.63       261\n",
      "           3       0.97      0.77      0.86       257\n",
      "           4       1.00      0.35      0.52       236\n",
      "           5       0.00      0.00      0.00       185\n",
      "           6       0.43      1.00      0.60       300\n",
      "           7       0.17      0.01      0.02       242\n",
      "           8       0.36      0.90      0.51       243\n",
      "           9       0.99      0.51      0.68       214\n",
      "\n",
      "    accuracy                           0.58      2449\n",
      "   macro avg       0.64      0.55      0.53      2449\n",
      "weighted avg       0.65      0.58      0.54      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# raw_angle_files_1 = glob(os.path.join(\"D:\\Research_Project\\My_project_22\\input\\preprocessed_modified\", \"*\", \"*.csv\"))\n",
    "# # print(raw_angle_files_1)\n",
    "# all_filenames = [i for i in raw_angle_files_1]\n",
    "# df = pd.concat(map(pd.read_csv, all_filenames),ignore_index=True)\n",
    "# data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\tree_classifier_extracted.csv\")\n",
    "# print(df.shape)\n",
    "# df = df.sample(frac = 1)\n",
    "# # df.iloc[1:10,:]\n",
    "\n",
    "\n",
    "list_train_accuracy=[]\n",
    "list_val_accuracy=[]\n",
    "for opt in adam_optimisers:\n",
    "    data=pd.read_csv(r\"D:\\Research_Project\\My_project_22\\input\\final_preprocessed_merged\\fischer_extracted_extended_modified.csv\")\n",
    "    data=pd.get_dummies(data,columns=['target'])\n",
    "    data=data.to_numpy()\n",
    "    x,y=split_sequences(data,20)\n",
    "    # x=x[None:]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # data=data.to_numpy()\n",
    "    print(data.shape)\n",
    "\n",
    "    # y=data.iloc[:,26:]\n",
    "    # x=data.iloc[:,:26]\n",
    "    # # x=data[-1:26]\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # x=x.to_numpy()\n",
    "    # y=y.to_numpy()\n",
    "    x_train,x_test,y_train,y_test= train_test_split(x, y, test_size = 0.2)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(type(y_train))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(20,60)))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=7, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # model.summary()\n",
    "    # # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # print(y_train)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    # # print(x.shape)\n",
    "    history = model.fit(x_train, y_train,batch_size=config.BATCH_SIZE,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    y_pred=model.predict(x_test)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    y_test=np.argmax(y_test,axis=1)\n",
    "    list_train_accuracy.append(history.history[\"accuracy\"][-1])\n",
    "    list_val_accuracy.append(history.history[\"val_accuracy\"][-1])\n",
    "    print(y_pred)\n",
    "    cf_matrix=confusion_matrix(y_test,y_pred)\n",
    "    print('Confusion matrix\\n',cf_matrix)\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.plot( list_learning_rate,list_train_accuracy, marker = 's' )\n",
    "plt.plot( list_learning_rate ,list_val_accuracy, marker = 's' )\n",
    "plt.xlabel( 'learning rate', fontsize = 17 )\n",
    "plt.ylabel( 'Accuracy', fontsize = 18 )\n",
    "plt.legend(['Train Set', 'Test Set'], loc='best',fontsize = 14)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks( fontsize = 14 )\n",
    "plt.yticks( fontsize = 14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a4e75e3d00>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# setting x values\n",
    "x =['Geeks', 'for', 'geeks', 'tutorials']\n",
    "  \n",
    "# Setting y values \n",
    "y =[2,2, 3, 4]\n",
    "fig,ax=plt.subplots()\n",
    "  \n",
    "# Adding label on the y-axis\n",
    "plt.ylabel('Numbers label')\n",
    "plt.xlabel('x-axis',labelpad=80)\n",
    "plt.ylim(0,6)\n",
    "ax.set_xticks([1,2,3,4,5])\n",
    "\n",
    "  \n",
    "# plotting the graph\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Axis.set_ticks() missing 1 required positional argument: 'ticks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m ax\u001b[39m.\u001b[39mplot(x, y)\n\u001b[0;32m     15\u001b[0m \u001b[39m# change the fontsize\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# ax.set_xticklabels(x, fontsize=20)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m ax\u001b[39m.\u001b[39;49mset_xticks(spacing\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# show the plot\u001b[39;00m\n\u001b[0;32m     20\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:75\u001b[0m, in \u001b[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m get_method(\u001b[39mself\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Axis.set_ticks() missing 1 required positional argument: 'ticks'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = ['afsfffffffffffffffffffffffffffffffffffffff','ddfsf','sfsfg','sfggs','sfsgsgsghfh','gsgaggg']\n",
    "y = np.arange(1,7,1)\n",
    " \n",
    "# make objects of subplots\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "# plot the data\n",
    "ax.plot(x, y)\n",
    " \n",
    "# change the fontsize\n",
    "# ax.set_xticklabels(x, fontsize=20)\n",
    "ax.set_xticks(spacing=30)\n",
    " \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.69314718, 1.09861229, 1.38629436, 1.60943791,\n",
       "       1.79175947, 1.94591015, 2.07944154, 2.19722458, 2.30258509])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(range(1, 11, 1))\n",
    "x\n",
    "y = np.log(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229cdfb8eedfa4964725b7eb0da8d7a63b25d97a6ab808f09bd6b506844c0629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
